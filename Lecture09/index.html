
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../image/logo.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.1.2">
    
    
      
        <title>Lecture09 自注意力模型、Transformers - Xiaoyu's Blog</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.f7951f6f.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.e6a45f82.min.css">
        
          
          
          <meta name="theme-color" content="#000000">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Serif+Pro:300,400,400i,700%7C&display=fallback">
        <style>:root{--md-text-font:"Source Serif Pro";--md-code-font:""}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="black" data-md-color-accent="">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lecture09-transformers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Xiaoyu&#39;s Blog" class="md-header__button md-logo" aria-label="Xiaoyu's Blog" data-md-component="logo">
      
  <img src="../image/logo2.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Xiaoyu's Blog
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Lecture09 自注意力模型、Transformers
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/chuxiaoyu/blog/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    chuxiaoyu/blog
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href=".." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../dw-index/" class="md-tabs__link md-tabs__link--active">
        Datawhale组队学习
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../nand2tetris_part_1/" class="md-tabs__link">
        CS Self Learning
      </a>
    </li>
  

      
        
  
  


  <li class="md-tabs__item">
    <a href="https://www.chuxiaoyu.cn/" class="md-tabs__link">
      About
    </a>
  </li>

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Xiaoyu&#39;s Blog" class="md-nav__button md-logo" aria-label="Xiaoyu's Blog" data-md-component="logo">
      
  <img src="../image/logo2.png" alt="logo">

    </a>
    Xiaoyu's Blog
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/chuxiaoyu/blog/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    chuxiaoyu/blog
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Datawhale组队学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Datawhale组队学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Datawhale组队学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../dw-index/" class="md-nav__link">
        简介
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2_2">
          CS224n自然语言处理
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="CS224n自然语言处理" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          CS224n自然语言处理
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Lecture07/" class="md-nav__link">
        Lecture07 机器翻译、seq2seq模型、注意力机制
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Lecture09 自注意力模型、Transformers
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Lecture09 自注意力模型、Transformers
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    本节主要内容
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-rnnnlp" class="md-nav__link">
    1 从RNN到基于注意力的NLP模型
  </a>
  
    <nav class="md-nav" aria-label="1 从RNN到基于注意力的NLP模型">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-rnn" class="md-nav__link">
    1.1 RNN模型存在的问题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    1.2 自注意力模型介绍
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13" class="md-nav__link">
    1.3 自注意力模块的处理
  </a>
  
    <nav class="md-nav" aria-label="1.3 自注意力模块的处理">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#131" class="md-nav__link">
    1.3.1 位置编码
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#132" class="md-nav__link">
    1.3.2 遮罩
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-transformer" class="md-nav__link">
    2 Transformer模型介绍
  </a>
  
    <nav class="md-nav" aria-label="2 Transformer模型介绍">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-transformer" class="md-nav__link">
    2.1 Transformer概览
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-transformer" class="md-nav__link">
    2.2 Transformer编码器
  </a>
  
    <nav class="md-nav" aria-label="2.2 Transformer编码器">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-query-key-value" class="md-nav__link">
    2.2.1 Query-Key-Value向量矩阵
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222" class="md-nav__link">
    2.2.2 多头注意力机制
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#223" class="md-nav__link">
    2.2.3 残差连接
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#224" class="md-nav__link">
    2.2.4 归一化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#225" class="md-nav__link">
    2.2.5 缩放点积运算
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#226" class="md-nav__link">
    2.2.6 小结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-transformer" class="md-nav__link">
    2.3 Transformer解码器
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Lecture10/" class="md-nav__link">
        Lecture10 预训练
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Lecture11/" class="md-nav__link">
        Lecture11 问答系统
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_3" type="checkbox" id="__nav_2_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_3">
          基于transformers的NLP
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="基于transformers的NLP" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_3">
          <span class="md-nav__icon md-icon"></span>
          基于transformers的NLP
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../nlp-transformer-task01/" class="md-nav__link">
        Task01 NLP学习概览
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../nlp-transformer-task02/" class="md-nav__link">
        Task02 学习Attentioin和Transformer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../nlp-transformer-task03/" class="md-nav__link">
        Task03 学习BERT
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../nlp-transformer-task04/" class="md-nav__link">
        Task04 学习GPT
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../nlp-transformer-task05/" class="md-nav__link">
        Task05 编写BERT模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../nlp-transformer-task06/" class="md-nav__link">
        Task06 BERT应用、训练和优化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../nlp-transformer-task07/" class="md-nav__link">
        Task07 使用Transformers解决文本分类任务
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_4" type="checkbox" id="__nav_2_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_4">
          深入浅出PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="深入浅出PyTorch" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_4">
          <span class="md-nav__icon md-icon"></span>
          深入浅出PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch-chap01-02/" class="md-nav__link">
        Chapter01-02 PyTorch的简介和安装、PyTorch基础知识
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch-chap03/" class="md-nav__link">
        Chapter03 PyTorch的主要组成模块
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch-chap04/" class="md-nav__link">
        Chapter04 PyTorch基础实战——FashionMNIST图像分类
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          CS Self Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="CS Self Learning" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          CS Self Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../nand2tetris_part_1/" class="md-nav__link">
        Nand2Tetris Part1
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="https://www.chuxiaoyu.cn/" class="md-nav__link">
        About
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    本节主要内容
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-rnnnlp" class="md-nav__link">
    1 从RNN到基于注意力的NLP模型
  </a>
  
    <nav class="md-nav" aria-label="1 从RNN到基于注意力的NLP模型">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-rnn" class="md-nav__link">
    1.1 RNN模型存在的问题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    1.2 自注意力模型介绍
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13" class="md-nav__link">
    1.3 自注意力模块的处理
  </a>
  
    <nav class="md-nav" aria-label="1.3 自注意力模块的处理">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#131" class="md-nav__link">
    1.3.1 位置编码
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#132" class="md-nav__link">
    1.3.2 遮罩
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-transformer" class="md-nav__link">
    2 Transformer模型介绍
  </a>
  
    <nav class="md-nav" aria-label="2 Transformer模型介绍">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-transformer" class="md-nav__link">
    2.1 Transformer概览
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-transformer" class="md-nav__link">
    2.2 Transformer编码器
  </a>
  
    <nav class="md-nav" aria-label="2.2 Transformer编码器">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-query-key-value" class="md-nav__link">
    2.2.1 Query-Key-Value向量矩阵
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222" class="md-nav__link">
    2.2.2 多头注意力机制
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#223" class="md-nav__link">
    2.2.3 残差连接
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#224" class="md-nav__link">
    2.2.4 归一化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#225" class="md-nav__link">
    2.2.5 缩放点积运算
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#226" class="md-nav__link">
    2.2.6 小结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-transformer" class="md-nav__link">
    2.3 Transformer解码器
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
<a href="https://github.com/chuxiaoyu/blog/edit/master/docs/Lecture09.md" title="Edit this page" class="md-content__button md-icon">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
</a>


<h1 id="lecture09-transformers">Lecture09: 自注意力模型、Transformers<a class="headerlink" href="#lecture09-transformers" title="Permanent link">&para;</a></h1>
<h2 id="_1">本节主要内容<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<ul>
<li>从RNN到基于注意力的NLP模型</li>
<li>Transformer模型</li>
</ul>
<h2 id="1-rnnnlp">1 从RNN到基于注意力的NLP模型<a class="headerlink" href="#1-rnnnlp" title="Permanent link">&para;</a></h2>
<h3 id="11-rnn">1.1 RNN模型存在的问题<a class="headerlink" href="#11-rnn" title="Permanent link">&para;</a></h3>
<ol>
<li>线性相互作用距离（Linear interaction distance），即长距离依赖问题。</li>
<li>缺少并行性（parallelizability）。</li>
</ol>
<p>为了解决上述问题，人们考虑到了注意力机制。</p>
<blockquote>
<p><em>[学生提问]注意力和全连接网络的区别是什么？</em></p>
<p><em>1.注意力的权重是动态的</em></p>
<p><em>2.参数的计算不同</em></p>
</blockquote>
<h3 id="12">1.2 自注意力模型介绍<a class="headerlink" href="#12" title="Permanent link">&para;</a></h3>
<p>注意力机制的运作需要queris, keys, values向量：</p>
<ul>
<li><strong>queries</strong>  <span class="arithmatex"><span class="MathJax_Preview">q_1, q_2,..., q_T</span><script type="math/tex">q_1, q_2,..., q_T</script></span>，<span class="arithmatex"><span class="MathJax_Preview">q_i∈R^d</span><script type="math/tex">q_i∈R^d</script></span></li>
<li><strong>keys</strong>  <span class="arithmatex"><span class="MathJax_Preview">k_1, k_2,..., k_T</span><script type="math/tex">k_1, k_2,..., k_T</script></span>，<span class="arithmatex"><span class="MathJax_Preview">k_i∈R^d</span><script type="math/tex">k_i∈R^d</script></span></li>
<li><strong>values</strong>  <span class="arithmatex"><span class="MathJax_Preview">v_1, v_2,..., v_T</span><script type="math/tex">v_1, v_2,..., v_T</script></span>，<span class="arithmatex"><span class="MathJax_Preview">v_i∈R^d</span><script type="math/tex">v_i∈R^d</script></span></li>
</ul>
<p>在自注意力模型（self-attention）中，queries，keys，values来源相同。</p>
<ul>
<li>例如，如果某层的输出是<span class="arithmatex"><span class="MathJax_Preview">x_1, x_2,..., x_T</span><script type="math/tex">x_1, x_2,..., x_T</script></span>，那么可以使<span class="arithmatex"><span class="MathJax_Preview">v_i = k_i = q_i = x_i</span><script type="math/tex">v_i = k_i = q_i = x_i</script></span>[?]</li>
</ul>
<p>那么，自注意力的计算（以点积为例）如下：</p>
<p>（1）计算query-key乘积，得到注意力分数<span class="arithmatex"><span class="MathJax_Preview">e_{ij}</span><script type="math/tex">e_{ij}</script></span>
$$
e_{ij} = q_i^Tk_j
$$
（2）计算注意力权重<span class="arithmatex"><span class="MathJax_Preview">α</span><script type="math/tex">α</script></span>
$$
α_{ij} = softmax(e_{ij})
$$
（3）计算输出
$$
output_i = \sum_j{α_{ij}v_j}
$$</p>
<h3 id="13">1.3 自注意力模块的处理<a class="headerlink" href="#13" title="Permanent link">&para;</a></h3>
<h4 id="131">1.3.1 位置编码<a class="headerlink" href="#131" title="Permanent link">&para;</a></h4>
<p>因为自注意力没有考虑位置信息，所以需要将序列的位置编码到keys，queries，values向量中。</p>
<p>考虑将序列索引（sequence index）用向量（vector）表示：
$$
p_i∈R^d, for\; i∈{1,2,...,T}
$$
<span class="arithmatex"><span class="MathJax_Preview">p_i</span><script type="math/tex">p_i</script></span>即位置向量（positional vector）。</p>
<p>得到位置向量后，我们将其加到输入里。假设<span class="arithmatex"><span class="MathJax_Preview">\widetilde q</span><script type="math/tex">\widetilde q</script></span>，<span class="arithmatex"><span class="MathJax_Preview">\widetilde k</span><script type="math/tex">\widetilde k</script></span>，<span class="arithmatex"><span class="MathJax_Preview">\widetilde v</span><script type="math/tex">\widetilde v</script></span>是之前的向量，则：
$$
q_i =\widetilde q_i + p_i\
k_i =\widetilde k_i + p_i\
v_i =\widetilde v_i + p_i\
$$</p>
<blockquote>
<p><em>[?]位置向量不是加到输入里吗？为什么这里是加入到q,k,v？</em></p>
</blockquote>
<p>位置向量有多种计算方式，如正弦位置表示等，最常用的是绝对位置表示（absolute position representations）。（注：有点像独热编码）</p>
<h4 id="132">1.3.2 遮罩<a class="headerlink" href="#132" title="Permanent link">&para;</a></h4>
<p>进行序列预测的时候，不能看到后面的信息，因此采用了遮罩（Masking）处理，即将后面单词的注意力分数设置为<span class="arithmatex"><span class="MathJax_Preview">-\infty</span><script type="math/tex">-\infty</script></span>：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
e_{ij} = \left\{
\begin{aligned}
q^T_ik_j, j&lt;i\\
-\infty, j≥i
\end{aligned}
\right.
</div>
<script type="math/tex; mode=display">
e_{ij} = \left\{
\begin{aligned}
q^T_ik_j, j<i\\
-\infty, j≥i
\end{aligned}
\right.
</script>
</div>
<h2 id="2-transformer">2 Transformer模型介绍<a class="headerlink" href="#2-transformer" title="Permanent link">&para;</a></h2>
<h3 id="21-transformer">2.1 Transformer概览<a class="headerlink" href="#21-transformer" title="Permanent link">&para;</a></h3>
<p><img alt="image-20220113163942108" src="../image/image-20220113163942108.png" /></p>
<h3 id="22-transformer">2.2 Transformer编码器<a class="headerlink" href="#22-transformer" title="Permanent link">&para;</a></h3>
<p>编码器（Encoder）包含以下模块：</p>
<ul>
<li>
<p>Q-K-V向量</p>
</li>
<li>
<p>多头注意力机制（Multi-head attention）</p>
</li>
</ul>
<p>其他训练技巧（这些技巧不能提升模型能做什么，而是加速训练过程）</p>
<ul>
<li>残差连接（Residual connections）</li>
<li>归一化（Layer normalization）</li>
<li>缩放点积运算（Scaled Dot Product）</li>
</ul>
<h4 id="221-query-key-value">2.2.1 Query-Key-Value向量矩阵<a class="headerlink" href="#221-query-key-value" title="Permanent link">&para;</a></h4>
<p>用<span class="arithmatex"><span class="MathJax_Preview">x_1,...,x_T</span><script type="math/tex">x_1,...,x_T</script></span> <span class="arithmatex"><span class="MathJax_Preview">(x_i∈R^d)</span><script type="math/tex">(x_i∈R^d)</script></span>表示Transformer编码器的输入向量，则queries, keys, values的计算如下：</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">q_i=Qx_i</span><script type="math/tex">q_i=Qx_i</script></span>，<span class="arithmatex"><span class="MathJax_Preview">Q∈R^{d×d}</span><script type="math/tex">Q∈R^{d×d}</script></span>是权重矩阵</li>
<li><span class="arithmatex"><span class="MathJax_Preview">k_i=Kx_i</span><script type="math/tex">k_i=Kx_i</script></span>，<span class="arithmatex"><span class="MathJax_Preview">K∈R^{d×d}</span><script type="math/tex">K∈R^{d×d}</script></span>是权重矩阵</li>
<li><span class="arithmatex"><span class="MathJax_Preview">v_i=Vx_i</span><script type="math/tex">v_i=Vx_i</script></span>，<span class="arithmatex"><span class="MathJax_Preview">V∈R^{d×d}</span><script type="math/tex">V∈R^{d×d}</script></span>是权重矩阵</li>
</ul>
<p>对不同的参数矩阵对原始输入向量做线性变换，从而让不同的变换结果承担不同角色。</p>
<p>让我们通过矩阵的视角来看Q，K，V是如何计算的：</p>
<p>首先，用<span class="arithmatex"><span class="MathJax_Preview">X=[x_1;...;x_T]∈R^{T×d}</span><script type="math/tex">X=[x_1;...;x_T]∈R^{T×d}</script></span>表示输入向量的拼接矩阵，那么<span class="arithmatex"><span class="MathJax_Preview">XQ∈R^{T×d}</span><script type="math/tex">XQ∈R^{T×d}</script></span>，<span class="arithmatex"><span class="MathJax_Preview">XK∈R^{T×d}</span><script type="math/tex">XK∈R^{T×d}</script></span>，<span class="arithmatex"><span class="MathJax_Preview">XV∈R^{T×d}</span><script type="math/tex">XV∈R^{T×d}</script></span>。输出就可以表示为：
$$
output = softmax(XQ(XK)^T)×XV
$$
<img alt="image-20220113180152211" src="../image/image-20220113180152211.png" /></p>
<h4 id="222">2.2.2 多头注意力机制<a class="headerlink" href="#222" title="Permanent link">&para;</a></h4>
<p>对于单词<span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>，自注意力只注意到<span class="arithmatex"><span class="MathJax_Preview">x^T_iQ^TKx_j</span><script type="math/tex">x^T_iQ^TKx_j</script></span>高的地方，但是我们如何关注到不同的<span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>呢？</p>
<blockquote>
<p><em>这里对为什么要使用多头的解释并不清楚，可以参考：为什么Transformer 需要进行 Multi-head Attention？ - 知乎 <a href="https://www.zhihu.com/question/341222779">https://www.zhihu.com/question/341222779</a></em></p>
<p><em>《自然语言处理——基于预训练模型的方法》P93中的解释是：</em></p>
<p><em>“由于自注意力结果需要经过归一化，导致即使一个输入和多个其他的输入相关，也无法同时为这些输入赋予较大的注意力值，即自注意力结果之间是互斥的，无法同时关注多个输入。因此，如果能使用多组注意力模型产生多组不同的注意力结果，则不同组注意力模型可能关注到不同的输入上，从而增强模型的表达能力。“</em></p>
</blockquote>
<p>我们通过多个<span class="arithmatex"><span class="MathJax_Preview">Q, K, V</span><script type="math/tex">Q, K, V</script></span>矩阵定义多头注意力（Multi-headed Attention）。</p>
<p>用<span class="arithmatex"><span class="MathJax_Preview">Q_ℓ,K_ℓ,V_ℓ∈R^{d×d/h}</span><script type="math/tex">Q_ℓ,K_ℓ,V_ℓ∈R^{d×d/h}</script></span> 表示不同的参数矩阵，其中<span class="arithmatex"><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span>表示注意力头的序号，<span class="arithmatex"><span class="MathJax_Preview">ℓ</span><script type="math/tex">ℓ</script></span>的取值范围是从<span class="arithmatex"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>到<span class="arithmatex"><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span>。<em>(<span class="arithmatex"><span class="MathJax_Preview">R^{d×d/h}</span><script type="math/tex">R^{d×d/h}</script></span>)</em></p>
<p>每个注意力头独立运算：
$$
output_ℓ = softmax(XQ_ℓK<sup>T_ℓX</sup>T)*XV_ℓ
$$
其中<span class="arithmatex"><span class="MathJax_Preview">output_ℓ∈R^{d/h}</span><script type="math/tex">output_ℓ∈R^{d/h}</script></span>。</p>
<p>然后，将所有的输出混合：
$$
output = Y[output_1;...;output_h], Y∈R^{d×d}
$$
下图是单头注意力和多头注意力的简单示意图：</p>
<p><img alt="image-20220114135758701" src="../image/image-20220114135758701.png" /></p>
<p>可以看出，多头注意力和单个注意力的计算量是一样的。<em>（都是把矩阵拼起来计算一次）</em></p>
<h4 id="223">2.2.3 残差连接<a class="headerlink" href="#223" title="Permanent link">&para;</a></h4>
<p>残差连接（Residual connections）是一种提升模型训练效果的技巧。</p>
<p>正常情况：<span class="arithmatex"><span class="MathJax_Preview">X^{(i)} = Layer(X^{(i-1)})</span><script type="math/tex">X^{(i)} = Layer(X^{(i-1)})</script></span></p>
<p><img alt="image-20220114140717577" src="../image/image-20220114140717577.png" /></p>
<p>残差连接：<span class="arithmatex"><span class="MathJax_Preview">X^{(i)} = X^{(i-1)}+Layer(X^{(i-1)})</span><script type="math/tex">X^{(i)} = X^{(i-1)}+Layer(X^{(i-1)})</script></span></p>
<p><img alt="image-20220114140832475" src="../image/image-20220114140832475.png" /></p>
<h4 id="224">2.2.4 归一化<a class="headerlink" href="#224" title="Permanent link">&para;</a></h4>
<p>归一化（Layer normalization）是一种提升模型训练速度的技巧。
$$
output = \frac{x-μ}{\sqrt{𝜎}+𝜖}*𝛾+𝛽
$$
其中，𝜇是均值，𝜎是标准差。𝛾和𝛽是gain和bias参数[?]。</p>
<h4 id="225">2.2.5 缩放点积运算<a class="headerlink" href="#225" title="Permanent link">&para;</a></h4>
<p>缩放点积运算是为了防止在维数过大时，梯度变小或消失。</p>
<p>正常情况：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
output_ℓ = softmax(XQ_ℓK_ℓ^TX^T)*XV_ℓ
</div>
<script type="math/tex; mode=display">
output_ℓ = softmax(XQ_ℓK_ℓ^TX^T)*XV_ℓ
</script>
</div>
<p>缩放点积运算：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
output_ℓ = softmax(\frac{XQ_ℓK_ℓ^TX^T}{\sqrt{d/h}})*XV_ℓ
</div>
<script type="math/tex; mode=display">
output_ℓ = softmax(\frac{XQ_ℓK_ℓ^TX^T}{\sqrt{d/h}})*XV_ℓ
</script>
</div>
<p>可以看出，就是将注意力分数除以维数<span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>除以注意力头的数量<span class="arithmatex"><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span>的根。</p>
<h4 id="226">2.2.6 小结<a class="headerlink" href="#226" title="Permanent link">&para;</a></h4>
<p>2.1中给出了Tranformer编码器的整体概览，经过对模块组成的分解，编码器更具体的结构如下图所示：</p>
<p><img alt="image-20220114143617158" src="../image/image-20220114143617158.png" /></p>
<h3 id="23-transformer">2.3 Transformer解码器<a class="headerlink" href="#23-transformer" title="Permanent link">&para;</a></h3>
<p>解码器的结构与编码器类似，如图：</p>
<p><img alt="image-20220114144135116" src="../image/image-20220114144135116.png" /></p>
<p>可以看出，稍微不一样的地方在于交叉注意力（Cross attention）。</p>
<p><strong>交叉注意力机制</strong></p>
<p>假设<span class="arithmatex"><span class="MathJax_Preview">h_1,...,h_T</span><script type="math/tex">h_1,...,h_T</script></span>是Transformer编码器的输出向量，<span class="arithmatex"><span class="MathJax_Preview">z_1,...,z_T</span><script type="math/tex">z_1,...,z_T</script></span>是Transformer解码器的输入向量，那么，</p>
<ul>
<li>keys和values来自编码器：<span class="arithmatex"><span class="MathJax_Preview">k_i = Kh_i,v_i = Vh_i</span><script type="math/tex">k_i = Kh_i,v_i = Vh_i</script></span></li>
<li>queries来自解码器：<span class="arithmatex"><span class="MathJax_Preview">q_i=Qz_i</span><script type="math/tex">q_i=Qz_i</script></span>.</li>
</ul>
<p>假设<span class="arithmatex"><span class="MathJax_Preview">H = [h_1;...;h_T]</span><script type="math/tex">H = [h_1;...;h_T]</script></span>是编码器向量的拼接，<span class="arithmatex"><span class="MathJax_Preview">Z = [z_1;...;z_T]</span><script type="math/tex">Z = [z_1;...;z_T]</script></span>是解码器向量的拼接，那么输出可以定义为：
$$
output = softmax(ZQ(HK)^T)×HV
$$
<img alt="image-20220114145530907" src="../image/image-20220114145530907.png" /></p>

  <hr>
<div class="md-source-file">
  <small>
    
      Last update:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">May 25, 2022</span>
      
    
  </small>
</div>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../Lecture07/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Lecture07 机器翻译、seq2seq模型、注意力机制" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Lecture07 机器翻译、seq2seq模型、注意力机制
            </div>
          </div>
        </a>
      
      
        
        <a href="../Lecture10/" class="md-footer__link md-footer__link--next" aria-label="Next: Lecture10 预训练" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Lecture10 预训练
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2022 by Xiaoyu Chu
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.tabs.sticky"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.0bbba5b5.min.js"}</script>
    
    
      <script src="../assets/javascripts/bundle.649a939e.min.js"></script>
      
        <script src="../js/extra.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>