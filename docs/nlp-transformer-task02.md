---
title: Task02 å­¦ä¹ Attentioinå’ŒTransformer
toc: true
tags: [NLP, é¢„è®­ç»ƒæ¨¡å‹, ç»„é˜Ÿå­¦ä¹ , attention, transfomer, ç¬”è®°]
categories: [04 ç»„é˜Ÿå­¦ä¹ , 2021-09 åŸºäºtransformerçš„NLP]
thumbnail: "https://github.com/chuxiaoyu/blog_image/blob/master/nlp/00_bg.png?raw=true"
date: 2021-09-17 09:09:24
---
# Task02 å­¦ä¹ Attentioinå’ŒTransformer
## Attention
### seq2seq
seq2seqæ˜¯ä¸€ç§å¸¸è§çš„NLPæ¨¡å‹ç»“æ„ï¼Œå…¨ç§°æ˜¯ï¼šsequence to sequenceï¼Œç¿»è¯‘ä¸ºâ€œåºåˆ—åˆ°åºåˆ—â€ã€‚é¡¾åæ€ä¹‰ï¼šä»ä¸€ä¸ªæ–‡æœ¬åºåˆ—å¾—åˆ°ä¸€ä¸ªæ–°çš„æ–‡æœ¬åºåˆ—ã€‚å…¸å‹çš„ä»»åŠ¡æœ‰ï¼šæœºå™¨ç¿»è¯‘ä»»åŠ¡ï¼Œæ–‡æœ¬æ‘˜è¦ä»»åŠ¡ã€‚

seq2seqæ¨¡å‹ç”±ç¼–ç å™¨ï¼ˆencoderï¼‰å’Œè§£ç å™¨ï¼ˆdecoderï¼‰ç»„æˆï¼Œç¼–ç å™¨ç”¨æ¥åˆ†æè¾“å…¥åºåˆ—ï¼Œè§£ç å™¨ç”¨æ¥ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚ç¼–ç å™¨ä¼šå¤„ç†è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼ŒæŠŠè¿™äº›ä¿¡æ¯è½¬æ¢æˆä¸ºä¸€ä¸ªèƒŒæ™¯å‘é‡ï¼ˆcontext vectorï¼‰ã€‚å½“æˆ‘ä»¬å¤„ç†å®Œæ•´ä¸ªè¾“å…¥åºåˆ—åï¼Œç¼–ç å™¨æŠŠèƒŒæ™¯å‘é‡å‘é€ç»™è§£ç å™¨ï¼Œè§£ç å™¨é€šè¿‡èƒŒæ™¯å‘é‡ä¸­çš„ä¿¡æ¯ï¼Œé€ä¸ªå…ƒç´ è¾“å‡ºæ–°çš„åºåˆ—ã€‚

**åœ¨transformeræ¨¡å‹ä¹‹å‰ï¼Œseq2seqä¸­çš„ç¼–ç å™¨å’Œè§£ç å™¨ä¸€èˆ¬é‡‡ç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰**ï¼Œè™½ç„¶éå¸¸ç»å…¸ï¼Œä½†æ˜¯å±€é™æ€§ä¹Ÿéå¸¸å¤§ã€‚æœ€å¤§çš„å±€é™æ€§å°±åœ¨äºç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„å”¯ä¸€è”ç³»å°±æ˜¯ä¸€ä¸ªå›ºå®šé•¿åº¦çš„contextå‘é‡ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç¼–ç å™¨è¦å°†æ•´ä¸ªåºåˆ—çš„ä¿¡æ¯å‹ç¼©è¿›ä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡ä¸­ã€‚è¿™æ ·åšå­˜åœ¨ä¸¤ä¸ªå¼Šç«¯ï¼š
- è¯­ä¹‰å‘é‡å¯èƒ½æ— æ³•å®Œå…¨è¡¨ç¤ºæ•´ä¸ªåºåˆ—çš„ä¿¡æ¯
- å…ˆè¾“å…¥åˆ°ç½‘ç»œçš„å†…å®¹æºå¸¦çš„ä¿¡æ¯ä¼šè¢«åè¾“å…¥çš„ä¿¡æ¯è¦†ç›–æ‰ï¼Œè¾“å…¥åºåˆ—è¶Šé•¿ï¼Œè¿™ä¸ªç°è±¡å°±è¶Šä¸¥é‡

### Attention
ä¸ºäº†è§£å†³seq2seqæ¨¡å‹ä¸­çš„ä¸¤ä¸ªå¼Šç«¯ï¼ŒBahdanauç­‰äººåœ¨è®ºæ–‡ã€ŠNeural Machine Translation by Jointly Learning to Align and Translateã€‹ä¸­æå‡ºä½¿ç”¨Attentionæœºåˆ¶ï¼Œä½¿å¾—seq2seqæ¨¡å‹å¯ä»¥æœ‰åŒºåˆ†åº¦ã€æœ‰é‡ç‚¹åœ°å…³æ³¨è¾“å…¥åºåˆ—ï¼Œä»è€Œæå¤§åœ°æé«˜äº†æœºå™¨ç¿»è¯‘çš„è´¨é‡ã€‚

ä¸€ä¸ªæœ‰æ³¨æ„åŠ›æœºåˆ¶çš„seq2seqä¸ç»å…¸çš„seq2seqä¸»è¦æœ‰2ç‚¹ä¸åŒï¼š
1. é¦–å…ˆï¼Œç¼–ç å™¨ä¼šæŠŠæ›´å¤šçš„æ•°æ®ä¼ é€’ç»™è§£ç å™¨ã€‚ç¼–ç å™¨æŠŠæ‰€æœ‰æ—¶é—´æ­¥çš„ hidden stateï¼ˆéšè—å±‚çŠ¶æ€ï¼‰ä¼ é€’ç»™è§£ç å™¨ï¼Œè€Œä¸æ˜¯åªä¼ é€’æœ€åä¸€ä¸ª hidden stateï¼ˆéšè—å±‚çŠ¶æ€ï¼‰
2. æ³¨æ„åŠ›æ¨¡å‹çš„è§£ç å™¨åœ¨äº§ç”Ÿè¾“å‡ºä¹‹å‰ï¼Œåšäº†ä¸€ä¸ªé¢å¤–çš„attentionå¤„ç†

## Transformer
### æ¨¡å‹æ¶æ„
transformeråŸè®ºæ–‡çš„æ¶æ„å›¾ï¼š

<img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/trm_1.png?raw=true" width="400" alt="" align="center" />

ä¸€ä¸ªæ›´æ¸…æ™°çš„æ¶æ„å›¾ï¼š
<img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/trm_2.png?raw=true" width="600" alt="" align="center" />

ä»è¾“å…¥åˆ°è¾“å‡ºæ‹†å¼€çœ‹å°±æ˜¯ï¼š
- INPUTï¼šinput vector + position encoding
- ENCODERsï¼ˆÃ—6ï¼‰ï¼Œand each encoder includesï¼š
  - input
  - multi-head self-attention
  - residual connection&norm
  - full-connected network
  - residual connection&norm
  - output
- DECODERsï¼ˆÃ—6ï¼‰ï¼Œand each decoder includesï¼š
  - input 
  - Masked multihead self-attention
  - residual connection&norm
  - multi-head self-attention
  - residual connection&norm
  - full-connected network
  - residual connection&norm
  - output
- OUTPUTï¼š
  - output (decoder's)
  - linear layer
  - softmax layer
  - output


### æ¨¡å‹è¾“å…¥
#### è¯å‘é‡
å’Œå¸¸è§çš„NLPä»»åŠ¡ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆä¼šä½¿ç”¨è¯åµŒå…¥ç®—æ³•ï¼ˆembeddingï¼‰ï¼Œå°†è¾“å…¥æ–‡æœ¬åºåˆ—çš„æ¯ä¸ªè¯è½¬æ¢ä¸ºä¸€ä¸ªè¯å‘é‡ã€‚

#### ä½ç½®å‘é‡
Transformeræ¨¡å‹å¯¹æ¯ä¸ªè¾“å…¥çš„è¯å‘é‡éƒ½åŠ ä¸Šäº†ä¸€ä¸ªä½ç½®å‘é‡ã€‚è¿™äº›å‘é‡æœ‰åŠ©äºç¡®å®šæ¯ä¸ªå•è¯çš„ä½ç½®ç‰¹å¾ï¼Œæˆ–è€…å¥å­ä¸­ä¸åŒå•è¯ä¹‹é—´çš„è·ç¦»ç‰¹å¾ã€‚è¯å‘é‡åŠ ä¸Šä½ç½®å‘é‡èƒŒåçš„ç›´è§‰æ˜¯ï¼šå°†è¿™äº›è¡¨ç¤ºä½ç½®çš„å‘é‡æ·»åŠ åˆ°è¯å‘é‡ä¸­ï¼Œå¾—åˆ°çš„æ–°å‘é‡ï¼Œå¯ä»¥ä¸ºæ¨¡å‹æä¾›æ›´å¤šæœ‰æ„ä¹‰çš„ä¿¡æ¯ï¼Œæ¯”å¦‚è¯çš„ä½ç½®ï¼Œè¯ä¹‹é—´çš„è·ç¦»ç­‰ã€‚

*ï¼ˆç”Ÿæˆä½ç½®ç¼–ç å‘é‡çš„æ–¹æ³•æœ‰å¾ˆå¤šç§ï¼‰*

### ç¼–ç å™¨å’Œè§£ç å™¨
*æ³¨ï¼š1. ç¼–ç å™¨å’Œè§£ç å™¨ä¸­æœ‰ç›¸ä¼¼çš„æ¨¡å—å’Œç»“æ„ï¼Œæ‰€ä»¥åˆå¹¶åˆ°ä¸€èµ·ä»‹ç»ã€‚*
*2. æœ¬éƒ¨åˆ†æŒ‰ç…§æå®æ¯…è€å¸ˆçš„Attentionï¼ŒTransformeréƒ¨åˆ†çš„è¯¾ç¨‹PPTæ¥ï¼Œå› ä¸ºleeçš„è¯¾ç¨‹å¯¹æ–°æ‰‹æ›´å‹å¥½ã€‚*

#### Self-Attention
self-attentionå¯¹äºæ¯ä¸ªå‘é‡éƒ½ä¼šè€ƒè™‘æ•´ä¸ªsequenceçš„ä¿¡æ¯åè¾“å‡ºä¸€ä¸ªå‘é‡ï¼Œself-attentionç»“æ„å¦‚ä¸‹ï¼š
<img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/04_attention_1.png?raw=true" width="600" alt="" align="center" />
FCï¼šFully-connected network å…¨è¿æ¥ç½‘ç»œ
ai: è¾“å…¥å˜é‡ã€‚å¯èƒ½æ˜¯æ•´ä¸ªç½‘ç»œçš„è¾“å…¥ï¼Œä¹Ÿå¯èƒ½æ˜¯æŸä¸ªéšè—å±‚çš„è¾“å‡º
bi: è€ƒè™‘æ•´ä¸ªsequenceä¿¡æ¯åçš„è¾“å‡ºå˜é‡

çŸ©é˜µè®¡ç®—ï¼š
<img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/13_matrix_4.jpg?raw=true" width="300" alt="" align="center" />
ç›®æ ‡ï¼šæ ¹æ®è¾“å…¥å‘é‡çŸ©é˜µIï¼Œè®¡ç®—è¾“å‡ºå‘é‡çŸ©é˜µOã€‚çŸ©é˜µè¿ç®—è¿‡ç¨‹ï¼š
1. çŸ©é˜µIåˆ†åˆ«ä¹˜ä»¥Wq, Wk, Wvï¼ˆå‚æ•°çŸ©é˜µï¼Œéœ€è¦æ¨¡å‹è¿›è¡Œå­¦ä¹ ï¼‰ï¼Œå¾—åˆ°çŸ©é˜µQ, K, Vã€‚
2. çŸ©é˜µKçš„è½¬ç½®ä¹˜ä»¥Qï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡çŸ©é˜µAï¼Œå½’ä¸€åŒ–å¾—åˆ°çŸ©é˜µAâ€™ã€‚
3. çŸ©é˜µVä¹˜çŸ©é˜µAâ€˜ï¼Œå¾—åˆ°è¾“å‡ºå‘é‡çŸ©é˜µOã€‚

#### Multi Head Self-Attention
*ç®€å•åœ°è¯´ï¼Œå¤šäº†å‡ ç»„Qï¼ŒKï¼ŒVã€‚åœ¨Self-Attentionä¸­ï¼Œæˆ‘ä»¬æ˜¯ä½¿ç”¨ğ‘å»å¯»æ‰¾ä¸ä¹‹ç›¸å…³çš„ğ‘˜ï¼Œä½†æ˜¯è¿™ä¸ªç›¸å…³æ€§å¹¶ä¸ä¸€å®šæœ‰ä¸€ç§ã€‚é‚£å¤šç§ç›¸å…³æ€§ä½“ç°åˆ°è®¡ç®—æ–¹å¼ä¸Šå°±æ˜¯æœ‰å¤šä¸ªçŸ©é˜µğ‘ï¼Œä¸åŒçš„ğ‘è´Ÿè´£ä»£è¡¨ä¸åŒçš„ç›¸å…³æ€§ã€‚*

Transformer çš„è®ºæ–‡é€šè¿‡å¢åŠ å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆä¸€ç»„æ³¨æ„åŠ›ç§°ä¸ºä¸€ä¸ª attention headï¼‰ï¼Œè¿›ä¸€æ­¥å®Œå–„äº†Self-Attentionã€‚è¿™ç§æœºåˆ¶ä»å¦‚ä¸‹ä¸¤ä¸ªæ–¹é¢å¢å¼ºäº†attentionå±‚çš„èƒ½åŠ›ï¼š
- å®ƒæ‰©å±•äº†æ¨¡å‹å…³æ³¨ä¸åŒä½ç½®çš„èƒ½åŠ›ã€‚
- å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶èµ‹äºˆattentionå±‚å¤šä¸ªâ€œå­è¡¨ç¤ºç©ºé—´â€ã€‚

#### æ®‹å·®é“¾æ¥å’Œå½’ä¸€åŒ–
æ®‹å·®é“¾æ¥ï¼šä¸€ç§æŠŠinputå‘é‡å’Œoutputå‘é‡ç›´æ¥åŠ èµ·æ¥çš„æ¶æ„ã€‚
å½’ä¸€åŒ–ï¼šæŠŠæ•°æ®æ˜ å°„åˆ°0ï½1èŒƒå›´ä¹‹å†…å¤„ç†ã€‚

### æ¨¡å‹è¾“å‡º
#### çº¿æ€§å±‚å’Œsoftmax
Decoder æœ€ç»ˆçš„è¾“å‡ºæ˜¯ä¸€ä¸ªå‘é‡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯æµ®ç‚¹æ•°ã€‚æˆ‘ä»¬æ€ä¹ˆæŠŠè¿™ä¸ªå‘é‡è½¬æ¢ä¸ºå•è¯å‘¢ï¼Ÿè¿™æ˜¯çº¿æ€§å±‚å’Œsoftmaxå®Œæˆçš„ã€‚

çº¿æ€§å±‚å°±æ˜¯ä¸€ä¸ªæ™®é€šçš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œå¯ä»¥æŠŠè§£ç å™¨è¾“å‡ºçš„å‘é‡ï¼Œæ˜ å°„åˆ°ä¸€ä¸ªæ›´å¤§çš„å‘é‡ï¼Œè¿™ä¸ªå‘é‡ç§°ä¸º logits å‘é‡ï¼šå‡è®¾æˆ‘ä»¬çš„æ¨¡å‹æœ‰ 10000 ä¸ªè‹±è¯­å•è¯ï¼ˆæ¨¡å‹çš„è¾“å‡ºè¯æ±‡è¡¨ï¼‰ï¼Œæ­¤ logits å‘é‡ä¾¿ä¼šæœ‰ 10000 ä¸ªæ•°å­—ï¼Œæ¯ä¸ªæ•°è¡¨ç¤ºä¸€ä¸ªå•è¯çš„åˆ†æ•°ã€‚

ç„¶åï¼ŒSoftmax å±‚ä¼šæŠŠè¿™äº›åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆæŠŠæ‰€æœ‰çš„åˆ†æ•°è½¬æ¢ä¸ºæ­£æ•°ï¼Œå¹¶ä¸”åŠ èµ·æ¥ç­‰äº 1ï¼‰ã€‚ç„¶åé€‰æ‹©æœ€é«˜æ¦‚ç‡çš„é‚£ä¸ªæ•°å­—å¯¹åº”çš„è¯ï¼Œå°±æ˜¯è¿™ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºå•è¯ã€‚

#### æŸå¤±å‡½æ•°
Transformerè®­ç»ƒçš„æ—¶å€™ï¼Œéœ€è¦å°†è§£ç å™¨çš„è¾“å‡ºå’Œlabelä¸€åŒé€å…¥æŸå¤±å‡½æ•°ï¼Œä»¥è·å¾—lossï¼Œæœ€ç»ˆæ¨¡å‹æ ¹æ®lossè¿›è¡Œæ–¹å‘ä¼ æ’­ã€‚

åªè¦Transformerè§£ç å™¨é¢„æµ‹äº†ç»„æ¦‚ç‡ï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠè¿™ç»„æ¦‚ç‡å’Œæ­£ç¡®çš„è¾“å‡ºæ¦‚ç‡åšå¯¹æ¯”ï¼Œç„¶åä½¿ç”¨åå‘ä¼ æ’­æ¥è°ƒæ•´æ¨¡å‹çš„æƒé‡ï¼Œä½¿å¾—è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒæ›´åŠ æ¥è¿‘æ•´æ•°è¾“å‡ºã€‚

é‚£æˆ‘ä»¬è¦æ€ä¹ˆæ¯”è¾ƒä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒå‘¢ï¼Ÿï¼šæˆ‘ä»¬å¯ä»¥ç®€å•çš„ç”¨ä¸¤ç»„æ¦‚ç‡å‘é‡çš„çš„ç©ºé—´è·ç¦»ä½œä¸ºlossï¼ˆå‘é‡ç›¸å‡ï¼Œç„¶åæ±‚å¹³æ–¹å’Œï¼Œå†å¼€æ–¹ï¼‰ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥ä½¿ç”¨äº¤å‰ç†µ(cross-entropy)]å’ŒKL æ•£åº¦(Kullbackâ€“Leibler divergence)ã€‚

## å‚è€ƒèµ„æ–™

**ç†è®ºéƒ¨åˆ†**
[1] (å¼ºæ¨)æå®æ¯…2021æ˜¥æœºå™¨å­¦ä¹ è¯¾ç¨‹ [https://www.bilibili.com/video/BV1Wv411h7kN?from=search&seid=17090062977285779802&spm_id_from=333.337.0.0](https://www.bilibili.com/video/BV1Wv411h7kN?from=search&seid=17090062977285779802&spm_id_from=333.337.0.0)
[2] **åŸºäºtransformersçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨ï¼ˆæ¶µç›–äº†å›¾è§£ç³»åˆ—ã€annotated transformerã€huggingfaceï¼‰** [https://github.com/datawhalechina/learn-nlp-with-transformers](https://github.com/datawhalechina/learn-nlp-with-transformers)
[3] å›¾è§£transformer|The Illustrated Transformer [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)
[4] å›¾è§£seq2seq, attention|Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) [https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

**ä»£ç éƒ¨åˆ†**
[5] The Annotated Transformer [http://nlp.seas.harvard.edu//2018/04/03/attention.html](http://nlp.seas.harvard.edu//2018/04/03/attention.html)
[6] Huggingface/transformers [https://github.com/huggingface/transformers/blob/master/README_zh-hans.md](https://github.com/huggingface/transformers/blob/master/README_zh-hans.md)

**è®ºæ–‡éƒ¨åˆ†**
Attention is all "we" need.

**å…¶ä»–ä¸é”™çš„åšå®¢æˆ–æ•™ç¨‹**
[7] åŸºäºtransformersçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨--åœ¨çº¿é˜…è¯» [https://datawhalechina.github.io/learn-nlp-with-transformers/#/](https://datawhalechina.github.io/learn-nlp-with-transformers/#/)
[8] æå®æ¯…2021æ˜¥æœºå™¨å­¦ä¹ è¯¾ç¨‹ç¬”è®°â€”â€”è‡ªæ³¨æ„åŠ›æœºåˆ¶ [https://www.cnblogs.com/sykline/p/14730088.html](https://www.cnblogs.com/sykline/p/14730088.html)
[9] æå®æ¯…2021æ˜¥æœºå™¨å­¦ä¹ è¯¾ç¨‹ç¬”è®°â€”â€”Transformeræ¨¡å‹ [https://www.cnblogs.com/sykline/p/14785552.html](https://www.cnblogs.com/sykline/p/14785552.html)
[10] æå®æ¯…æœºå™¨å­¦ä¹ å­¦ä¹ ç¬”è®°â€”â€”è‡ªæ³¨æ„åŠ›æœºåˆ¶ [https://blog.csdn.net/p_memory/article/details/116271274](https://blog.csdn.net/p_memory/article/details/116271274)
[11] è½¦ä¸‡ç¿”-è‡ªç„¶è¯­è¨€å¤„ç†æ–°èŒƒå¼ï¼šåŸºäºé¢„è®­ç»ƒçš„æ–¹æ³•ã€è®²åº§+PPTã€‘ [https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true](https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true)
[12] è‹å‰‘æ—-ã€ŠAttention is All You Needã€‹æµ…è¯»ï¼ˆç®€ä»‹+ä»£ç ï¼‰[https://spaces.ac.cn/archives/4765](https://spaces.ac.cn/archives/4765)
