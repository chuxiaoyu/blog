---
title: Task06 BERTåº”ç”¨ã€è®­ç»ƒå’Œä¼˜åŒ–
toc: true
tags: [NLP, é¢„è®­ç»ƒæ¨¡å‹, ç»„é˜Ÿå­¦ä¹ , transfomer, BERT, ç¬”è®°]
categories: [04 ç»„é˜Ÿå­¦ä¹ , 2021-09 åŸºäºtransformerçš„NLP]
thumbnail: "https://github.com/chuxiaoyu/blog_image/blob/master/nlp/00_bg.png?raw=true"
date: 2021-09-24 15:18:42
---
# Task06 BERTåº”ç”¨ã€è®­ç»ƒå’Œä¼˜åŒ–
*è¯¥éƒ¨åˆ†çš„å†…å®¹ç¿»è¯‘è‡ªğŸ¤—HuggingFaceå®˜ç½‘æ•™ç¨‹ç¬¬1éƒ¨åˆ†ï¼ˆ1-4ç« ï¼‰ï¼Œè§ [https://huggingface.co/course/chapter1](https://huggingface.co/course/chapter1)ã€‚è¯¥ç³»åˆ—æ•™ç¨‹ç”±3å¤§éƒ¨åˆ†å…±12ç« ç»„æˆï¼ˆå¦‚å›¾ï¼‰ï¼Œå…¶ä¸­ç¬¬1éƒ¨åˆ†ä»‹ç»transformersåº“çš„ä¸»è¦æ¦‚å¿µã€æ¨¡å‹çš„å·¥ä½œåŸç†å’Œä½¿ç”¨æ–¹æ³•ã€æ€æ ·åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå¾®è°ƒç­‰å†…å®¹ã€‚*
<img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_1.png?raw=true" width="500" alt="" align="center" />

## ç¯å¢ƒæ­å»º
ç®€å•çš„è¯´ï¼Œæœ‰ä¸¤ç§å¯ä»¥è·‘æ¨¡å‹ä»£ç çš„æ–¹å¼ï¼š
1. Google Colab
2. æœ¬åœ°è™šæ‹Ÿç¯å¢ƒ `pip install transformers`

è¯¦è§ [https://huggingface.co/course/chapter0?fw=pt](https://huggingface.co/course/chapter0?fw=pt)

## Transformeræ¨¡å‹æ¦‚è¿°

### Transformers, å¯ä»¥åšä»€ä¹ˆï¼Ÿ

ç›®å‰å¯ç”¨çš„ä¸€äº›pipelineæ˜¯ï¼š
- feature-extraction è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º
- fill-mask å®Œå½¢å¡«ç©º
- ner (named entity recognition) å‘½åå®ä½“è¯†åˆ«
- question-answering é—®ç­”
- sentiment-analysis æƒ…æ„Ÿåˆ†æ
- summarization æ‘˜è¦ç”Ÿæˆ
- text-generation æ–‡æœ¬ç”Ÿæˆ
- translation ç¿»è¯‘
- zero-shot-classification é›¶æ ·æœ¬åˆ†ç±»

*pipeline: ç›´è¯‘ç®¡é“/æµæ°´çº¿ï¼Œå¯ä»¥ç†è§£ä¸ºæµç¨‹ã€‚*

### Transformers, å¦‚ä½•å·¥ä½œï¼Ÿ

#### Transformerç®€å²
Transformer æ¶æ„äº 2017 å¹´ 6 æœˆæ¨å‡ºã€‚åŸå§‹ç ”ç©¶çš„é‡ç‚¹æ˜¯ç¿»è¯‘ä»»åŠ¡ã€‚éšåæ¨å‡ºäº†å‡ ä¸ªæœ‰å½±å“åŠ›çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š
- 2018 å¹´ 6 æœˆï¼šGPTï¼Œç¬¬ä¸€ä¸ªé¢„è®­ç»ƒçš„ Transformer æ¨¡å‹ï¼Œç”¨äºå„ç§ NLP ä»»åŠ¡çš„å¾®è°ƒå¹¶è·å¾—æœ€å…ˆè¿›çš„ç»“æœ
- 2018 å¹´ 10 æœˆï¼šBERTï¼Œå¦ä¸€ä¸ªå¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¨åœ¨ç”Ÿæˆæ›´å¥½çš„å¥å­æ‘˜è¦
- 2019 å¹´ 2 æœˆï¼šGPT-2ï¼ŒGPT çš„æ”¹è¿›ï¼ˆå’Œæ›´å¤§ï¼‰ç‰ˆæœ¬
- 2019 å¹´ 10 æœˆï¼šDistilBERTï¼ŒBERT çš„è’¸é¦ç‰ˆæœ¬ï¼Œé€Ÿåº¦æé«˜ 60%ï¼Œå†…å­˜å‡è½» 40%ï¼Œä½†ä»ä¿ç•™ BERT 97% çš„æ€§èƒ½
- 2019 å¹´ 10 æœˆï¼šBART å’Œ T5ï¼Œä¸¤ä¸ªä½¿ç”¨ä¸åŸå§‹ Transformer æ¨¡å‹ç›¸åŒæ¶æ„çš„å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼ˆç¬¬ä¸€ä¸ªè¿™æ ·åšï¼‰
- 2020 å¹´ 5 æœˆï¼ŒGPT-3ï¼ŒGPT-2 çš„æ›´å¤§ç‰ˆæœ¬ï¼Œæ— éœ€å¾®è°ƒå³å¯åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼ˆç§°ä¸ºé›¶æ ·æœ¬å­¦ä¹ zero-shot learningï¼‰

å¤§ä½“ä¸Šï¼Œå®ƒä»¬å¯ä»¥åˆ†ä¸ºä¸‰ç±»ï¼š
- GPTç±»ï¼ˆåˆç§°ä¸ºè‡ªå›å½’ Transformer æ¨¡å‹ï¼‰ï¼šåªä½¿ç”¨transformer-decoderéƒ¨åˆ†
- BERTç±»ï¼ˆåˆç§°ä¸ºè‡ªç¼–ç  Transformer æ¨¡å‹ï¼‰ï¼šåªä½¿ç”¨transformer-encoderéƒ¨åˆ†
- BART/T5ç±»ï¼ˆåˆç§°ä¸ºåºåˆ—åˆ°åºåˆ— Transformer æ¨¡å‹ï¼‰ï¼šä½¿ç”¨Transformer-encoder-decoderéƒ¨åˆ†

å®ƒä»¬çš„åˆ†ç±»ã€å…·ä½“æ¨¡å‹ã€ä¸»è¦åº”ç”¨ä»»åŠ¡å¦‚ä¸‹ï¼š
<img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_2.jpg?raw=true" width="800" alt="" align="center" />


å…¶ä»–éœ€è¦çŸ¥é“çš„ï¼š
- Transformersæ˜¯è¯­è¨€æ¨¡å‹
- Transformersæ˜¯å¤§æ¨¡å‹
- Transformersçš„åº”ç”¨é€šè¿‡é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªè¿‡ç¨‹

#### åè¯è§£é‡Šï¼šArchitectureå’ŒCheckpoints
**Architecture/æ¶æ„**ï¼šå®šä¹‰äº†æ¨¡å‹çš„åŸºæœ¬ç»“æ„å’ŒåŸºæœ¬è¿ç®—ã€‚
**Checkpoints/æ£€æŸ¥ç‚¹**ï¼šæ¨¡å‹çš„æŸä¸ªè®­ç»ƒçŠ¶æ€ï¼ŒåŠ è½½æ­¤checkpointä¼šåŠ è½½æ­¤æ—¶çš„æƒé‡ã€‚è®­ç»ƒæ—¶å¯ä»¥é€‰æ‹©è‡ªåŠ¨ä¿å­˜checkpointã€‚æ¨¡å‹åœ¨è®­ç»ƒæ—¶å¯ä»¥è®¾ç½®è‡ªåŠ¨ä¿å­˜äºæŸä¸ªæ—¶é—´ç‚¹ï¼ˆæ¯”å¦‚æ¨¡å‹è®­ç»ƒäº†ä¸€è½®epochï¼Œæ›´æ–°äº†å‚æ•°ï¼Œå°†è¿™ä¸ªçŠ¶æ€çš„æ¨¡å‹ä¿å­˜ä¸‹æ¥ï¼Œä¸ºä¸€ä¸ªcheckpointã€‚ï¼‰ æ‰€ä»¥æ¯ä¸ªcheckpointå¯¹åº”æ¨¡å‹çš„ä¸€ä¸ªçŠ¶æ€ï¼Œä¸€ç»„æƒé‡ã€‚

## ä½¿ç”¨Transformers

### 3ä¸ªå¤„ç†æ­¥éª¤

å°†ä¸€äº›æ–‡æœ¬ä¼ é€’åˆ°pipelineæ—¶æ¶‰åŠ3ä¸ªä¸»è¦æ­¥éª¤ï¼š
1. æ–‡æœ¬è¢«é¢„å¤„ç†ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼ã€‚
2. é¢„å¤„ç†åçš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹ã€‚
3. æ¨¡å‹çš„é¢„æµ‹ç»“æœè¢«åå¤„ç†ä¸ºäººç±»å¯ä»¥ç†è§£çš„æ ¼å¼ã€‚

Pipelineå°†3ä¸ªæ­¥éª¤ç»„åˆåœ¨ä¸€èµ·ï¼šé¢„å¤„ç†/Tokenizerã€é€šè¿‡æ¨¡å‹ä¼ é€’è¾“å…¥/Modelå’Œåå¤„ç†/Post-Processingï¼š
<img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_3.png?raw=true" width="800" alt="" align="center" />

### Tokenizer/é¢„å¤„ç†
Tokenizerçš„ä½œç”¨ï¼š
- å°†è¾“å…¥æ‹†åˆ†ä¸ºç§°ä¸ºtokençš„å•è¯ã€å­è¯/subwordæˆ–ç¬¦å·/symbolsï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ï¼‰
- å°†æ¯ä¸ªtokenæ˜ å°„åˆ°ä¸€ä¸ªæ•´æ•°
- æ·»åŠ å¯èƒ½å¯¹æ¨¡å‹æœ‰ç”¨çš„å…¶ä»–è¾“å…¥

### Going Through Models/ç©¿è¿‡æ¨¡å‹

#### æ¨¡å‹å®ä¾‹åŒ–
```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```
åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬ä¸‹è½½äº†åœ¨pipelineä¸­ä½¿ç”¨çš„ç›¸åŒæ£€æŸ¥ç‚¹ï¼ˆå®é™…ä¸Šå·²ç»ç¼“å­˜ï¼‰å¹¶å°†æ¨¡å‹å®ä¾‹åŒ–ã€‚

#### æ¨¡å‹çš„è¾“å‡ºï¼šé«˜ç»´å‘é‡
æ¨¡å‹çš„è¾“å‡ºå‘é‡é€šå¸¸æœ‰ä¸‰ä¸ªç»´åº¦ï¼š
- Batch size: ä¸€æ¬¡å¤„ç†çš„åºåˆ—æ•°
- Sequence length: åºåˆ—å‘é‡çš„é•¿åº¦
- Hidden size: æ¯ä¸ªæ¨¡å‹è¾“å…¥å¤„ç†åçš„å‘é‡ç»´åº¦ï¼ˆhidden state vectorï¼‰

#### Model Headsï¼šä¸ºäº†å¤„ç†ä¸åŒçš„ä»»åŠ¡
Model heads:å°†éšè—çŠ¶æ€çš„é«˜ç»´å‘é‡ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å®ƒä»¬æŠ•å½±åˆ°ä¸åŒçš„ç»´åº¦ä¸Šã€‚å®ƒä»¬é€šå¸¸ç”±ä¸€ä¸ªæˆ–å‡ ä¸ªçº¿æ€§å±‚ç»„æˆã€‚
<img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_4.png?raw=true" width="800" alt="è¿™ä¸ªå›¾è¡¨ç¤ºäº†Pipelineç¬¬äºŒæ­¥åœ¨ç»è¿‡æ¨¡å‹æ—¶å‘ç”Ÿçš„äº‹æƒ…ã€‚" align="center" />
å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œç´«è‰²ä»£è¡¨å‘é‡ï¼Œç²‰è‰²ä»£è¡¨æ¨¡ç»„ï¼ŒEmbeddings+layersè¡¨ç¤ºTransformerçš„æ¶æ„ï¼Œç»è¿‡è¿™å±‚æ¶æ„åçš„è¾“å‡ºé€å…¥Model Headè¿›è¡Œå¤„ç†ï¼Œä»è€Œåº”ç”¨åˆ°ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚
ğŸ¤— Transformers ä¸­æœ‰è®¸å¤šä¸åŒçš„Headæ¶æ„å¯ç”¨ï¼Œæ¯ä¸€ç§æ¶æ„éƒ½å›´ç»•ç€å¤„ç†ç‰¹å®šä»»åŠ¡è€Œè®¾è®¡ã€‚ ä¸‹é¢åˆ—ä¸¾äº†éƒ¨åˆ†Model headsï¼š

- *Model (retrieve the hidden states)
- *ForCausalLM
- *ForMaskedLM
- *ForMultipleChoice
- *ForQuestionAnswering
- *ForSequenceClassification
- *ForTokenClassification
- and others ğŸ¤—

### Post-processing/åå¤„ç†
ä»æ¨¡å‹ä¸­è·å¾—çš„ä½œä¸ºè¾“å‡ºçš„å€¼æœ¬èº«å¹¶ä¸ä¸€å®šæœ‰æ„ä¹‰ã€‚è¦è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œå®ƒä»¬éœ€è¦ç»è¿‡ä¸€ä¸ª SoftMax å±‚ã€‚

## å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹

#### æ•°æ®å¤„ç†
åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨MRPCï¼ˆMicrosoft Research Praphrase Corpusï¼‰æ•°æ®é›†ä½œä¸ºç¤ºä¾‹ã€‚è¯¥DataSetç”±5,801å¯¹å¥å­ç»„æˆï¼Œæ ‡ç­¾æŒ‡ç¤ºå®ƒä»¬æ˜¯å¦æ˜¯åŒä¹‰å¥ï¼ˆå³ä¸¤ä¸ªå¥å­æ˜¯å¦è¡¨ç¤ºç›¸åŒçš„æ„æ€ï¼‰ã€‚ æˆ‘ä»¬é€‰æ‹©å®ƒæ˜¯å› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå°å‹æ•°æ®é›†ï¼Œå› æ­¤å¯ä»¥è½»æ¾è®­ç»ƒã€‚

#### ä»Hubä¸ŠåŠ è½½æ•°æ®é›†
Hubä¸ä»…åŒ…å«æ¨¡å‹ï¼Œè¿˜å«æœ‰å¤šç§è¯­è¨€çš„datasetsã€‚
ä¾‹å¦‚ï¼ŒMRPCæ•°æ®é›†æ˜¯æ„æˆ GLUE benchmarkçš„ 10 ä¸ªæ•°æ®é›†ä¹‹ä¸€ã€‚GLUEï¼ˆGeneral Language Understanding Evaluationï¼‰æ˜¯ä¸€ä¸ªå¤šä»»åŠ¡çš„è‡ªç„¶è¯­è¨€ç†è§£åŸºå‡†å’Œåˆ†æå¹³å°ã€‚GLUEåŒ…å«ä¹é¡¹NLUä»»åŠ¡ï¼Œè¯­è¨€å‡ä¸ºè‹±è¯­ã€‚GLUEä¹é¡¹ä»»åŠ¡æ¶‰åŠåˆ°è‡ªç„¶è¯­è¨€æ¨æ–­ã€æ–‡æœ¬è•´å«ã€æƒ…æ„Ÿåˆ†æã€è¯­ä¹‰ç›¸ä¼¼ç­‰å¤šä¸ªä»»åŠ¡ã€‚åƒBERTã€XLNetã€RoBERTaã€ERINEã€T5ç­‰çŸ¥åæ¨¡å‹éƒ½ä¼šåœ¨æ­¤åŸºå‡†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚

ğŸ¤— Datasetsåº“æä¾›äº†ä¸€ä¸ªéå¸¸ç®€å•çš„å‘½ä»¤æ¥ä¸‹è½½å’Œç¼“å­˜Hubä¸Šçš„datasetã€‚ æˆ‘ä»¬å¯ä»¥åƒè¿™æ ·ä¸‹è½½ MRPC æ•°æ®é›†ï¼š
```python
>>> from datasets import load_dataset

>>> raw_datasets = load_dataset("glue", "mrpc")
>>> raw_datasets
```
è¾“å‡ºå¦‚ä¸‹ï¼š
```python
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```
è¿™æ ·å°±å¾—åˆ°ä¸€ä¸ªDatasetDictå¯¹è±¡ï¼ŒåŒ…å«è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œè®­ç»ƒé›†ä¸­æœ‰3,668 ä¸ªå¥å­å¯¹ï¼ŒéªŒè¯é›†ä¸­æœ‰408å¯¹ï¼Œæµ‹è¯•é›†ä¸­æœ‰1,725 å¯¹ã€‚æ¯ä¸ªå¥å­å¯¹åŒ…å«å››ä¸ªå­—æ®µï¼š'sentence1', 'sentence2', 'label'å’Œ 'idx'ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ç´¢å¼•è®¿é—®raw_datasets çš„å¥å­å¯¹ï¼š
```python
>>> raw_train_dataset = raw_datasets["train"]
>>> raw_train_dataset[0]
```
è¾“å‡ºå¦‚ä¸‹ï¼š
```python
{'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .', 
'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .', 
'label': 1, 
'idx': 0}
```
æˆ‘ä»¬å¯ä»¥é€šè¿‡featuresè·å¾—æ•°æ®é›†çš„å­—æ®µç±»å‹ï¼š
```python
>>> raw_train_dataset.features
```
è¾“å‡ºå¦‚ä¸‹ï¼š
```python
{'sentence1': Value(dtype='string', id=None), 
'sentence2': Value(dtype='string', id=None), 
'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None), 
'idx': Value(dtype='int32', id=None)}
```


>TIPSï¼š
>1. æ²¡æœ‰æ•°æ®é›†çš„è¯é¦–å…ˆå®‰è£…ä¸€ä¸‹ï¼š`pip install datasets`
>2. è¿™é‡Œå¾ˆå®¹æ˜“å‡ºç°è¿æ¥é”™è¯¯ï¼Œè§£å†³æ–¹æ³•å¦‚ä¸‹ï¼š[https://blog.csdn.net/qq_20849045/article/details/117462846?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link](https://blog.csdn.net/qq_20849045/article/details/117462846?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link)


#### æ•°æ®é›†é¢„å¤„ç†
é€šè¿‡æ•°æ®é›†é¢„å¤„ç†ï¼Œæˆ‘ä»¬å°†æ–‡æœ¬è½¬æ¢æˆæ¨¡å‹èƒ½ç†è§£çš„å‘é‡ã€‚è¿™ä¸ªè¿‡ç¨‹é€šè¿‡Tokenizerå®ç°ï¼š
```python
>>> from transformers import AutoTokenizer

>>> checkpoint = "bert-base-uncased"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
>>> tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

ï¼ˆTODOï¼‰

### ä½¿ç”¨Trainer APIå¾®è°ƒä¸€ä¸ªæ¨¡å‹

#### è®­ç»ƒ
#### è¯„ä¼°å‡½æ•°

## è¡¥å……éƒ¨åˆ†
### ä¸ºä»€ä¹ˆ4ä¸­ç”¨Traineræ¥å¾®è°ƒæ¨¡å‹ï¼Ÿ
### Training Argumentsä¸»è¦å‚æ•°
### ä¸åŒæ¨¡å‹çš„åŠ è½½æ–¹å¼
### Dynamic Paddingâ€”â€”åŠ¨æ€å¡«å……æŠ€æœ¯

## å‚è€ƒèµ„æ–™
- åŸºäºtransformersçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨--åœ¨çº¿é˜…è¯» [https://datawhalechina.github.io/learn-nlp-with-transformers/#/](https://datawhalechina.github.io/learn-nlp-with-transformers/#/)
- Huggingfaceå®˜æ–¹æ•™ç¨‹ [https://huggingface.co/course/chapter1](https://huggingface.co/course/chapter1)