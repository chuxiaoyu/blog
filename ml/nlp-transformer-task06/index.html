<!DOCTYPE html>
<html lang="en">
<head>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <meta name="generator" content="mkdocs-1.4.3, mkdocs-terminal-4.4.0">
     
     
    <link rel="icon" type="image/png" sizes="192x192" href="../../img/android-chrome-192x192.png" />
<link rel="icon" type="image/png" sizes="512x512" href="../../img/android-chrome-512x512.png" />
<link rel="apple-touch-icon" sizes="180x180" href="../../img/apple-touch-icon.png" />
<link rel="shortcut icon" type="image/png" sizes="48x48" href="../../img/favicon.ico" />
<link rel="icon" type="image/png" sizes="16x16" href="../../img/favicon-16x16.png" />
<link rel="icon" type="image/png" sizes="32x32" href="../../img/favicon-32x32.png" />


    
 
<title>Task06 BERT应用、训练和优化 - 起 身 獨 立 向 荒 原 。</title>


<link href="../../css/fontawesome/css/fontawesome.min.css" rel="stylesheet">
<link href="../../css/fontawesome/css/solid.min.css" rel="stylesheet">
<link href="../../css/normalize.css" rel="stylesheet">
<link href="../../css/terminal.css" rel="stylesheet">
<link href="../../css/theme.css" rel="stylesheet">
<link href="../../css/theme.tile_grid.css" rel="stylesheet">
<link href="../../css/theme.footer.css" rel="stylesheet">
<!-- default color palette -->
<link href="../../css/palettes/default.css" rel="stylesheet">

<!-- page layout -->
<style>
/* initially set page layout to a one column grid */
.terminal-mkdocs-main-grid {
    display: grid;
    grid-column-gap: 1.4em;
    grid-template-columns: auto;
    grid-template-rows: auto;
}

/*  
*   when side navigation is not hidden, use a two column grid.  
*   if the screen is too narrow, fall back to the initial one column grid layout.
*   in this case the main content will be placed under the navigation panel. 
*/
@media only screen and (min-width: 70em) {
    .terminal-mkdocs-main-grid {
        grid-template-columns: 4fr 9fr;
    }
}</style>

<!-- link underline override -->
<style>
#terminal-mkdocs-main-content a:not(.headerlink){
    text-decoration: none;
}
</style>

     
    
    

    
    <!-- search css support -->
<link href="../../css/search/bootstrap-modal.css" rel="stylesheet">
<!-- search scripts -->
<script>
    var base_url = "../..",
    shortcuts = "{}";
</script>
<script src="../../js/jquery/jquery-1.10.1.min.js" defer></script>
<script src="../../js/bootstrap/bootstrap.min.js" defer></script>
<script src="../../js/mkdocs/base.js" defer></script>
    
    
    
    
    <script src="../../js/extra.js"></script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    
    <script src="../../search/main.js"></script>
    

    
</head>

<body class="terminal"><div class="container">
    <div class="terminal-nav">
        <header class="terminal-logo">
            <div id="mkdocs-terminal-site-name" class="logo terminal-prompt"><a href="/" class="no-style">起 身 獨 立 向 荒 原 。</a></div>
        </header>
        
        <nav class="terminal-menu">
            
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                
                
                <li property="itemListElement" typeof="ListItem">
                    <a href="../.." class="menu-item " property="item" typeof="WebPage">
                        <span property="name">Home</span>
                    </a>
                    <meta property="position" content="0">
                </li>
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                    
                    


<li property="itemListElement" typeof="ListItem">
    <a href="#" class="menu-item" data-toggle="modal" data-target="#mkdocs_search_modal" property="item" typeof="SearchAction">
        <i aria-hidden="true" class="fa fa-search"></i> <span property="name">Search</span>
    </a>
    <meta property="position" content="1">
</li>
                    
            </ul>
            
        </nav>
    </div>
</div>
        
    <div class="container">
        <div class="terminal-mkdocs-main-grid"><aside id="terminal-mkdocs-side-panel"><nav>
  
    <ul class="terminal-mkdocs-side-nav-items">
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../..">Home</a>
        
    
    
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">Deep Learning for Time Series Forecasting</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../time-series/01-time-series-types/">Taxonomy of Time Series Forecasting</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../time-series/02-time-series-supervised-learning/">Transforming Time Series to Supervised Learning</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">CS224n自然语言处理</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../Lecture07/">Lecture07 机器翻译、seq2seq模型、注意力机制</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../Lecture09/">Lecture09 自注意力模型、Transformers</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../Lecture10/">Lecture10 预训练</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../Lecture11/">Lecture11 问答系统</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item--active terminal-mkdocs-side-nav-section-no-index">基于transformers的NLP</span>
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task01/">Task01 NLP学习概览</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task02/">Task02 学习Attentioin和Transformer</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task03/">Task03 学习BERT</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task04/">Task04 学习GPT</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task05/">Task05 编写BERT模型</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        <span class="

    terminal-mkdocs-side-nav-item--active">Task06 BERT应用、训练和优化</span>
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task07/">Task07 使用Transformers解决文本分类任务</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">深入浅出PyTorch</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../pytorch-chap01-02/">Chapter01-02 PyTorch的简介和安装、PyTorch基础知识</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../pytorch-chap03/">Chapter03 PyTorch的主要组成模块</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../pytorch-chap04/">Chapter04 PyTorch基础实战——FashionMNIST图像分类</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">Nand2Tetris 计算机系统要素</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/nand2tetris_part_1/">Nand2Tetris Part1 (Hardware)</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/nand2tetris_part_2/">Nand2Tetris Part2 (Software)</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">ML Compilation 机器学习编译</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-01/">01 机器学习编译概述</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-02/">02 张量程序抽象</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-03/">03 张量程序抽象案例研究：TensorIR</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-04/">04 端到端模型整合</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-05/">05 自动程序优化</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">Resources</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../resource/ml-system-learning-list/">ML System Learning List</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../resource/open-source-projects/">Open Source Projects</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../resource/post-types-of-paper/">Types of CS Paper</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">Philosophy</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../philosophy/40-philosophers/">A Quick Introduction to 40+ Philosophers</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
    </ul>
  
</nav><hr>
<nav>
    <ul>
        <li><a href="#task06-bert">Task06 BERT应用、训练和优化</a></li>
        <li><a href="#_1">环境搭建</a></li>
        <li><a href="#transformer">Transformer模型概述</a></li>
        <li><a href="#transformers">Transformers, 可以做什么？</a></li>
        <li><a href="#transformers_1">Transformers, 如何工作？</a></li>
        <li><a href="#transformer_1">Transformer简史</a></li>
        <li><a href="#architecturecheckpoints">名词解释：Architecture和Checkpoints</a></li>
        <li><a href="#transformers_2">使用Transformers</a></li>
        <li><a href="#3">3个处理步骤</a></li>
        <li><a href="#tokenizer">Tokenizer/预处理</a></li>
        <li><a href="#going-through-models">Going Through Models/穿过模型</a></li>
        <li><a href="#_2">模型实例化</a></li>
        <li><a href="#_3">模型的输出：高维向量</a></li>
        <li><a href="#model-heads">Model Heads：为了处理不同的任务</a></li>
        <li><a href="#post-processing">Post-processing/后处理</a></li>
        <li><a href="#_4">微调一个预训练模型</a></li>
        <li><a href="#_5">数据处理</a></li>
        <li><a href="#hub">从Hub上加载数据集</a></li>
        <li><a href="#_6">数据集预处理</a></li>
        <li><a href="#trainer-api">使用Trainer API微调一个模型</a></li>
        <li><a href="#_7">训练</a></li>
        <li><a href="#_8">评估函数</a></li>
        <li><a href="#_9">补充部分</a></li>
        <li><a href="#4trainer">为什么4中用Trainer来微调模型？</a></li>
        <li><a href="#training-arguments">Training Arguments主要参数</a></li>
        <li><a href="#_10">不同模型的加载方式</a></li>
        <li><a href="#dynamic-padding">Dynamic Padding——动态填充技术</a></li>
        <li><a href="#_11">参考资料</a></li>
        
    </ul>
</nav>
</aside>
            <main id="terminal-mkdocs-main-content">
    
    
    
    
    

<section id="mkdocs-terminal-content">
    <h6 id="task06-bert">Task06 BERT应用、训练和优化<a class="headerlink" href="#task06-bert" title="Permanent link">&para;</a></h6>
<p><em>该部分的内容翻译自🤗HuggingFace官网教程第1部分（1-4章），见 <a href="https://huggingface.co/course/chapter1">https://huggingface.co/course/chapter1</a>。该系列教程由3大部分共12章组成（如图），其中第1部分介绍transformers库的主要概念、模型的工作原理和使用方法、怎样在特定数据集上微调等内容。</em>
<img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_1.png?raw=true" width="500" alt="" align="center" /></p>
<h6 id="_1">环境搭建<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h6>
<p>简单的说，有两种可以跑模型代码的方式：
1. Google Colab
2. 本地虚拟环境 <code>pip install transformers</code></p>
<p>详见 <a href="https://huggingface.co/course/chapter0?fw=pt">https://huggingface.co/course/chapter0?fw=pt</a></p>
<h6 id="transformer">Transformer模型概述<a class="headerlink" href="#transformer" title="Permanent link">&para;</a></h6>
<h6 id="transformers">Transformers, 可以做什么？<a class="headerlink" href="#transformers" title="Permanent link">&para;</a></h6>
<p>目前可用的一些pipeline是：
- feature-extraction 获取文本的向量表示
- fill-mask 完形填空
- ner (named entity recognition) 命名实体识别
- question-answering 问答
- sentiment-analysis 情感分析
- summarization 摘要生成
- text-generation 文本生成
- translation 翻译
- zero-shot-classification 零样本分类</p>
<p><em>pipeline: 直译管道/流水线，可以理解为流程。</em></p>
<h6 id="transformers_1">Transformers, 如何工作？<a class="headerlink" href="#transformers_1" title="Permanent link">&para;</a></h6>
<h6 id="transformer_1">Transformer简史<a class="headerlink" href="#transformer_1" title="Permanent link">&para;</a></h6>
<p>Transformer 架构于 2017 年 6 月推出。原始研究的重点是翻译任务。随后推出了几个有影响力的模型，包括：
- 2018 年 6 月：GPT，第一个预训练的 Transformer 模型，用于各种 NLP 任务的微调并获得最先进的结果
- 2018 年 10 月：BERT，另一个大型预训练模型，该模型旨在生成更好的句子摘要
- 2019 年 2 月：GPT-2，GPT 的改进（和更大）版本
- 2019 年 10 月：DistilBERT，BERT 的蒸馏版本，速度提高 60%，内存减轻 40%，但仍保留 BERT 97% 的性能
- 2019 年 10 月：BART 和 T5，两个使用与原始 Transformer 模型相同架构的大型预训练模型（第一个这样做）
- 2020 年 5 月，GPT-3，GPT-2 的更大版本，无需微调即可在各种任务上表现良好（称为零样本学习zero-shot learning）</p>
<p>大体上，它们可以分为三类：
- GPT类（又称为自回归 Transformer 模型）：只使用transformer-decoder部分
- BERT类（又称为自编码 Transformer 模型）：只使用transformer-encoder部分
- BART/T5类（又称为序列到序列 Transformer 模型）：使用Transformer-encoder-decoder部分</p>
<p>它们的分类、具体模型、主要应用任务如下：
<img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_2.jpg?raw=true" width="800" alt="" align="center" /></p>
<p>其他需要知道的：
- Transformers是语言模型
- Transformers是大模型
- Transformers的应用通过预训练和微调两个过程</p>
<h6 id="architecturecheckpoints">名词解释：Architecture和Checkpoints<a class="headerlink" href="#architecturecheckpoints" title="Permanent link">&para;</a></h6>
<p><strong>Architecture/架构</strong>：定义了模型的基本结构和基本运算。
<strong>Checkpoints/检查点</strong>：模型的某个训练状态，加载此checkpoint会加载此时的权重。训练时可以选择自动保存checkpoint。模型在训练时可以设置自动保存于某个时间点（比如模型训练了一轮epoch，更新了参数，将这个状态的模型保存下来，为一个checkpoint。） 所以每个checkpoint对应模型的一个状态，一组权重。</p>
<h6 id="transformers_2">使用Transformers<a class="headerlink" href="#transformers_2" title="Permanent link">&para;</a></h6>
<h6 id="3">3个处理步骤<a class="headerlink" href="#3" title="Permanent link">&para;</a></h6>
<p>将一些文本传递到pipeline时涉及3个主要步骤：
1. 文本被预处理为模型可以理解的格式。
2. 预处理后的输入传递给模型。
3. 模型的预测结果被后处理为人类可以理解的格式。</p>
<p>Pipeline将3个步骤组合在一起：预处理/Tokenizer、通过模型传递输入/Model和后处理/Post-Processing：
<img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_3.png?raw=true" width="800" alt="" align="center" /></p>
<h6 id="tokenizer">Tokenizer/预处理<a class="headerlink" href="#tokenizer" title="Permanent link">&para;</a></h6>
<p>Tokenizer的作用：
- 将输入拆分为称为token的单词、子词/subword或符号/symbols（如标点符号）
- 将每个token映射到一个整数
- 添加可能对模型有用的其他输入</p>
<h6 id="going-through-models">Going Through Models/穿过模型<a class="headerlink" href="#going-through-models" title="Permanent link">&para;</a></h6>
<h6 id="_2">模型实例化<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h6>
<p><div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></code></pre></div>
在这段代码中，我们下载了在pipeline中使用的相同检查点（实际上已经缓存）并将模型实例化。</p>
<h6 id="_3">模型的输出：高维向量<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h6>
<p>模型的输出向量通常有三个维度：
- Batch size: 一次处理的序列数
- Sequence length: 序列向量的长度
- Hidden size: 每个模型输入处理后的向量维度（hidden state vector）</p>
<h6 id="model-heads">Model Heads：为了处理不同的任务<a class="headerlink" href="#model-heads" title="Permanent link">&para;</a></h6>
<p>Model heads:将隐藏状态的高维向量作为输入，并将它们投影到不同的维度上。它们通常由一个或几个线性层组成。
<img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_4.png?raw=true" width="800" alt="这个图表示了Pipeline第二步在经过模型时发生的事情。" align="center" />
如上图所示，紫色代表向量，粉色代表模组，Embeddings+layers表示Transformer的架构，经过这层架构后的输出送入Model Head进行处理，从而应用到不同的下游任务。
🤗 Transformers 中有许多不同的Head架构可用，每一种架构都围绕着处理特定任务而设计。 下面列举了部分Model heads：</p>
<ul>
<li>*Model (retrieve the hidden states)</li>
<li>*ForCausalLM</li>
<li>*ForMaskedLM</li>
<li>*ForMultipleChoice</li>
<li>*ForQuestionAnswering</li>
<li>*ForSequenceClassification</li>
<li>*ForTokenClassification</li>
<li>and others 🤗</li>
</ul>
<h6 id="post-processing">Post-processing/后处理<a class="headerlink" href="#post-processing" title="Permanent link">&para;</a></h6>
<p>从模型中获得的作为输出的值本身并不一定有意义。要转换为概率，它们需要经过一个 SoftMax 层。</p>
<h6 id="_4">微调一个预训练模型<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h6>
<h6 id="_5">数据处理<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h6>
<p>在本节中，我们将使用MRPC（Microsoft Research Praphrase Corpus）数据集作为示例。该DataSet由5,801对句子组成，标签指示它们是否是同义句（即两个句子是否表示相同的意思）。 我们选择它是因为它是一个小型数据集，因此可以轻松训练。</p>
<h6 id="hub">从Hub上加载数据集<a class="headerlink" href="#hub" title="Permanent link">&para;</a></h6>
<p>Hub不仅包含模型，还含有多种语言的datasets。
例如，MRPC数据集是构成 GLUE benchmark的 10 个数据集之一。GLUE（General Language Understanding Evaluation）是一个多任务的自然语言理解基准和分析平台。GLUE包含九项NLU任务，语言均为英语。GLUE九项任务涉及到自然语言推断、文本蕴含、情感分析、语义相似等多个任务。像BERT、XLNet、RoBERTa、ERINE、T5等知名模型都会在此基准上进行测试。</p>
<p>🤗 Datasets库提供了一个非常简单的命令来下载和缓存Hub上的dataset。 我们可以像这样下载 MRPC 数据集：
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">raw_datasets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;glue&quot;</span><span class="p">,</span> <span class="s2">&quot;mrpc&quot;</span><span class="p">)</span>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">raw_datasets</span>
</span></code></pre></div>
输出如下：
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="n">DatasetDict</span><span class="p">({</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>    <span class="n">train</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">({</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;sentence1&#39;</span><span class="p">,</span> <span class="s1">&#39;sentence2&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;idx&#39;</span><span class="p">],</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">3668</span>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>    <span class="p">})</span>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>    <span class="n">validation</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">({</span>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;sentence1&#39;</span><span class="p">,</span> <span class="s1">&#39;sentence2&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;idx&#39;</span><span class="p">],</span>
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">408</span>
</span><span id="__span-2-9"><a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>    <span class="p">})</span>
</span><span id="__span-2-10"><a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>    <span class="n">test</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">({</span>
</span><span id="__span-2-11"><a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;sentence1&#39;</span><span class="p">,</span> <span class="s1">&#39;sentence2&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;idx&#39;</span><span class="p">],</span>
</span><span id="__span-2-12"><a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">1725</span>
</span><span id="__span-2-13"><a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>    <span class="p">})</span>
</span><span id="__span-2-14"><a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a><span class="p">})</span>
</span></code></pre></div>
这样就得到一个DatasetDict对象，包含训练集、验证集和测试集，训练集中有3,668 个句子对，验证集中有408对，测试集中有1,725 对。每个句子对包含四个字段：'sentence1', 'sentence2', 'label'和 'idx'。</p>
<p>我们可以通过索引访问raw_datasets 的句子对：
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">raw_train_dataset</span> <span class="o">=</span> <span class="n">raw_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">raw_train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></code></pre></div>
输出如下：
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="p">{</span><span class="s1">&#39;sentence1&#39;</span><span class="p">:</span> <span class="s1">&#39;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#39;</span><span class="p">,</span> 
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="s1">&#39;sentence2&#39;</span><span class="p">:</span> <span class="s1">&#39;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#39;</span><span class="p">,</span> 
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="s1">&#39;label&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> 
</span><span id="__span-4-4"><a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a><span class="s1">&#39;idx&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
</span></code></pre></div>
我们可以通过features获得数据集的字段类型：
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">raw_train_dataset</span><span class="o">.</span><span class="n">features</span>
</span></code></pre></div>
输出如下：
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="p">{</span><span class="s1">&#39;sentence1&#39;</span><span class="p">:</span> <span class="n">Value</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;string&#39;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span> 
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="s1">&#39;sentence2&#39;</span><span class="p">:</span> <span class="n">Value</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;string&#39;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span> 
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span class="s1">&#39;label&#39;</span><span class="p">:</span> <span class="n">ClassLabel</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;not_equivalent&#39;</span><span class="p">,</span> <span class="s1">&#39;equivalent&#39;</span><span class="p">],</span> <span class="n">names_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span> 
</span><span id="__span-6-4"><a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span class="s1">&#39;idx&#39;</span><span class="p">:</span> <span class="n">Value</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="kc">None</span><span class="p">)}</span>
</span></code></pre></div></p>
<blockquote>
<p>TIPS：
1. 没有数据集的话首先安装一下：<code>pip install datasets</code>
2. 这里很容易出现连接错误，解决方法如下：<a href="https://blog.csdn.net/qq_20849045/article/details/117462846?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link">https://blog.csdn.net/qq_20849045/article/details/117462846?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link</a></p>
</blockquote>
<h6 id="_6">数据集预处理<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h6>
<p>通过数据集预处理，我们将文本转换成模型能理解的向量。这个过程通过Tokenizer实现：
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>
</span><span id="__span-7-3"><a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>
</span><span id="__span-7-4"><a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span><span id="__span-7-5"><a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">tokenized_sentences_1</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;sentence1&quot;</span><span class="p">])</span>
</span><span id="__span-7-6"><a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">tokenized_sentences_2</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;sentence2&quot;</span><span class="p">])</span>
</span></code></pre></div></p>
<p>（TODO）</p>
<h6 id="trainer-api">使用Trainer API微调一个模型<a class="headerlink" href="#trainer-api" title="Permanent link">&para;</a></h6>
<h6 id="_7">训练<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h6>
<h6 id="_8">评估函数<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h6>
<h6 id="_9">补充部分<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h6>
<h6 id="4trainer">为什么4中用Trainer来微调模型？<a class="headerlink" href="#4trainer" title="Permanent link">&para;</a></h6>
<h6 id="training-arguments">Training Arguments主要参数<a class="headerlink" href="#training-arguments" title="Permanent link">&para;</a></h6>
<h6 id="_10">不同模型的加载方式<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h6>
<h6 id="dynamic-padding">Dynamic Padding——动态填充技术<a class="headerlink" href="#dynamic-padding" title="Permanent link">&para;</a></h6>
<h6 id="_11">参考资料<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h6>
<ul>
<li>基于transformers的自然语言处理(NLP)入门--在线阅读 <a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/">https://datawhalechina.github.io/learn-nlp-with-transformers/#/</a></li>
<li>Huggingface官方教程 <a href="https://huggingface.co/course/chapter1">https://huggingface.co/course/chapter1</a></li>
</ul>
</section>

<section id="mkdocs-terminal-after-content">
    
</section>
<section id="mkdocs-terminal-revision">
<br>
<aside>
    <p>
        <small>
            <i>Page last updated 2023-12-10. </i>
        </small>
    </p>
</aside>
</section>
            </main>
        </div>
        <hr><footer>
    <div class="terminal-mkdocs-footer-grid">
        <div id="terminal-mkdocs-footer-copyright-info">
            
            <p class="text-center text-muted">&copy; 2022-2023 by <a href='https://chuxiaoyu.github.io/' target='_blank'> Xiaoyu Chu </a></p>
             Site built with <a href="http://www.mkdocs.org">MkDocs</a> and <a href="https://github.com/ntno/mkdocs-terminal">Terminal for MkDocs</a>.
        </div>
        <div id="terminal-mkdocs-footer-prev-next">
            <nav class="btn-group">
                <a href="../nlp-transformer-task05/" title="Task05 编写BERT模型">Previous</a>
                |
                <a href="../nlp-transformer-task07/" title="Task07 使用Transformers解决文本分类任务">Next</a>
            </nav>
        </div>
    </div>
</footer>
    </div>

    
    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="alertdialog" aria-modal="true" aria-labelledby="searchModalLabel">
    <div class="modal-dialog modal-lg" role="search">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="searchModalLabel">Search</h5>
                <button type="button" class="close btn btn-default btn-ghost" data-dismiss="modal"><span aria-hidden="true">x</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p id="searchInputLabel">Type to start searching</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" aria-labelledby="searchInputLabel" placeholder="" id="mkdocs-search-query" title="Please enter search terms here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No document matches found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>
    
    
</body>

</html>