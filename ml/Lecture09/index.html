<!DOCTYPE html>
<html lang="en">
<head>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <meta name="generator" content="mkdocs-1.4.3, mkdocs-terminal-4.4.0">
     
     
    <link rel="icon" type="image/png" sizes="192x192" href="../../img/android-chrome-192x192.png" />
<link rel="icon" type="image/png" sizes="512x512" href="../../img/android-chrome-512x512.png" />
<link rel="apple-touch-icon" sizes="180x180" href="../../img/apple-touch-icon.png" />
<link rel="shortcut icon" type="image/png" sizes="48x48" href="../../img/favicon.ico" />
<link rel="icon" type="image/png" sizes="16x16" href="../../img/favicon-16x16.png" />
<link rel="icon" type="image/png" sizes="32x32" href="../../img/favicon-32x32.png" />


    
 
<title>Lecture09 自注意力模型、Transformers - 起 身 獨 立 向 荒 原 。</title>


<link href="../../css/fontawesome/css/fontawesome.min.css" rel="stylesheet">
<link href="../../css/fontawesome/css/solid.min.css" rel="stylesheet">
<link href="../../css/normalize.css" rel="stylesheet">
<link href="../../css/terminal.css" rel="stylesheet">
<link href="../../css/theme.css" rel="stylesheet">
<link href="../../css/theme.tile_grid.css" rel="stylesheet">
<link href="../../css/theme.footer.css" rel="stylesheet">
<!-- default color palette -->
<link href="../../css/palettes/default.css" rel="stylesheet">

<!-- page layout -->
<style>
/* initially set page layout to a one column grid */
.terminal-mkdocs-main-grid {
    display: grid;
    grid-column-gap: 1.4em;
    grid-template-columns: auto;
    grid-template-rows: auto;
}

/*  
*   when side navigation is not hidden, use a two column grid.  
*   if the screen is too narrow, fall back to the initial one column grid layout.
*   in this case the main content will be placed under the navigation panel. 
*/
@media only screen and (min-width: 70em) {
    .terminal-mkdocs-main-grid {
        grid-template-columns: 4fr 9fr;
    }
}</style>

<!-- link underline override -->
<style>
#terminal-mkdocs-main-content a:not(.headerlink){
    text-decoration: none;
}
</style>

     
    
    

    
    <!-- search css support -->
<link href="../../css/search/bootstrap-modal.css" rel="stylesheet">
<!-- search scripts -->
<script>
    var base_url = "../..",
    shortcuts = "{}";
</script>
<script src="../../js/jquery/jquery-1.10.1.min.js" defer></script>
<script src="../../js/bootstrap/bootstrap.min.js" defer></script>
<script src="../../js/mkdocs/base.js" defer></script>
    
    
    
    
    <script src="../../js/extra.js"></script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    
    <script src="../../search/main.js"></script>
    

    
</head>

<body class="terminal"><div class="container">
    <div class="terminal-nav">
        <header class="terminal-logo">
            <div id="mkdocs-terminal-site-name" class="logo terminal-prompt"><a href="/" class="no-style">起 身 獨 立 向 荒 原 。</a></div>
        </header>
        
        <nav class="terminal-menu">
            
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                
                
                <li property="itemListElement" typeof="ListItem">
                    <a href="../.." class="menu-item " property="item" typeof="WebPage">
                        <span property="name">Home</span>
                    </a>
                    <meta property="position" content="0">
                </li>
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                    
                    


<li property="itemListElement" typeof="ListItem">
    <a href="#" class="menu-item" data-toggle="modal" data-target="#mkdocs_search_modal" property="item" typeof="SearchAction">
        <i aria-hidden="true" class="fa fa-search"></i> <span property="name">Search</span>
    </a>
    <meta property="position" content="1">
</li>
                    
            </ul>
            
        </nav>
    </div>
</div>
        
    <div class="container">
        <div class="terminal-mkdocs-main-grid"><aside id="terminal-mkdocs-side-panel"><nav>
  
    <ul class="terminal-mkdocs-side-nav-items">
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../..">Home</a>
        
    
    
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">Deep Learning for Time Series Forecasting</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../time-series/01-time-series-types/">Taxonomy of Time Series Forecasting</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item--active terminal-mkdocs-side-nav-section-no-index">CS224n自然语言处理</span>
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../Lecture07/">Lecture07 机器翻译、seq2seq模型、注意力机制</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        <span class="

    terminal-mkdocs-side-nav-item--active">Lecture09 自注意力模型、Transformers</span>
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../Lecture10/">Lecture10 预训练</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../Lecture11/">Lecture11 问答系统</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">基于transformers的NLP</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task01/">Task01 NLP学习概览</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task02/">Task02 学习Attentioin和Transformer</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task03/">Task03 学习BERT</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task04/">Task04 学习GPT</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task05/">Task05 编写BERT模型</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task06/">Task06 BERT应用、训练和优化</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task07/">Task07 使用Transformers解决文本分类任务</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">深入浅出PyTorch</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../pytorch-chap01-02/">Chapter01-02 PyTorch的简介和安装、PyTorch基础知识</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../pytorch-chap03/">Chapter03 PyTorch的主要组成模块</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../pytorch-chap04/">Chapter04 PyTorch基础实战——FashionMNIST图像分类</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">Nand2Tetris 计算机系统要素</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/nand2tetris_part_1/">Nand2Tetris Part1 (Hardware)</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/nand2tetris_part_2/">Nand2Tetris Part2 (Software)</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">ML Compilation 机器学习编译</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-01/">01 机器学习编译概述</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-02/">02 张量程序抽象</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-03/">03 张量程序抽象案例研究：TensorIR</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-04/">04 端到端模型整合</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-05/">05 自动程序优化</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">Resources</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../resource/ml-system-learning-list/">ML System Learning List</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../resource/open-source-projects/">Open Source Projects</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../resource/post-types-of-paper/">Types of CS Paper</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
    </ul>
  
</nav><hr>
<nav>
    <ul>
        <li><a href="#lecture09-transformers">Lecture09: 自注意力模型、Transformers</a></li>
        <li><a href="#_1">本节主要内容</a></li>
        <li><a href="#1-rnnnlp">1 从RNN到基于注意力的NLP模型</a></li>
        <li><a href="#11-rnn">1.1 RNN模型存在的问题</a></li>
        <li><a href="#12">1.2 自注意力模型介绍</a></li>
        <li><a href="#13">1.3 自注意力模块的处理</a></li>
        <li><a href="#131">1.3.1 位置编码</a></li>
        <li><a href="#132">1.3.2 遮罩</a></li>
        <li><a href="#2-transformer">2 Transformer模型介绍</a></li>
        <li><a href="#21-transformer">2.1 Transformer概览</a></li>
        <li><a href="#22-transformer">2.2 Transformer编码器</a></li>
        <li><a href="#221-query-key-value">2.2.1 Query-Key-Value向量矩阵</a></li>
        <li><a href="#222">2.2.2 多头注意力机制</a></li>
        <li><a href="#223">2.2.3 残差连接</a></li>
        <li><a href="#224">2.2.4 归一化</a></li>
        <li><a href="#225">2.2.5 缩放点积运算</a></li>
        <li><a href="#226">2.2.6 小结</a></li>
        <li><a href="#23-transformer">2.3 Transformer解码器</a></li>
        
    </ul>
</nav>
</aside>
            <main id="terminal-mkdocs-main-content">
    
    
    
    
    

<section id="mkdocs-terminal-content">
    <h6 id="lecture09-transformers">Lecture09: 自注意力模型、Transformers<a class="headerlink" href="#lecture09-transformers" title="Permanent link">&para;</a></h6>
<h6 id="_1">本节主要内容<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h6>
<ul>
<li>从RNN到基于注意力的NLP模型</li>
<li>Transformer模型</li>
</ul>
<h6 id="1-rnnnlp">1 从RNN到基于注意力的NLP模型<a class="headerlink" href="#1-rnnnlp" title="Permanent link">&para;</a></h6>
<h6 id="11-rnn">1.1 RNN模型存在的问题<a class="headerlink" href="#11-rnn" title="Permanent link">&para;</a></h6>
<ol>
<li>线性相互作用距离（Linear interaction distance），即长距离依赖问题。</li>
<li>缺少并行性（parallelizability）。</li>
</ol>
<p>为了解决上述问题，人们考虑到了注意力机制。</p>
<blockquote>
<p><em>[学生提问]注意力和全连接网络的区别是什么？</em></p>
<p><em>1.注意力的权重是动态的</em></p>
<p><em>2.参数的计算不同</em></p>
</blockquote>
<h6 id="12">1.2 自注意力模型介绍<a class="headerlink" href="#12" title="Permanent link">&para;</a></h6>
<p>注意力机制的运作需要queris, keys, values向量：</p>
<ul>
<li><strong>queries</strong>  <span class="arithmatex"><span class="MathJax_Preview">q_1, q_2,..., q_T</span><script type="math/tex">q_1, q_2,..., q_T</script></span>，<span class="arithmatex"><span class="MathJax_Preview">q_i∈R^d</span><script type="math/tex">q_i∈R^d</script></span></li>
<li><strong>keys</strong>  <span class="arithmatex"><span class="MathJax_Preview">k_1, k_2,..., k_T</span><script type="math/tex">k_1, k_2,..., k_T</script></span>，<span class="arithmatex"><span class="MathJax_Preview">k_i∈R^d</span><script type="math/tex">k_i∈R^d</script></span></li>
<li><strong>values</strong>  <span class="arithmatex"><span class="MathJax_Preview">v_1, v_2,..., v_T</span><script type="math/tex">v_1, v_2,..., v_T</script></span>，<span class="arithmatex"><span class="MathJax_Preview">v_i∈R^d</span><script type="math/tex">v_i∈R^d</script></span></li>
</ul>
<p>在自注意力模型（self-attention）中，queries，keys，values来源相同。</p>
<ul>
<li>例如，如果某层的输出是<span class="arithmatex"><span class="MathJax_Preview">x_1, x_2,..., x_T</span><script type="math/tex">x_1, x_2,..., x_T</script></span>，那么可以使<span class="arithmatex"><span class="MathJax_Preview">v_i = k_i = q_i = x_i</span><script type="math/tex">v_i = k_i = q_i = x_i</script></span>[?]</li>
</ul>
<p>那么，自注意力的计算（以点积为例）如下：</p>
<p>（1）计算query-key乘积，得到注意力分数<span class="arithmatex"><span class="MathJax_Preview">e_{ij}</span><script type="math/tex">e_{ij}</script></span>
$$
e_{ij} = q_i^Tk_j
$$
（2）计算注意力权重<span class="arithmatex"><span class="MathJax_Preview">α</span><script type="math/tex">α</script></span>
$$
α_{ij} = softmax(e_{ij})
$$
（3）计算输出
$$
output_i = \sum_j{α_{ij}v_j}
$$</p>
<h6 id="13">1.3 自注意力模块的处理<a class="headerlink" href="#13" title="Permanent link">&para;</a></h6>
<h6 id="131">1.3.1 位置编码<a class="headerlink" href="#131" title="Permanent link">&para;</a></h6>
<p>因为自注意力没有考虑位置信息，所以需要将序列的位置编码到keys，queries，values向量中。</p>
<p>考虑将序列索引（sequence index）用向量（vector）表示：
$$
p_i∈R^d, for\; i∈{1,2,...,T}
$$
<span class="arithmatex"><span class="MathJax_Preview">p_i</span><script type="math/tex">p_i</script></span>即位置向量（positional vector）。</p>
<p>得到位置向量后，我们将其加到输入里。假设<span class="arithmatex"><span class="MathJax_Preview">\widetilde q</span><script type="math/tex">\widetilde q</script></span>，<span class="arithmatex"><span class="MathJax_Preview">\widetilde k</span><script type="math/tex">\widetilde k</script></span>，<span class="arithmatex"><span class="MathJax_Preview">\widetilde v</span><script type="math/tex">\widetilde v</script></span>是之前的向量，则：
$$
q_i =\widetilde q_i + p_i\
k_i =\widetilde k_i + p_i\
v_i =\widetilde v_i + p_i\
$$</p>
<blockquote>
<p><em>[?]位置向量不是加到输入里吗？为什么这里是加入到q,k,v？</em></p>
</blockquote>
<p>位置向量有多种计算方式，如正弦位置表示等，最常用的是绝对位置表示（absolute position representations）。（注：有点像独热编码）</p>
<h6 id="132">1.3.2 遮罩<a class="headerlink" href="#132" title="Permanent link">&para;</a></h6>
<p>进行序列预测的时候，不能看到后面的信息，因此采用了遮罩（Masking）处理，即将后面单词的注意力分数设置为<span class="arithmatex"><span class="MathJax_Preview">-\infty</span><script type="math/tex">-\infty</script></span>：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
e_{ij} = \left\{
\begin{aligned}
q^T_ik_j, j&lt;i\\
-\infty, j≥i
\end{aligned}
\right.
</div>
<script type="math/tex; mode=display">
e_{ij} = \left\{
\begin{aligned}
q^T_ik_j, j<i\\
-\infty, j≥i
\end{aligned}
\right.
</script>
</div>
<h6 id="2-transformer">2 Transformer模型介绍<a class="headerlink" href="#2-transformer" title="Permanent link">&para;</a></h6>
<h6 id="21-transformer">2.1 Transformer概览<a class="headerlink" href="#21-transformer" title="Permanent link">&para;</a></h6>
<p><img alt="image-20220113163942108" src="../image/image-20220113163942108.png" /></p>
<h6 id="22-transformer">2.2 Transformer编码器<a class="headerlink" href="#22-transformer" title="Permanent link">&para;</a></h6>
<p>编码器（Encoder）包含以下模块：</p>
<ul>
<li>
<p>Q-K-V向量</p>
</li>
<li>
<p>多头注意力机制（Multi-head attention）</p>
</li>
</ul>
<p>其他训练技巧（这些技巧不能提升模型能做什么，而是加速训练过程）</p>
<ul>
<li>残差连接（Residual connections）</li>
<li>归一化（Layer normalization）</li>
<li>缩放点积运算（Scaled Dot Product）</li>
</ul>
<h6 id="221-query-key-value">2.2.1 Query-Key-Value向量矩阵<a class="headerlink" href="#221-query-key-value" title="Permanent link">&para;</a></h6>
<p>用<span class="arithmatex"><span class="MathJax_Preview">x_1,...,x_T</span><script type="math/tex">x_1,...,x_T</script></span> <span class="arithmatex"><span class="MathJax_Preview">(x_i∈R^d)</span><script type="math/tex">(x_i∈R^d)</script></span>表示Transformer编码器的输入向量，则queries, keys, values的计算如下：</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">q_i=Qx_i</span><script type="math/tex">q_i=Qx_i</script></span>，<span class="arithmatex"><span class="MathJax_Preview">Q∈R^{d×d}</span><script type="math/tex">Q∈R^{d×d}</script></span>是权重矩阵</li>
<li><span class="arithmatex"><span class="MathJax_Preview">k_i=Kx_i</span><script type="math/tex">k_i=Kx_i</script></span>，<span class="arithmatex"><span class="MathJax_Preview">K∈R^{d×d}</span><script type="math/tex">K∈R^{d×d}</script></span>是权重矩阵</li>
<li><span class="arithmatex"><span class="MathJax_Preview">v_i=Vx_i</span><script type="math/tex">v_i=Vx_i</script></span>，<span class="arithmatex"><span class="MathJax_Preview">V∈R^{d×d}</span><script type="math/tex">V∈R^{d×d}</script></span>是权重矩阵</li>
</ul>
<p>对不同的参数矩阵对原始输入向量做线性变换，从而让不同的变换结果承担不同角色。</p>
<p>让我们通过矩阵的视角来看Q，K，V是如何计算的：</p>
<p>首先，用<span class="arithmatex"><span class="MathJax_Preview">X=[x_1;...;x_T]∈R^{T×d}</span><script type="math/tex">X=[x_1;...;x_T]∈R^{T×d}</script></span>表示输入向量的拼接矩阵，那么<span class="arithmatex"><span class="MathJax_Preview">XQ∈R^{T×d}</span><script type="math/tex">XQ∈R^{T×d}</script></span>，<span class="arithmatex"><span class="MathJax_Preview">XK∈R^{T×d}</span><script type="math/tex">XK∈R^{T×d}</script></span>，<span class="arithmatex"><span class="MathJax_Preview">XV∈R^{T×d}</span><script type="math/tex">XV∈R^{T×d}</script></span>。输出就可以表示为：
$$
output = softmax(XQ(XK)^T)×XV
$$
<img alt="image-20220113180152211" src="../image/image-20220113180152211.png" /></p>
<h6 id="222">2.2.2 多头注意力机制<a class="headerlink" href="#222" title="Permanent link">&para;</a></h6>
<p>对于单词<span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>，自注意力只注意到<span class="arithmatex"><span class="MathJax_Preview">x^T_iQ^TKx_j</span><script type="math/tex">x^T_iQ^TKx_j</script></span>高的地方，但是我们如何关注到不同的<span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>呢？</p>
<blockquote>
<p><em>这里对为什么要使用多头的解释并不清楚，可以参考：为什么Transformer 需要进行 Multi-head Attention？ - 知乎 <a href="https://www.zhihu.com/question/341222779">https://www.zhihu.com/question/341222779</a></em></p>
<p><em>《自然语言处理——基于预训练模型的方法》P93中的解释是：</em></p>
<p><em>“由于自注意力结果需要经过归一化，导致即使一个输入和多个其他的输入相关，也无法同时为这些输入赋予较大的注意力值，即自注意力结果之间是互斥的，无法同时关注多个输入。因此，如果能使用多组注意力模型产生多组不同的注意力结果，则不同组注意力模型可能关注到不同的输入上，从而增强模型的表达能力。“</em></p>
</blockquote>
<p>我们通过多个<span class="arithmatex"><span class="MathJax_Preview">Q, K, V</span><script type="math/tex">Q, K, V</script></span>矩阵定义多头注意力（Multi-headed Attention）。</p>
<p>用<span class="arithmatex"><span class="MathJax_Preview">Q_ℓ,K_ℓ,V_ℓ∈R^{d×d/h}</span><script type="math/tex">Q_ℓ,K_ℓ,V_ℓ∈R^{d×d/h}</script></span> 表示不同的参数矩阵，其中<span class="arithmatex"><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span>表示注意力头的序号，<span class="arithmatex"><span class="MathJax_Preview">ℓ</span><script type="math/tex">ℓ</script></span>的取值范围是从<span class="arithmatex"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>到<span class="arithmatex"><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span>。<em>(<span class="arithmatex"><span class="MathJax_Preview">R^{d×d/h}</span><script type="math/tex">R^{d×d/h}</script></span>)</em></p>
<p>每个注意力头独立运算：
$$
output_ℓ = softmax(XQ_ℓK<sup>T_ℓX</sup>T)*XV_ℓ
$$
其中<span class="arithmatex"><span class="MathJax_Preview">output_ℓ∈R^{d/h}</span><script type="math/tex">output_ℓ∈R^{d/h}</script></span>。</p>
<p>然后，将所有的输出混合：
$$
output = Y[output_1;...;output_h], Y∈R^{d×d}
$$
下图是单头注意力和多头注意力的简单示意图：</p>
<p><img alt="image-20220114135758701" src="../image/image-20220114135758701.png" /></p>
<p>可以看出，多头注意力和单个注意力的计算量是一样的。<em>（都是把矩阵拼起来计算一次）</em></p>
<h6 id="223">2.2.3 残差连接<a class="headerlink" href="#223" title="Permanent link">&para;</a></h6>
<p>残差连接（Residual connections）是一种提升模型训练效果的技巧。</p>
<p>正常情况：<span class="arithmatex"><span class="MathJax_Preview">X^{(i)} = Layer(X^{(i-1)})</span><script type="math/tex">X^{(i)} = Layer(X^{(i-1)})</script></span></p>
<p><img alt="image-20220114140717577" src="../image/image-20220114140717577.png" /></p>
<p>残差连接：<span class="arithmatex"><span class="MathJax_Preview">X^{(i)} = X^{(i-1)}+Layer(X^{(i-1)})</span><script type="math/tex">X^{(i)} = X^{(i-1)}+Layer(X^{(i-1)})</script></span></p>
<p><img alt="image-20220114140832475" src="../image/image-20220114140832475.png" /></p>
<h6 id="224">2.2.4 归一化<a class="headerlink" href="#224" title="Permanent link">&para;</a></h6>
<p>归一化（Layer normalization）是一种提升模型训练速度的技巧。
$$
output = \frac{x-μ}{\sqrt{𝜎}+𝜖}*𝛾+𝛽
$$
其中，𝜇是均值，𝜎是标准差。𝛾和𝛽是gain和bias参数[?]。</p>
<h6 id="225">2.2.5 缩放点积运算<a class="headerlink" href="#225" title="Permanent link">&para;</a></h6>
<p>缩放点积运算是为了防止在维数过大时，梯度变小或消失。</p>
<p>正常情况：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
output_ℓ = softmax(XQ_ℓK_ℓ^TX^T)*XV_ℓ
</div>
<script type="math/tex; mode=display">
output_ℓ = softmax(XQ_ℓK_ℓ^TX^T)*XV_ℓ
</script>
</div>
<p>缩放点积运算：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
output_ℓ = softmax(\frac{XQ_ℓK_ℓ^TX^T}{\sqrt{d/h}})*XV_ℓ
</div>
<script type="math/tex; mode=display">
output_ℓ = softmax(\frac{XQ_ℓK_ℓ^TX^T}{\sqrt{d/h}})*XV_ℓ
</script>
</div>
<p>可以看出，就是将注意力分数除以维数<span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>除以注意力头的数量<span class="arithmatex"><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span>的根。</p>
<h6 id="226">2.2.6 小结<a class="headerlink" href="#226" title="Permanent link">&para;</a></h6>
<p>2.1中给出了Tranformer编码器的整体概览，经过对模块组成的分解，编码器更具体的结构如下图所示：</p>
<p><img alt="image-20220114143617158" src="../image/image-20220114143617158.png" /></p>
<h6 id="23-transformer">2.3 Transformer解码器<a class="headerlink" href="#23-transformer" title="Permanent link">&para;</a></h6>
<p>解码器的结构与编码器类似，如图：</p>
<p><img alt="image-20220114144135116" src="../image/image-20220114144135116.png" /></p>
<p>可以看出，稍微不一样的地方在于交叉注意力（Cross attention）。</p>
<p><strong>交叉注意力机制</strong></p>
<p>假设<span class="arithmatex"><span class="MathJax_Preview">h_1,...,h_T</span><script type="math/tex">h_1,...,h_T</script></span>是Transformer编码器的输出向量，<span class="arithmatex"><span class="MathJax_Preview">z_1,...,z_T</span><script type="math/tex">z_1,...,z_T</script></span>是Transformer解码器的输入向量，那么，</p>
<ul>
<li>keys和values来自编码器：<span class="arithmatex"><span class="MathJax_Preview">k_i = Kh_i,v_i = Vh_i</span><script type="math/tex">k_i = Kh_i,v_i = Vh_i</script></span></li>
<li>queries来自解码器：<span class="arithmatex"><span class="MathJax_Preview">q_i=Qz_i</span><script type="math/tex">q_i=Qz_i</script></span>.</li>
</ul>
<p>假设<span class="arithmatex"><span class="MathJax_Preview">H = [h_1;...;h_T]</span><script type="math/tex">H = [h_1;...;h_T]</script></span>是编码器向量的拼接，<span class="arithmatex"><span class="MathJax_Preview">Z = [z_1;...;z_T]</span><script type="math/tex">Z = [z_1;...;z_T]</script></span>是解码器向量的拼接，那么输出可以定义为：
$$
output = softmax(ZQ(HK)^T)×HV
$$
<img alt="image-20220114145530907" src="../image/image-20220114145530907.png" /></p>
</section>

<section id="mkdocs-terminal-after-content">
    
</section>
<section id="mkdocs-terminal-revision">
<br>
<aside>
    <p>
        <small>
            <i>Page last updated 2023-12-10. </i>
        </small>
    </p>
</aside>
</section>
            </main>
        </div>
        <hr><footer>
    <div class="terminal-mkdocs-footer-grid">
        <div id="terminal-mkdocs-footer-copyright-info">
            
            <p class="text-center text-muted">&copy; 2022-2023 by <a href='https://chuxiaoyu.github.io/' target='_blank'> Xiaoyu Chu </a></p>
             Site built with <a href="http://www.mkdocs.org">MkDocs</a> and <a href="https://github.com/ntno/mkdocs-terminal">Terminal for MkDocs</a>.
        </div>
        <div id="terminal-mkdocs-footer-prev-next">
            <nav class="btn-group">
                <a href="../Lecture07/" title="Lecture07 机器翻译、seq2seq模型、注意力机制">Previous</a>
                |
                <a href="../Lecture10/" title="Lecture10 预训练">Next</a>
            </nav>
        </div>
    </div>
</footer>
    </div>

    
    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="alertdialog" aria-modal="true" aria-labelledby="searchModalLabel">
    <div class="modal-dialog modal-lg" role="search">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="searchModalLabel">Search</h5>
                <button type="button" class="close btn btn-default btn-ghost" data-dismiss="modal"><span aria-hidden="true">x</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p id="searchInputLabel">Type to start searching</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" aria-labelledby="searchInputLabel" placeholder="" id="mkdocs-search-query" title="Please enter search terms here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No document matches found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>
    
    
</body>

</html>