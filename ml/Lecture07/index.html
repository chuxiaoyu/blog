<!DOCTYPE html>
<html lang="en">
<head>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <meta name="generator" content="mkdocs-1.4.3, mkdocs-terminal-4.4.0">
     
     
    <link rel="icon" type="image/png" sizes="192x192" href="../../img/android-chrome-192x192.png" />
<link rel="icon" type="image/png" sizes="512x512" href="../../img/android-chrome-512x512.png" />
<link rel="apple-touch-icon" sizes="180x180" href="../../img/apple-touch-icon.png" />
<link rel="shortcut icon" type="image/png" sizes="48x48" href="../../img/favicon.ico" />
<link rel="icon" type="image/png" sizes="16x16" href="../../img/favicon-16x16.png" />
<link rel="icon" type="image/png" sizes="32x32" href="../../img/favicon-32x32.png" />


    
 
<title>Lecture07 机器翻译、seq2seq模型、注意力机制 - 起身獨立向荒原。</title>


<link href="../../css/fontawesome/css/fontawesome.min.css" rel="stylesheet">
<link href="../../css/fontawesome/css/solid.min.css" rel="stylesheet">
<link href="../../css/normalize.css" rel="stylesheet">
<link href="../../css/terminal.css" rel="stylesheet">
<link href="../../css/theme.css" rel="stylesheet">
<link href="../../css/theme.tile_grid.css" rel="stylesheet">
<link href="../../css/theme.footer.css" rel="stylesheet">
<!-- default color palette -->
<link href="../../css/palettes/default.css" rel="stylesheet">

<!-- page layout -->
<style>
/* initially set page layout to a one column grid */
.terminal-mkdocs-main-grid {
    display: grid;
    grid-column-gap: 1.4em;
    grid-template-columns: auto;
    grid-template-rows: auto;
}

/*  
*   when side navigation is not hidden, use a two column grid.  
*   if the screen is too narrow, fall back to the initial one column grid layout.
*   in this case the main content will be placed under the navigation panel. 
*/
@media only screen and (min-width: 70em) {
    .terminal-mkdocs-main-grid {
        grid-template-columns: 4fr 9fr;
    }
}</style>

<!-- link underline override -->
<style>
#terminal-mkdocs-main-content a:not(.headerlink){
    text-decoration: none;
}
</style>

     
    
    

    
    <!-- search css support -->
<link href="../../css/search/bootstrap-modal.css" rel="stylesheet">
<!-- search scripts -->
<script>
    var base_url = "../..",
    shortcuts = "{}";
</script>
<script src="../../js/jquery/jquery-1.10.1.min.js" defer></script>
<script src="../../js/bootstrap/bootstrap.min.js" defer></script>
<script src="../../js/mkdocs/base.js" defer></script>
    
    
    
    
    <script src="../../js/extra.js"></script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    
    <script src="../../search/main.js"></script>
    

    
</head>

<body class="terminal"><div class="container">
    <div class="terminal-nav">
        <header class="terminal-logo">
            <div id="mkdocs-terminal-site-name" class="logo terminal-prompt"><a href="/" class="no-style">起身獨立向荒原。</a></div>
        </header>
        
        <nav class="terminal-menu">
            
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                
                
                <li property="itemListElement" typeof="ListItem">
                    <a href="../.." class="menu-item " property="item" typeof="WebPage">
                        <span property="name">Home</span>
                    </a>
                    <meta property="position" content="0">
                </li>
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                    
                    


<li property="itemListElement" typeof="ListItem">
    <a href="#" class="menu-item" data-toggle="modal" data-target="#mkdocs_search_modal" property="item" typeof="SearchAction">
        <i aria-hidden="true" class="fa fa-search"></i> <span property="name">Search</span>
    </a>
    <meta property="position" content="1">
</li>
                    
            </ul>
            
        </nav>
    </div>
</div>
        
    <div class="container">
        <div class="terminal-mkdocs-main-grid"><aside id="terminal-mkdocs-side-panel"><nav>
  
    <ul class="terminal-mkdocs-side-nav-items">
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../..">Home</a>
        
    
    
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">Deep Learning for Time Series Forecasting</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../time-series/01-time-series-types/">1.Taxonomy of Time Series Forecasting</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../time-series/02-time-series-supervised-learning/">2.Transforming Time Series to Supervised Learning</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../time-series/03-mlp-time-series-forecasting/">3.MLP for Time Series Forecasting</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item--active terminal-mkdocs-side-nav-section-no-index">CS224n自然语言处理</span>
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        <span class="

    terminal-mkdocs-side-nav-item--active">Lecture07 机器翻译、seq2seq模型、注意力机制</span>
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../Lecture09/">Lecture09 自注意力模型、Transformers</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../Lecture10/">Lecture10 预训练</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../Lecture11/">Lecture11 问答系统</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">基于transformers的NLP</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task01/">Task01 NLP学习概览</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task02/">Task02 学习Attentioin和Transformer</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task03/">Task03 学习BERT</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task04/">Task04 学习GPT</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task05/">Task05 编写BERT模型</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task06/">Task06 BERT应用、训练和优化</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../nlp-transformer-task07/">Task07 使用Transformers解决文本分类任务</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">深入浅出PyTorch</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../pytorch-chap01-02/">Chapter01-02 PyTorch的简介和安装、PyTorch基础知识</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../pytorch-chap03/">Chapter03 PyTorch的主要组成模块</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../pytorch-chap04/">Chapter04 PyTorch基础实战——FashionMNIST图像分类</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">Nand2Tetris 计算机系统要素</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/nand2tetris_part_1/">Nand2Tetris Part1 (Hardware)</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/nand2tetris_part_2/">Nand2Tetris Part2 (Software)</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">ML Compilation 机器学习编译</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-01/">01 机器学习编译概述</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-02/">02 张量程序抽象</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-03/">03 张量程序抽象案例研究：TensorIR</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-04/">04 端到端模型整合</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../system/ml-compilation-05/">05 自动程序优化</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">Resources</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../resource/ml-system-learning-list/">ML System Learning List</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../resource/open-source-projects/">Open Source Projects</a>
        
    
    </li>
            
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../resource/post-types-of-paper/">Types of CS Paper</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
          



<li class="terminal-mkdocs-side-nav-li">
    
    
        
        
            
                
        

        
            
    
        
        
            
            
            <span class="
        
            
        
    

    terminal-mkdocs-side-nav-item terminal-mkdocs-side-nav-section-no-index">Philosophy</span>
        
    
    
        
      
        
            <ul class="terminal-mkdocs-side-nav-li-ul">
        
            
            
                
                
            

             
                <li class="terminal-mkdocs-side-nav-li-ul-li">
    
        
        
            <a class="

    terminal-mkdocs-side-nav-item" href="../../philosophy/40-philosophers/">A Quick Introduction to 40+ Philosophers</a>
        
    
    </li>
            
            
    </ul>
        
    
  </li>
        
    </ul>
  
</nav><hr>
<nav>
    <ul>
        <li><a href="#lecture07-seq2seq">Lecture07: 机器翻译、seq2seq模型、注意力机制</a></li>
        <li><a href="#_1">本节主要内容</a></li>
        <li><a href="#1">1 深度学习之前的机器翻译</a></li>
        <li><a href="#11">1.1 机器翻译任务定义</a></li>
        <li><a href="#12">1.2 机器翻译的发展阶段</a></li>
        <li><a href="#13">1.3 基于统计的机器翻译</a></li>
        <li><a href="#2">2 基于神经网络的机器翻译</a></li>
        <li><a href="#3">3 注意力机制</a></li>
        <li><a href="#31-seq2seq">3.1 seq2seq架构存在的问题</a></li>
        <li><a href="#32-seq2seq">3.2 有注意力机制的seq2seq模型</a></li>
        <li><a href="#33">3.3 注意力机制的公式表示</a></li>
        <li><a href="#33_1">3.3 注意力机制的优点</a></li>
        <li><a href="#34">3.4 注意力机制是一种广泛性的深度学习技巧</a></li>
        <li><a href="#35">3.5 注意力分数的计算</a></li>
        
    </ul>
</nav>
</aside>
            <main id="terminal-mkdocs-main-content">
    
    
    
    
    

<section id="mkdocs-terminal-content">
    <h6 id="lecture07-seq2seq">Lecture07: 机器翻译、seq2seq模型、注意力机制<a class="headerlink" href="#lecture07-seq2seq" title="Permanent link">&para;</a></h6>
<h6 id="_1">本节主要内容<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h6>
<ul>
<li>机器翻译任务</li>
<li>seq2seq模型架构</li>
<li>注意力机制</li>
</ul>
<h6 id="1">1 深度学习之前的机器翻译<a class="headerlink" href="#1" title="Permanent link">&para;</a></h6>
<h6 id="11">1.1 机器翻译任务定义<a class="headerlink" href="#11" title="Permanent link">&para;</a></h6>
<p>机器翻译（Machine Translation, MT）任务：将句子<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>从一种语言（Source Language）翻译成另一种语言（Target Language）的句子<span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>。</p>
<h6 id="12">1.2 机器翻译的发展阶段<a class="headerlink" href="#12" title="Permanent link">&para;</a></h6>
<ul>
<li>1950s: 早期机器翻译</li>
<li>1990s-2010s: 基于统计的机器翻译（Statistics Machine Translation, SMT）</li>
<li>2014-: 基于神经网络的机器翻译（Neural Machine Translation, NMT）</li>
<li>2017-: 以Transformer为代表的预训练模型时代</li>
</ul>
<h6 id="13">1.3 基于统计的机器翻译<a class="headerlink" href="#13" title="Permanent link">&para;</a></h6>
<p>基于统计的机器翻译（Statistics Machine Translation, SMT）的核心思想是从数据中学习一个概率模型。</p>
<p>例如，给定一个法语语句<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>，找出对应的最合适的英文翻译语句<span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>，公式如下：
$$
argmax_yP(y|x)
$$
利用贝叶斯公式，可以将上述概率转化为两部分，分别进行学习：
$$
argmax_yP(x|y)P(y)
$$
上式中，<span class="arithmatex"><span class="MathJax_Preview">P(x|y)</span><script type="math/tex">P(x|y)</script></span>代表翻译模型（Translation Model），负责学习如何准确翻译单词、短语（fidelity，保真性）；<span class="arithmatex"><span class="MathJax_Preview">P(y)</span><script type="math/tex">P(y)</script></span>代表语言模型（Language Model），是对语言的流畅性（fluency）进行学习。<em>（注：对应翻译的两个标准：忠实、通顺）</em></p>
<p>那么，如何学习翻译模型<span class="arithmatex"><span class="MathJax_Preview">P(x|y)</span><script type="math/tex">P(x|y)</script></span>？</p>
<p>首先，需要大量的平行数据（Parallel Data），如大量人工翻译的法语/英语语句对。</p>
<p>然后，引入隐变量<span class="arithmatex"><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>：<span class="arithmatex"><span class="MathJax_Preview">P(x, a|y)</span><script type="math/tex">P(x, a|y)</script></span>，<span class="arithmatex"><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>是<span class="arithmatex"><span class="MathJax_Preview">alignment</span><script type="math/tex">alignment</script></span>，即语句<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>和语句<span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>之间的单词级别的对应关系。这种对应非常负责，考虑以下情形：</p>
<ul>
<li>有些单词没有或不需要对应的翻译词（no counterpart）</li>
<li>多个单词对应一个翻译词（many-to-one）</li>
<li>一个单词对应多个翻译词（one-to-many）</li>
<li>多个单词对应多个翻译词（many-to-many）</li>
</ul>
<p>对应关系<span class="arithmatex"><span class="MathJax_Preview">alignment</span><script type="math/tex">alignment</script></span>没有显性地存在于数据中，因此需要特殊的学习算法（如EM算法）。</p>
<p>基于统计的机器翻译缺点如下：</p>
<ul>
<li>整个翻译系统及其复杂，包含大量未提及的细节和单独设计的子模块</li>
<li>需要大量的特征工程（需要针对不同的特殊用法设计各种特征）</li>
<li>需要大量的人力和成本来维护语料资源</li>
</ul>
<h6 id="2">2 基于神经网络的机器翻译<a class="headerlink" href="#2" title="Permanent link">&para;</a></h6>
<p>基于神经网络的机器翻译（Neural Machine Translation, NMT）是一种用端对端神经网络进行机器翻译的方式。这种神经网络架构称为sequence-to-sequence（seq2seq）模型，包含两个循环神经网络（RNNs）。</p>
<p>seq2seq不只在机器翻译中使用，还被广泛应用于以下任务：</p>
<ul>
<li>文本摘要</li>
<li>对话系统</li>
<li>句法解析</li>
<li>代码生成</li>
</ul>
<p>seq2seq可以视为条件语言模型（Conditional Language Model）的一种：</p>
<ul>
<li>语言模型是因为它的解码器用于预测目标语句<span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>的下一个单词</li>
<li>条件是因为它的预测是基于源语句<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>的（条件概率）</li>
</ul>
<p>NMT直接计算<span class="arithmatex"><span class="MathJax_Preview">P(y|x)</span><script type="math/tex">P(y|x)</script></span>：
$$
P(y|x) = P(y_1|x)P(y_2|y_1,x)P(y_3|y_1,y_2,x)...P(y_T|y_1,...,y_{T-1}, x)
$$
其中，<span class="arithmatex"><span class="MathJax_Preview">P(y_T|y_1,...,y_{T-1}, x)</span><script type="math/tex">P(y_T|y_1,...,y_{T-1}, x)</script></span>表示给定已有的目标单词和源语句<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>，下一个目标单词的概率。</p>
<p>训练NMT需要大量平行语料，并且可以使用多层RNN。</p>
<p>RNN有两种解码方式：</p>
<ul>
<li>Greedy decoding</li>
<li>Beam search decoding</li>
</ul>
<p>相较于SMT，NMT有如下优点：</p>
<ul>
<li>性能更好</li>
<li>没有子模块</li>
<li>降低了人力需求（不需要特征工程和考虑特例）</li>
</ul>
<p>同时也有如下缺点：</p>
<ul>
<li>不可解释性</li>
<li>很难受控制</li>
</ul>
<p>从BLEU（Bilingual Evaluation Understudy）评测结果可以看出，NMT的性能已经远远超过SMT。</p>
<p>NMT可能是深度学习在NLP中最成功的应用：</p>
<ul>
<li>
<p>2014年，seq2seq论文发表；</p>
</li>
<li>
<p>2016年，NMT成为机器翻译的标准方法，谷歌翻译从SMT转为NMT；</p>
</li>
<li>
<p>2018年，所有公司的翻译工具均应用了NMT。</p>
</li>
</ul>
<p>然而，机器学习任务并未被彻底终结，许多困难仍然存在，如常识信息未得到应用、背景信息在长文本中难以维护以及模型偏见等。</p>
<h6 id="3">3 注意力机制<a class="headerlink" href="#3" title="Permanent link">&para;</a></h6>
<h6 id="31-seq2seq">3.1 seq2seq架构存在的问题<a class="headerlink" href="#31-seq2seq" title="Permanent link">&para;</a></h6>
<ol>
<li>语义向量可能无法完全捕捉到整个输入序列的信息；</li>
<li>先输入到网络的内容携带的信息会被后输入的信息覆盖掉。输入序列越长，这个现象就越严重。</li>
</ol>
<p>因此，Attention（注意力机制）提供了一种解决方法。</p>
<h6 id="32-seq2seq">3.2 有注意力机制的seq2seq模型<a class="headerlink" href="#32-seq2seq" title="Permanent link">&para;</a></h6>
<p>注意力机制的核心思路是：在解码器进行解码的每一步，直接连接到编码器来关注输入序列中的特定部分。</p>
<p><img alt="image-20220111171442329" src="../image/image-20220111171442329.jpg" /></p>
<h6 id="33">3.3 注意力机制的公式表示<a class="headerlink" href="#33" title="Permanent link">&para;</a></h6>
<p>假设编码器隐藏状态为<span class="arithmatex"><span class="MathJax_Preview">h_1,...,h_N∈R^h</span><script type="math/tex">h_1,...,h_N∈R^h</script></span>，在时间步<span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>时解码器的隐藏状态为<span class="arithmatex"><span class="MathJax_Preview">s_t∈R^h</span><script type="math/tex">s_t∈R^h</script></span>，则该时间步的注意力分数<span class="arithmatex"><span class="MathJax_Preview">e_t</span><script type="math/tex">e_t</script></span>可以表示为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
e_t = [s^T_th_1,...,s^T_th_N]∈R^N
</div>
<script type="math/tex; mode=display">
e_t = [s^T_th_1,...,s^T_th_N]∈R^N
</script>
</div>
<p>然后使用softmax得到注意力概率分布<span class="arithmatex"><span class="MathJax_Preview">α^t</span><script type="math/tex">α^t</script></span>：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
α^t = softmax(e^t)∈R^N
</div>
<script type="math/tex; mode=display">
α^t = softmax(e^t)∈R^N
</script>
</div>
<p>利用<span class="arithmatex"><span class="MathJax_Preview">α^t</span><script type="math/tex">α^t</script></span>计算编码器隐藏状态的权重和，得到注意力输出<span class="arithmatex"><span class="MathJax_Preview">a_t</span><script type="math/tex">a_t</script></span>：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
a_t = \sum_{i=1}^n{α^t_ih_i}∈R^h
</div>
<script type="math/tex; mode=display">
a_t = \sum_{i=1}^n{α^t_ih_i}∈R^h
</script>
</div>
<p>最后，将注意力输出<span class="arithmatex"><span class="MathJax_Preview">a_t</span><script type="math/tex">a_t</script></span>与解码器隐藏状态<span class="arithmatex"><span class="MathJax_Preview">s_t</span><script type="math/tex">s_t</script></span>拼接，后续的处理与没有注意力的seq2seq模型一样。</p>
<div class="arithmatex">
<div class="MathJax_Preview">
[a_t; s_t]∈R^h
</div>
<script type="math/tex; mode=display">
[a_t; s_t]∈R^h
</script>
</div>
<h6 id="33_1">3.3 注意力机制的优点<a class="headerlink" href="#33_1" title="Permanent link">&para;</a></h6>
<ul>
<li>显著提升了NMT性能</li>
<li>解决了3.1中提到的seq2seq架构中的问题</li>
<li>对解决梯度消失问题有一定帮助[?]</li>
<li>提供了一定的可解释性</li>
</ul>
<h6 id="34">3.4 注意力机制是一种广泛性的深度学习技巧<a class="headerlink" href="#34" title="Permanent link">&para;</a></h6>
<p>注意力可以用于多种架构（不限于seq2seq）和多种任务（不限于机器翻译）中。</p>
<p><strong>因此，一种更广义的注意力定义是：给定一组向量<span class="arithmatex"><span class="MathJax_Preview">values</span><script type="math/tex">values</script></span>（值），和一个向量<span class="arithmatex"><span class="MathJax_Preview">query</span><script type="math/tex">query</script></span>（查询），注意力是一种基于<span class="arithmatex"><span class="MathJax_Preview">query</span><script type="math/tex">query</script></span>计算<span class="arithmatex"><span class="MathJax_Preview">values</span><script type="math/tex">values</script></span>的带权重的和的技巧。</strong></p>
<h6 id="35">3.5 注意力分数的计算<a class="headerlink" href="#35" title="Permanent link">&para;</a></h6>
<p>假设<span class="arithmatex"><span class="MathJax_Preview">h_1,...,h_N∈R^{d1}</span><script type="math/tex">h_1,...,h_N∈R^{d1}</script></span>，<span class="arithmatex"><span class="MathJax_Preview">s∈R^{d2}</span><script type="math/tex">s∈R^{d2}</script></span>，那么注意力分数<span class="arithmatex"><span class="MathJax_Preview">e_t∈R^N</span><script type="math/tex">e_t∈R^N</script></span>有多种计算方式：</p>
<ol>
<li>
<p>点积注意力（Basic dot-product attention）
   $$
   e_i = s^Th_i∈R
   $$</p>
</li>
<li>
<p>乘法注意力（Multiplicative attention）
   $$
   e_i = s^TWh_i∈R
   $$
   其中，<span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span>是权重矩阵。</p>
</li>
<li>
<p>加性注意力（Additive attention）
   $$
   e_i = v^Ttanh(W_1h_i + W_2s)∈R
   $$
   其中，<span class="arithmatex"><span class="MathJax_Preview">W_1, W_2</span><script type="math/tex">W_1, W_2</script></span>是权重矩阵，<span class="arithmatex"><span class="MathJax_Preview">v</span><script type="math/tex">v</script></span>是权重向量。</p>
</li>
</ol>
</section>

<section id="mkdocs-terminal-after-content">
    
</section>
<section id="mkdocs-terminal-revision">
<br>
<aside>
    <p>
        <small>
            <i>Page last updated 2023-12-10. </i>
        </small>
    </p>
</aside>
</section>
            </main>
        </div>
        <hr><footer>
    <div class="terminal-mkdocs-footer-grid">
        <div id="terminal-mkdocs-footer-copyright-info">
            
            <p class="text-center text-muted">&copy; 2022-2023 by <a href='https://chuxiaoyu.github.io/' target='_blank'> Xiaoyu Chu </a></p>
             Site built with <a href="http://www.mkdocs.org">MkDocs</a> and <a href="https://github.com/ntno/mkdocs-terminal">Terminal for MkDocs</a>.
        </div>
        <div id="terminal-mkdocs-footer-prev-next">
            <nav class="btn-group">
                <a href="../../time-series/03-mlp-time-series-forecasting/" title="3.MLP for Time Series Forecasting">Previous</a>
                |
                <a href="../Lecture09/" title="Lecture09 自注意力模型、Transformers">Next</a>
            </nav>
        </div>
    </div>
</footer>
    </div>

    
    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="alertdialog" aria-modal="true" aria-labelledby="searchModalLabel">
    <div class="modal-dialog modal-lg" role="search">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="searchModalLabel">Search</h5>
                <button type="button" class="close btn btn-default btn-ghost" data-dismiss="modal"><span aria-hidden="true">x</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p id="searchInputLabel">Type to start searching</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" aria-labelledby="searchInputLabel" placeholder="" id="mkdocs-search-query" title="Please enter search terms here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No document matches found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>
    
    
</body>

</html>