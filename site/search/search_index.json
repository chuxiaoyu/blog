{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to My Blog \u00b6 HELLO<<","title":"\ud83c\udfe0 Home"},{"location":"#welcome-to-my-blog","text":"HELLO<<","title":"Welcome to My Blog"},{"location":"Lecture07/","text":"Lecture07: \u673a\u5668\u7ffb\u8bd1\u3001seq2seq\u6a21\u578b\u3001\u6ce8\u610f\u529b\u673a\u5236 \u00b6 \u672c\u8282\u4e3b\u8981\u5185\u5bb9 \u00b6 \u673a\u5668\u7ffb\u8bd1\u4efb\u52a1 seq2seq\u6a21\u578b\u67b6\u6784 \u6ce8\u610f\u529b\u673a\u5236 1 \u6df1\u5ea6\u5b66\u4e60\u4e4b\u524d\u7684\u673a\u5668\u7ffb\u8bd1 \u00b6 1.1 \u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u5b9a\u4e49 \u00b6 \u673a\u5668\u7ffb\u8bd1\uff08Machine Translation, MT\uff09\u4efb\u52a1\uff1a\u5c06\u53e5\u5b50 x x \u4ece\u4e00\u79cd\u8bed\u8a00\uff08Source Language\uff09\u7ffb\u8bd1\u6210\u53e6\u4e00\u79cd\u8bed\u8a00\uff08Target Language\uff09\u7684\u53e5\u5b50 y y \u3002 1.2 \u673a\u5668\u7ffb\u8bd1\u7684\u53d1\u5c55\u9636\u6bb5 \u00b6 1950s: \u65e9\u671f\u673a\u5668\u7ffb\u8bd1 1990s-2010s: \u57fa\u4e8e\u7edf\u8ba1\u7684\u673a\u5668\u7ffb\u8bd1\uff08Statistics Machine Translation, SMT\uff09 2014-: \u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u673a\u5668\u7ffb\u8bd1\uff08Neural Machine Translation, NMT\uff09 2017-: \u4ee5Transformer\u4e3a\u4ee3\u8868\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\u4ee3 1.3 \u57fa\u4e8e\u7edf\u8ba1\u7684\u673a\u5668\u7ffb\u8bd1 \u00b6 \u57fa\u4e8e\u7edf\u8ba1\u7684\u673a\u5668\u7ffb\u8bd1\uff08Statistics Machine Translation, SMT\uff09\u7684\u6838\u5fc3\u601d\u60f3\u662f\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u4e00\u4e2a\u6982\u7387\u6a21\u578b\u3002 \u4f8b\u5982\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u6cd5\u8bed\u8bed\u53e5 x x \uff0c\u627e\u51fa\u5bf9\u5e94\u7684\u6700\u5408\u9002\u7684\u82f1\u6587\u7ffb\u8bd1\u8bed\u53e5 y y \uff0c\u516c\u5f0f\u5982\u4e0b\uff1a $$ argmax_yP(y|x) $$ \u5229\u7528\u8d1d\u53f6\u65af\u516c\u5f0f\uff0c\u53ef\u4ee5\u5c06\u4e0a\u8ff0\u6982\u7387\u8f6c\u5316\u4e3a\u4e24\u90e8\u5206\uff0c\u5206\u522b\u8fdb\u884c\u5b66\u4e60\uff1a $$ argmax_yP(x|y)P(y) $$ \u4e0a\u5f0f\u4e2d\uff0c P(x|y) P(x|y) \u4ee3\u8868\u7ffb\u8bd1\u6a21\u578b\uff08Translation Model\uff09\uff0c\u8d1f\u8d23\u5b66\u4e60\u5982\u4f55\u51c6\u786e\u7ffb\u8bd1\u5355\u8bcd\u3001\u77ed\u8bed\uff08fidelity\uff0c\u4fdd\u771f\u6027\uff09\uff1b P(y) P(y) \u4ee3\u8868\u8bed\u8a00\u6a21\u578b\uff08Language Model\uff09\uff0c\u662f\u5bf9\u8bed\u8a00\u7684\u6d41\u7545\u6027\uff08fluency\uff09\u8fdb\u884c\u5b66\u4e60\u3002 \uff08\u6ce8\uff1a\u5bf9\u5e94\u7ffb\u8bd1\u7684\u4e24\u4e2a\u6807\u51c6\uff1a\u5fe0\u5b9e\u3001\u901a\u987a\uff09 \u90a3\u4e48\uff0c\u5982\u4f55\u5b66\u4e60\u7ffb\u8bd1\u6a21\u578b P(x|y) P(x|y) \uff1f \u9996\u5148\uff0c\u9700\u8981\u5927\u91cf\u7684\u5e73\u884c\u6570\u636e\uff08Parallel Data\uff09\uff0c\u5982\u5927\u91cf\u4eba\u5de5\u7ffb\u8bd1\u7684\u6cd5\u8bed/\u82f1\u8bed\u8bed\u53e5\u5bf9\u3002 \u7136\u540e\uff0c\u5f15\u5165\u9690\u53d8\u91cf a a \uff1a P(x, a|y) P(x, a|y) \uff0c a a \u662f alignment alignment \uff0c\u5373\u8bed\u53e5 x x \u548c\u8bed\u53e5 y y \u4e4b\u95f4\u7684\u5355\u8bcd\u7ea7\u522b\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u8fd9\u79cd\u5bf9\u5e94\u975e\u5e38\u8d1f\u8d23\uff0c\u8003\u8651\u4ee5\u4e0b\u60c5\u5f62\uff1a \u6709\u4e9b\u5355\u8bcd\u6ca1\u6709\u6216\u4e0d\u9700\u8981\u5bf9\u5e94\u7684\u7ffb\u8bd1\u8bcd\uff08no counterpart\uff09 \u591a\u4e2a\u5355\u8bcd\u5bf9\u5e94\u4e00\u4e2a\u7ffb\u8bd1\u8bcd\uff08many-to-one\uff09 \u4e00\u4e2a\u5355\u8bcd\u5bf9\u5e94\u591a\u4e2a\u7ffb\u8bd1\u8bcd\uff08one-to-many\uff09 \u591a\u4e2a\u5355\u8bcd\u5bf9\u5e94\u591a\u4e2a\u7ffb\u8bd1\u8bcd\uff08many-to-many\uff09 \u5bf9\u5e94\u5173\u7cfb alignment alignment \u6ca1\u6709\u663e\u6027\u5730\u5b58\u5728\u4e8e\u6570\u636e\u4e2d\uff0c\u56e0\u6b64\u9700\u8981\u7279\u6b8a\u7684\u5b66\u4e60\u7b97\u6cd5\uff08\u5982EM\u7b97\u6cd5\uff09\u3002 \u57fa\u4e8e\u7edf\u8ba1\u7684\u673a\u5668\u7ffb\u8bd1\u7f3a\u70b9\u5982\u4e0b\uff1a \u6574\u4e2a\u7ffb\u8bd1\u7cfb\u7edf\u53ca\u5176\u590d\u6742\uff0c\u5305\u542b\u5927\u91cf\u672a\u63d0\u53ca\u7684\u7ec6\u8282\u548c\u5355\u72ec\u8bbe\u8ba1\u7684\u5b50\u6a21\u5757 \u9700\u8981\u5927\u91cf\u7684\u7279\u5f81\u5de5\u7a0b\uff08\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u7684\u7279\u6b8a\u7528\u6cd5\u8bbe\u8ba1\u5404\u79cd\u7279\u5f81\uff09 \u9700\u8981\u5927\u91cf\u7684\u4eba\u529b\u548c\u6210\u672c\u6765\u7ef4\u62a4\u8bed\u6599\u8d44\u6e90 2 \u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u673a\u5668\u7ffb\u8bd1 \u00b6 \u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u673a\u5668\u7ffb\u8bd1\uff08Neural Machine Translation, NMT\uff09\u662f\u4e00\u79cd\u7528\u7aef\u5bf9\u7aef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u673a\u5668\u7ffb\u8bd1\u7684\u65b9\u5f0f\u3002\u8fd9\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u79f0\u4e3asequence-to-sequence\uff08seq2seq\uff09\u6a21\u578b\uff0c\u5305\u542b\u4e24\u4e2a\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNNs\uff09\u3002 seq2seq\u4e0d\u53ea\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u4f7f\u7528\uff0c\u8fd8\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4ee5\u4e0b\u4efb\u52a1\uff1a \u6587\u672c\u6458\u8981 \u5bf9\u8bdd\u7cfb\u7edf \u53e5\u6cd5\u89e3\u6790 \u4ee3\u7801\u751f\u6210 seq2seq\u53ef\u4ee5\u89c6\u4e3a\u6761\u4ef6\u8bed\u8a00\u6a21\u578b\uff08Conditional Language Model\uff09\u7684\u4e00\u79cd\uff1a \u8bed\u8a00\u6a21\u578b\u662f\u56e0\u4e3a\u5b83\u7684\u89e3\u7801\u5668\u7528\u4e8e\u9884\u6d4b\u76ee\u6807\u8bed\u53e5 y y \u7684\u4e0b\u4e00\u4e2a\u5355\u8bcd \u6761\u4ef6\u662f\u56e0\u4e3a\u5b83\u7684\u9884\u6d4b\u662f\u57fa\u4e8e\u6e90\u8bed\u53e5 x x \u7684\uff08\u6761\u4ef6\u6982\u7387\uff09 NMT\u76f4\u63a5\u8ba1\u7b97 P(y|x) P(y|x) \uff1a $$ P(y|x) = P(y_1|x)P(y_2|y_1,x)P(y_3|y_1,y_2,x)...P(y_T|y_1,...,y_{T-1}, x) $$ \u5176\u4e2d\uff0c P(y_T|y_1,...,y_{T-1}, x) P(y_T|y_1,...,y_{T-1}, x) \u8868\u793a\u7ed9\u5b9a\u5df2\u6709\u7684\u76ee\u6807\u5355\u8bcd\u548c\u6e90\u8bed\u53e5 x x \uff0c\u4e0b\u4e00\u4e2a\u76ee\u6807\u5355\u8bcd\u7684\u6982\u7387\u3002 \u8bad\u7ec3NMT\u9700\u8981\u5927\u91cf\u5e73\u884c\u8bed\u6599\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f7f\u7528\u591a\u5c42RNN\u3002 RNN\u6709\u4e24\u79cd\u89e3\u7801\u65b9\u5f0f\uff1a Greedy decoding Beam search decoding \u76f8\u8f83\u4e8eSMT\uff0cNMT\u6709\u5982\u4e0b\u4f18\u70b9\uff1a \u6027\u80fd\u66f4\u597d \u6ca1\u6709\u5b50\u6a21\u5757 \u964d\u4f4e\u4e86\u4eba\u529b\u9700\u6c42\uff08\u4e0d\u9700\u8981\u7279\u5f81\u5de5\u7a0b\u548c\u8003\u8651\u7279\u4f8b\uff09 \u540c\u65f6\u4e5f\u6709\u5982\u4e0b\u7f3a\u70b9\uff1a \u4e0d\u53ef\u89e3\u91ca\u6027 \u5f88\u96be\u53d7\u63a7\u5236 \u4eceBLEU\uff08Bilingual Evaluation Understudy\uff09\u8bc4\u6d4b\u7ed3\u679c\u53ef\u4ee5\u770b\u51fa\uff0cNMT\u7684\u6027\u80fd\u5df2\u7ecf\u8fdc\u8fdc\u8d85\u8fc7SMT\u3002 NMT\u53ef\u80fd\u662f\u6df1\u5ea6\u5b66\u4e60\u5728NLP\u4e2d\u6700\u6210\u529f\u7684\u5e94\u7528\uff1a 2014\u5e74\uff0cseq2seq\u8bba\u6587\u53d1\u8868\uff1b 2016\u5e74\uff0cNMT\u6210\u4e3a\u673a\u5668\u7ffb\u8bd1\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u8c37\u6b4c\u7ffb\u8bd1\u4eceSMT\u8f6c\u4e3aNMT\uff1b 2018\u5e74\uff0c\u6240\u6709\u516c\u53f8\u7684\u7ffb\u8bd1\u5de5\u5177\u5747\u5e94\u7528\u4e86NMT\u3002 \u7136\u800c\uff0c\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u5e76\u672a\u88ab\u5f7b\u5e95\u7ec8\u7ed3\uff0c\u8bb8\u591a\u56f0\u96be\u4ecd\u7136\u5b58\u5728\uff0c\u5982\u5e38\u8bc6\u4fe1\u606f\u672a\u5f97\u5230\u5e94\u7528\u3001\u80cc\u666f\u4fe1\u606f\u5728\u957f\u6587\u672c\u4e2d\u96be\u4ee5\u7ef4\u62a4\u4ee5\u53ca\u6a21\u578b\u504f\u89c1\u7b49\u3002 3 \u6ce8\u610f\u529b\u673a\u5236 \u00b6 3.1 seq2seq\u67b6\u6784\u5b58\u5728\u7684\u95ee\u9898 \u00b6 \u8bed\u4e49\u5411\u91cf\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u5230\u6574\u4e2a\u8f93\u5165\u5e8f\u5217\u7684\u4fe1\u606f\uff1b \u5148\u8f93\u5165\u5230\u7f51\u7edc\u7684\u5185\u5bb9\u643a\u5e26\u7684\u4fe1\u606f\u4f1a\u88ab\u540e\u8f93\u5165\u7684\u4fe1\u606f\u8986\u76d6\u6389\u3002\u8f93\u5165\u5e8f\u5217\u8d8a\u957f\uff0c\u8fd9\u4e2a\u73b0\u8c61\u5c31\u8d8a\u4e25\u91cd\u3002 \u56e0\u6b64\uff0cAttention\uff08\u6ce8\u610f\u529b\u673a\u5236\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6cd5\u3002 3.2 \u6709\u6ce8\u610f\u529b\u673a\u5236\u7684seq2seq\u6a21\u578b \u00b6 \u6ce8\u610f\u529b\u673a\u5236\u7684\u6838\u5fc3\u601d\u8def\u662f\uff1a\u5728\u89e3\u7801\u5668\u8fdb\u884c\u89e3\u7801\u7684\u6bcf\u4e00\u6b65\uff0c\u76f4\u63a5\u8fde\u63a5\u5230\u7f16\u7801\u5668\u6765\u5173\u6ce8\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u7279\u5b9a\u90e8\u5206\u3002 3.3 \u6ce8\u610f\u529b\u673a\u5236\u7684\u516c\u5f0f\u8868\u793a \u00b6 \u5047\u8bbe\u7f16\u7801\u5668\u9690\u85cf\u72b6\u6001\u4e3a h_1,...,h_N\u2208R^h h_1,...,h_N\u2208R^h \uff0c\u5728\u65f6\u95f4\u6b65 t t \u65f6\u89e3\u7801\u5668\u7684\u9690\u85cf\u72b6\u6001\u4e3a s_t\u2208R^h s_t\u2208R^h \uff0c\u5219\u8be5\u65f6\u95f4\u6b65\u7684\u6ce8\u610f\u529b\u5206\u6570 e_t e_t \u53ef\u4ee5\u8868\u793a\u4e3a\uff1a e_t = [s^T_th_1,...,s^T_th_N]\u2208R^N e_t = [s^T_th_1,...,s^T_th_N]\u2208R^N \u7136\u540e\u4f7f\u7528softmax\u5f97\u5230\u6ce8\u610f\u529b\u6982\u7387\u5206\u5e03 \u03b1^t \u03b1^t \uff1a \u03b1^t = softmax(e^t)\u2208R^N \u03b1^t = softmax(e^t)\u2208R^N \u5229\u7528 \u03b1^t \u03b1^t \u8ba1\u7b97\u7f16\u7801\u5668\u9690\u85cf\u72b6\u6001\u7684\u6743\u91cd\u548c\uff0c\u5f97\u5230\u6ce8\u610f\u529b\u8f93\u51fa a_t a_t \uff1a a_t = \\sum_{i=1}^n{\u03b1^t_ih_i}\u2208R^h a_t = \\sum_{i=1}^n{\u03b1^t_ih_i}\u2208R^h \u6700\u540e\uff0c\u5c06\u6ce8\u610f\u529b\u8f93\u51fa a_t a_t \u4e0e\u89e3\u7801\u5668\u9690\u85cf\u72b6\u6001 s_t s_t \u62fc\u63a5\uff0c\u540e\u7eed\u7684\u5904\u7406\u4e0e\u6ca1\u6709\u6ce8\u610f\u529b\u7684seq2seq\u6a21\u578b\u4e00\u6837\u3002 [a_t; s_t]\u2208R^h [a_t; s_t]\u2208R^h 3.3 \u6ce8\u610f\u529b\u673a\u5236\u7684\u4f18\u70b9 \u00b6 \u663e\u8457\u63d0\u5347\u4e86NMT\u6027\u80fd \u89e3\u51b3\u4e863.1\u4e2d\u63d0\u5230\u7684seq2seq\u67b6\u6784\u4e2d\u7684\u95ee\u9898 \u5bf9\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u6709\u4e00\u5b9a\u5e2e\u52a9[?] \u63d0\u4f9b\u4e86\u4e00\u5b9a\u7684\u53ef\u89e3\u91ca\u6027 3.4 \u6ce8\u610f\u529b\u673a\u5236\u662f\u4e00\u79cd\u5e7f\u6cdb\u6027\u7684\u6df1\u5ea6\u5b66\u4e60\u6280\u5de7 \u00b6 \u6ce8\u610f\u529b\u53ef\u4ee5\u7528\u4e8e\u591a\u79cd\u67b6\u6784\uff08\u4e0d\u9650\u4e8eseq2seq\uff09\u548c\u591a\u79cd\u4efb\u52a1\uff08\u4e0d\u9650\u4e8e\u673a\u5668\u7ffb\u8bd1\uff09\u4e2d\u3002 \u56e0\u6b64\uff0c\u4e00\u79cd\u66f4\u5e7f\u4e49\u7684\u6ce8\u610f\u529b\u5b9a\u4e49\u662f\uff1a\u7ed9\u5b9a\u4e00\u7ec4\u5411\u91cf values values \uff08\u503c\uff09\uff0c\u548c\u4e00\u4e2a\u5411\u91cf query query \uff08\u67e5\u8be2\uff09\uff0c\u6ce8\u610f\u529b\u662f\u4e00\u79cd\u57fa\u4e8e query query \u8ba1\u7b97 values values \u7684\u5e26\u6743\u91cd\u7684\u548c\u7684\u6280\u5de7\u3002 3.5 \u6ce8\u610f\u529b\u5206\u6570\u7684\u8ba1\u7b97 \u00b6 \u5047\u8bbe h_1,...,h_N\u2208R^{d1} h_1,...,h_N\u2208R^{d1} \uff0c s\u2208R^{d2} s\u2208R^{d2} \uff0c\u90a3\u4e48\u6ce8\u610f\u529b\u5206\u6570 e_t\u2208R^N e_t\u2208R^N \u6709\u591a\u79cd\u8ba1\u7b97\u65b9\u5f0f\uff1a \u70b9\u79ef\u6ce8\u610f\u529b\uff08Basic dot-product attention\uff09 $$ e_i = s^Th_i\u2208R $$ \u4e58\u6cd5\u6ce8\u610f\u529b\uff08Multiplicative attention\uff09 $$ e_i = s^TWh_i\u2208R $$ \u5176\u4e2d\uff0c W W \u662f\u6743\u91cd\u77e9\u9635\u3002 \u52a0\u6027\u6ce8\u610f\u529b\uff08Additive attention\uff09 $$ e_i = v^Ttanh(W_1h_i + W_2s)\u2208R $$ \u5176\u4e2d\uff0c W_1, W_2 W_1, W_2 \u662f\u6743\u91cd\u77e9\u9635\uff0c v v \u662f\u6743\u91cd\u5411\u91cf\u3002","title":"Lecture07 \u673a\u5668\u7ffb\u8bd1\u3001seq2seq\u6a21\u578b\u3001\u6ce8\u610f\u529b\u673a\u5236"},{"location":"Lecture07/#lecture07-seq2seq","text":"","title":"Lecture07: \u673a\u5668\u7ffb\u8bd1\u3001seq2seq\u6a21\u578b\u3001\u6ce8\u610f\u529b\u673a\u5236"},{"location":"Lecture07/#_1","text":"\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1 seq2seq\u6a21\u578b\u67b6\u6784 \u6ce8\u610f\u529b\u673a\u5236","title":"\u672c\u8282\u4e3b\u8981\u5185\u5bb9"},{"location":"Lecture07/#1","text":"","title":"1 \u6df1\u5ea6\u5b66\u4e60\u4e4b\u524d\u7684\u673a\u5668\u7ffb\u8bd1"},{"location":"Lecture07/#11","text":"\u673a\u5668\u7ffb\u8bd1\uff08Machine Translation, MT\uff09\u4efb\u52a1\uff1a\u5c06\u53e5\u5b50 x x \u4ece\u4e00\u79cd\u8bed\u8a00\uff08Source Language\uff09\u7ffb\u8bd1\u6210\u53e6\u4e00\u79cd\u8bed\u8a00\uff08Target Language\uff09\u7684\u53e5\u5b50 y y \u3002","title":"1.1 \u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u5b9a\u4e49"},{"location":"Lecture07/#12","text":"1950s: \u65e9\u671f\u673a\u5668\u7ffb\u8bd1 1990s-2010s: \u57fa\u4e8e\u7edf\u8ba1\u7684\u673a\u5668\u7ffb\u8bd1\uff08Statistics Machine Translation, SMT\uff09 2014-: \u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u673a\u5668\u7ffb\u8bd1\uff08Neural Machine Translation, NMT\uff09 2017-: \u4ee5Transformer\u4e3a\u4ee3\u8868\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\u4ee3","title":"1.2 \u673a\u5668\u7ffb\u8bd1\u7684\u53d1\u5c55\u9636\u6bb5"},{"location":"Lecture07/#13","text":"\u57fa\u4e8e\u7edf\u8ba1\u7684\u673a\u5668\u7ffb\u8bd1\uff08Statistics Machine Translation, SMT\uff09\u7684\u6838\u5fc3\u601d\u60f3\u662f\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u4e00\u4e2a\u6982\u7387\u6a21\u578b\u3002 \u4f8b\u5982\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u6cd5\u8bed\u8bed\u53e5 x x \uff0c\u627e\u51fa\u5bf9\u5e94\u7684\u6700\u5408\u9002\u7684\u82f1\u6587\u7ffb\u8bd1\u8bed\u53e5 y y \uff0c\u516c\u5f0f\u5982\u4e0b\uff1a $$ argmax_yP(y|x) $$ \u5229\u7528\u8d1d\u53f6\u65af\u516c\u5f0f\uff0c\u53ef\u4ee5\u5c06\u4e0a\u8ff0\u6982\u7387\u8f6c\u5316\u4e3a\u4e24\u90e8\u5206\uff0c\u5206\u522b\u8fdb\u884c\u5b66\u4e60\uff1a $$ argmax_yP(x|y)P(y) $$ \u4e0a\u5f0f\u4e2d\uff0c P(x|y) P(x|y) \u4ee3\u8868\u7ffb\u8bd1\u6a21\u578b\uff08Translation Model\uff09\uff0c\u8d1f\u8d23\u5b66\u4e60\u5982\u4f55\u51c6\u786e\u7ffb\u8bd1\u5355\u8bcd\u3001\u77ed\u8bed\uff08fidelity\uff0c\u4fdd\u771f\u6027\uff09\uff1b P(y) P(y) \u4ee3\u8868\u8bed\u8a00\u6a21\u578b\uff08Language Model\uff09\uff0c\u662f\u5bf9\u8bed\u8a00\u7684\u6d41\u7545\u6027\uff08fluency\uff09\u8fdb\u884c\u5b66\u4e60\u3002 \uff08\u6ce8\uff1a\u5bf9\u5e94\u7ffb\u8bd1\u7684\u4e24\u4e2a\u6807\u51c6\uff1a\u5fe0\u5b9e\u3001\u901a\u987a\uff09 \u90a3\u4e48\uff0c\u5982\u4f55\u5b66\u4e60\u7ffb\u8bd1\u6a21\u578b P(x|y) P(x|y) \uff1f \u9996\u5148\uff0c\u9700\u8981\u5927\u91cf\u7684\u5e73\u884c\u6570\u636e\uff08Parallel Data\uff09\uff0c\u5982\u5927\u91cf\u4eba\u5de5\u7ffb\u8bd1\u7684\u6cd5\u8bed/\u82f1\u8bed\u8bed\u53e5\u5bf9\u3002 \u7136\u540e\uff0c\u5f15\u5165\u9690\u53d8\u91cf a a \uff1a P(x, a|y) P(x, a|y) \uff0c a a \u662f alignment alignment \uff0c\u5373\u8bed\u53e5 x x \u548c\u8bed\u53e5 y y \u4e4b\u95f4\u7684\u5355\u8bcd\u7ea7\u522b\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u8fd9\u79cd\u5bf9\u5e94\u975e\u5e38\u8d1f\u8d23\uff0c\u8003\u8651\u4ee5\u4e0b\u60c5\u5f62\uff1a \u6709\u4e9b\u5355\u8bcd\u6ca1\u6709\u6216\u4e0d\u9700\u8981\u5bf9\u5e94\u7684\u7ffb\u8bd1\u8bcd\uff08no counterpart\uff09 \u591a\u4e2a\u5355\u8bcd\u5bf9\u5e94\u4e00\u4e2a\u7ffb\u8bd1\u8bcd\uff08many-to-one\uff09 \u4e00\u4e2a\u5355\u8bcd\u5bf9\u5e94\u591a\u4e2a\u7ffb\u8bd1\u8bcd\uff08one-to-many\uff09 \u591a\u4e2a\u5355\u8bcd\u5bf9\u5e94\u591a\u4e2a\u7ffb\u8bd1\u8bcd\uff08many-to-many\uff09 \u5bf9\u5e94\u5173\u7cfb alignment alignment \u6ca1\u6709\u663e\u6027\u5730\u5b58\u5728\u4e8e\u6570\u636e\u4e2d\uff0c\u56e0\u6b64\u9700\u8981\u7279\u6b8a\u7684\u5b66\u4e60\u7b97\u6cd5\uff08\u5982EM\u7b97\u6cd5\uff09\u3002 \u57fa\u4e8e\u7edf\u8ba1\u7684\u673a\u5668\u7ffb\u8bd1\u7f3a\u70b9\u5982\u4e0b\uff1a \u6574\u4e2a\u7ffb\u8bd1\u7cfb\u7edf\u53ca\u5176\u590d\u6742\uff0c\u5305\u542b\u5927\u91cf\u672a\u63d0\u53ca\u7684\u7ec6\u8282\u548c\u5355\u72ec\u8bbe\u8ba1\u7684\u5b50\u6a21\u5757 \u9700\u8981\u5927\u91cf\u7684\u7279\u5f81\u5de5\u7a0b\uff08\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u7684\u7279\u6b8a\u7528\u6cd5\u8bbe\u8ba1\u5404\u79cd\u7279\u5f81\uff09 \u9700\u8981\u5927\u91cf\u7684\u4eba\u529b\u548c\u6210\u672c\u6765\u7ef4\u62a4\u8bed\u6599\u8d44\u6e90","title":"1.3 \u57fa\u4e8e\u7edf\u8ba1\u7684\u673a\u5668\u7ffb\u8bd1"},{"location":"Lecture07/#2","text":"\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u673a\u5668\u7ffb\u8bd1\uff08Neural Machine Translation, NMT\uff09\u662f\u4e00\u79cd\u7528\u7aef\u5bf9\u7aef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u673a\u5668\u7ffb\u8bd1\u7684\u65b9\u5f0f\u3002\u8fd9\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u79f0\u4e3asequence-to-sequence\uff08seq2seq\uff09\u6a21\u578b\uff0c\u5305\u542b\u4e24\u4e2a\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNNs\uff09\u3002 seq2seq\u4e0d\u53ea\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u4f7f\u7528\uff0c\u8fd8\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4ee5\u4e0b\u4efb\u52a1\uff1a \u6587\u672c\u6458\u8981 \u5bf9\u8bdd\u7cfb\u7edf \u53e5\u6cd5\u89e3\u6790 \u4ee3\u7801\u751f\u6210 seq2seq\u53ef\u4ee5\u89c6\u4e3a\u6761\u4ef6\u8bed\u8a00\u6a21\u578b\uff08Conditional Language Model\uff09\u7684\u4e00\u79cd\uff1a \u8bed\u8a00\u6a21\u578b\u662f\u56e0\u4e3a\u5b83\u7684\u89e3\u7801\u5668\u7528\u4e8e\u9884\u6d4b\u76ee\u6807\u8bed\u53e5 y y \u7684\u4e0b\u4e00\u4e2a\u5355\u8bcd \u6761\u4ef6\u662f\u56e0\u4e3a\u5b83\u7684\u9884\u6d4b\u662f\u57fa\u4e8e\u6e90\u8bed\u53e5 x x \u7684\uff08\u6761\u4ef6\u6982\u7387\uff09 NMT\u76f4\u63a5\u8ba1\u7b97 P(y|x) P(y|x) \uff1a $$ P(y|x) = P(y_1|x)P(y_2|y_1,x)P(y_3|y_1,y_2,x)...P(y_T|y_1,...,y_{T-1}, x) $$ \u5176\u4e2d\uff0c P(y_T|y_1,...,y_{T-1}, x) P(y_T|y_1,...,y_{T-1}, x) \u8868\u793a\u7ed9\u5b9a\u5df2\u6709\u7684\u76ee\u6807\u5355\u8bcd\u548c\u6e90\u8bed\u53e5 x x \uff0c\u4e0b\u4e00\u4e2a\u76ee\u6807\u5355\u8bcd\u7684\u6982\u7387\u3002 \u8bad\u7ec3NMT\u9700\u8981\u5927\u91cf\u5e73\u884c\u8bed\u6599\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f7f\u7528\u591a\u5c42RNN\u3002 RNN\u6709\u4e24\u79cd\u89e3\u7801\u65b9\u5f0f\uff1a Greedy decoding Beam search decoding \u76f8\u8f83\u4e8eSMT\uff0cNMT\u6709\u5982\u4e0b\u4f18\u70b9\uff1a \u6027\u80fd\u66f4\u597d \u6ca1\u6709\u5b50\u6a21\u5757 \u964d\u4f4e\u4e86\u4eba\u529b\u9700\u6c42\uff08\u4e0d\u9700\u8981\u7279\u5f81\u5de5\u7a0b\u548c\u8003\u8651\u7279\u4f8b\uff09 \u540c\u65f6\u4e5f\u6709\u5982\u4e0b\u7f3a\u70b9\uff1a \u4e0d\u53ef\u89e3\u91ca\u6027 \u5f88\u96be\u53d7\u63a7\u5236 \u4eceBLEU\uff08Bilingual Evaluation Understudy\uff09\u8bc4\u6d4b\u7ed3\u679c\u53ef\u4ee5\u770b\u51fa\uff0cNMT\u7684\u6027\u80fd\u5df2\u7ecf\u8fdc\u8fdc\u8d85\u8fc7SMT\u3002 NMT\u53ef\u80fd\u662f\u6df1\u5ea6\u5b66\u4e60\u5728NLP\u4e2d\u6700\u6210\u529f\u7684\u5e94\u7528\uff1a 2014\u5e74\uff0cseq2seq\u8bba\u6587\u53d1\u8868\uff1b 2016\u5e74\uff0cNMT\u6210\u4e3a\u673a\u5668\u7ffb\u8bd1\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u8c37\u6b4c\u7ffb\u8bd1\u4eceSMT\u8f6c\u4e3aNMT\uff1b 2018\u5e74\uff0c\u6240\u6709\u516c\u53f8\u7684\u7ffb\u8bd1\u5de5\u5177\u5747\u5e94\u7528\u4e86NMT\u3002 \u7136\u800c\uff0c\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u5e76\u672a\u88ab\u5f7b\u5e95\u7ec8\u7ed3\uff0c\u8bb8\u591a\u56f0\u96be\u4ecd\u7136\u5b58\u5728\uff0c\u5982\u5e38\u8bc6\u4fe1\u606f\u672a\u5f97\u5230\u5e94\u7528\u3001\u80cc\u666f\u4fe1\u606f\u5728\u957f\u6587\u672c\u4e2d\u96be\u4ee5\u7ef4\u62a4\u4ee5\u53ca\u6a21\u578b\u504f\u89c1\u7b49\u3002","title":"2 \u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u673a\u5668\u7ffb\u8bd1"},{"location":"Lecture07/#3","text":"","title":"3 \u6ce8\u610f\u529b\u673a\u5236"},{"location":"Lecture07/#31-seq2seq","text":"\u8bed\u4e49\u5411\u91cf\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u5230\u6574\u4e2a\u8f93\u5165\u5e8f\u5217\u7684\u4fe1\u606f\uff1b \u5148\u8f93\u5165\u5230\u7f51\u7edc\u7684\u5185\u5bb9\u643a\u5e26\u7684\u4fe1\u606f\u4f1a\u88ab\u540e\u8f93\u5165\u7684\u4fe1\u606f\u8986\u76d6\u6389\u3002\u8f93\u5165\u5e8f\u5217\u8d8a\u957f\uff0c\u8fd9\u4e2a\u73b0\u8c61\u5c31\u8d8a\u4e25\u91cd\u3002 \u56e0\u6b64\uff0cAttention\uff08\u6ce8\u610f\u529b\u673a\u5236\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6cd5\u3002","title":"3.1 seq2seq\u67b6\u6784\u5b58\u5728\u7684\u95ee\u9898"},{"location":"Lecture07/#32-seq2seq","text":"\u6ce8\u610f\u529b\u673a\u5236\u7684\u6838\u5fc3\u601d\u8def\u662f\uff1a\u5728\u89e3\u7801\u5668\u8fdb\u884c\u89e3\u7801\u7684\u6bcf\u4e00\u6b65\uff0c\u76f4\u63a5\u8fde\u63a5\u5230\u7f16\u7801\u5668\u6765\u5173\u6ce8\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u7279\u5b9a\u90e8\u5206\u3002","title":"3.2 \u6709\u6ce8\u610f\u529b\u673a\u5236\u7684seq2seq\u6a21\u578b"},{"location":"Lecture07/#33","text":"\u5047\u8bbe\u7f16\u7801\u5668\u9690\u85cf\u72b6\u6001\u4e3a h_1,...,h_N\u2208R^h h_1,...,h_N\u2208R^h \uff0c\u5728\u65f6\u95f4\u6b65 t t \u65f6\u89e3\u7801\u5668\u7684\u9690\u85cf\u72b6\u6001\u4e3a s_t\u2208R^h s_t\u2208R^h \uff0c\u5219\u8be5\u65f6\u95f4\u6b65\u7684\u6ce8\u610f\u529b\u5206\u6570 e_t e_t \u53ef\u4ee5\u8868\u793a\u4e3a\uff1a e_t = [s^T_th_1,...,s^T_th_N]\u2208R^N e_t = [s^T_th_1,...,s^T_th_N]\u2208R^N \u7136\u540e\u4f7f\u7528softmax\u5f97\u5230\u6ce8\u610f\u529b\u6982\u7387\u5206\u5e03 \u03b1^t \u03b1^t \uff1a \u03b1^t = softmax(e^t)\u2208R^N \u03b1^t = softmax(e^t)\u2208R^N \u5229\u7528 \u03b1^t \u03b1^t \u8ba1\u7b97\u7f16\u7801\u5668\u9690\u85cf\u72b6\u6001\u7684\u6743\u91cd\u548c\uff0c\u5f97\u5230\u6ce8\u610f\u529b\u8f93\u51fa a_t a_t \uff1a a_t = \\sum_{i=1}^n{\u03b1^t_ih_i}\u2208R^h a_t = \\sum_{i=1}^n{\u03b1^t_ih_i}\u2208R^h \u6700\u540e\uff0c\u5c06\u6ce8\u610f\u529b\u8f93\u51fa a_t a_t \u4e0e\u89e3\u7801\u5668\u9690\u85cf\u72b6\u6001 s_t s_t \u62fc\u63a5\uff0c\u540e\u7eed\u7684\u5904\u7406\u4e0e\u6ca1\u6709\u6ce8\u610f\u529b\u7684seq2seq\u6a21\u578b\u4e00\u6837\u3002 [a_t; s_t]\u2208R^h [a_t; s_t]\u2208R^h","title":"3.3 \u6ce8\u610f\u529b\u673a\u5236\u7684\u516c\u5f0f\u8868\u793a"},{"location":"Lecture07/#33_1","text":"\u663e\u8457\u63d0\u5347\u4e86NMT\u6027\u80fd \u89e3\u51b3\u4e863.1\u4e2d\u63d0\u5230\u7684seq2seq\u67b6\u6784\u4e2d\u7684\u95ee\u9898 \u5bf9\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u6709\u4e00\u5b9a\u5e2e\u52a9[?] \u63d0\u4f9b\u4e86\u4e00\u5b9a\u7684\u53ef\u89e3\u91ca\u6027","title":"3.3 \u6ce8\u610f\u529b\u673a\u5236\u7684\u4f18\u70b9"},{"location":"Lecture07/#34","text":"\u6ce8\u610f\u529b\u53ef\u4ee5\u7528\u4e8e\u591a\u79cd\u67b6\u6784\uff08\u4e0d\u9650\u4e8eseq2seq\uff09\u548c\u591a\u79cd\u4efb\u52a1\uff08\u4e0d\u9650\u4e8e\u673a\u5668\u7ffb\u8bd1\uff09\u4e2d\u3002 \u56e0\u6b64\uff0c\u4e00\u79cd\u66f4\u5e7f\u4e49\u7684\u6ce8\u610f\u529b\u5b9a\u4e49\u662f\uff1a\u7ed9\u5b9a\u4e00\u7ec4\u5411\u91cf values values \uff08\u503c\uff09\uff0c\u548c\u4e00\u4e2a\u5411\u91cf query query \uff08\u67e5\u8be2\uff09\uff0c\u6ce8\u610f\u529b\u662f\u4e00\u79cd\u57fa\u4e8e query query \u8ba1\u7b97 values values \u7684\u5e26\u6743\u91cd\u7684\u548c\u7684\u6280\u5de7\u3002","title":"3.4 \u6ce8\u610f\u529b\u673a\u5236\u662f\u4e00\u79cd\u5e7f\u6cdb\u6027\u7684\u6df1\u5ea6\u5b66\u4e60\u6280\u5de7"},{"location":"Lecture07/#35","text":"\u5047\u8bbe h_1,...,h_N\u2208R^{d1} h_1,...,h_N\u2208R^{d1} \uff0c s\u2208R^{d2} s\u2208R^{d2} \uff0c\u90a3\u4e48\u6ce8\u610f\u529b\u5206\u6570 e_t\u2208R^N e_t\u2208R^N \u6709\u591a\u79cd\u8ba1\u7b97\u65b9\u5f0f\uff1a \u70b9\u79ef\u6ce8\u610f\u529b\uff08Basic dot-product attention\uff09 $$ e_i = s^Th_i\u2208R $$ \u4e58\u6cd5\u6ce8\u610f\u529b\uff08Multiplicative attention\uff09 $$ e_i = s^TWh_i\u2208R $$ \u5176\u4e2d\uff0c W W \u662f\u6743\u91cd\u77e9\u9635\u3002 \u52a0\u6027\u6ce8\u610f\u529b\uff08Additive attention\uff09 $$ e_i = v^Ttanh(W_1h_i + W_2s)\u2208R $$ \u5176\u4e2d\uff0c W_1, W_2 W_1, W_2 \u662f\u6743\u91cd\u77e9\u9635\uff0c v v \u662f\u6743\u91cd\u5411\u91cf\u3002","title":"3.5 \u6ce8\u610f\u529b\u5206\u6570\u7684\u8ba1\u7b97"},{"location":"Lecture09/","text":"Lecture09: \u81ea\u6ce8\u610f\u529b\u6a21\u578b\u3001Transformers \u00b6 \u672c\u8282\u4e3b\u8981\u5185\u5bb9 \u00b6 \u4eceRNN\u5230\u57fa\u4e8e\u6ce8\u610f\u529b\u7684NLP\u6a21\u578b Transformer\u6a21\u578b 1 \u4eceRNN\u5230\u57fa\u4e8e\u6ce8\u610f\u529b\u7684NLP\u6a21\u578b \u00b6 1.1 RNN\u6a21\u578b\u5b58\u5728\u7684\u95ee\u9898 \u00b6 \u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u8ddd\u79bb\uff08Linear interaction distance\uff09\uff0c\u5373\u957f\u8ddd\u79bb\u4f9d\u8d56\u95ee\u9898\u3002 \u7f3a\u5c11\u5e76\u884c\u6027\uff08parallelizability\uff09\u3002 \u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u4eba\u4eec\u8003\u8651\u5230\u4e86\u6ce8\u610f\u529b\u673a\u5236\u3002 [\u5b66\u751f\u63d0\u95ee]\u6ce8\u610f\u529b\u548c\u5168\u8fde\u63a5\u7f51\u7edc\u7684\u533a\u522b\u662f\u4ec0\u4e48\uff1f 1.\u6ce8\u610f\u529b\u7684\u6743\u91cd\u662f\u52a8\u6001\u7684 2.\u53c2\u6570\u7684\u8ba1\u7b97\u4e0d\u540c 1.2 \u81ea\u6ce8\u610f\u529b\u6a21\u578b\u4ecb\u7ecd \u00b6 \u6ce8\u610f\u529b\u673a\u5236\u7684\u8fd0\u4f5c\u9700\u8981queris, keys, values\u5411\u91cf\uff1a queries q_1, q_2,..., q_T q_1, q_2,..., q_T \uff0c q_i\u2208R^d q_i\u2208R^d keys k_1, k_2,..., k_T k_1, k_2,..., k_T \uff0c k_i\u2208R^d k_i\u2208R^d values v_1, v_2,..., v_T v_1, v_2,..., v_T \uff0c v_i\u2208R^d v_i\u2208R^d \u5728\u81ea\u6ce8\u610f\u529b\u6a21\u578b\uff08self-attention\uff09\u4e2d\uff0cqueries\uff0ckeys\uff0cvalues\u6765\u6e90\u76f8\u540c\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u67d0\u5c42\u7684\u8f93\u51fa\u662f x_1, x_2,..., x_T x_1, x_2,..., x_T \uff0c\u90a3\u4e48\u53ef\u4ee5\u4f7f v_i = k_i = q_i = x_i v_i = k_i = q_i = x_i [?] \u90a3\u4e48\uff0c\u81ea\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\uff08\u4ee5\u70b9\u79ef\u4e3a\u4f8b\uff09\u5982\u4e0b\uff1a \uff081\uff09\u8ba1\u7b97query-key\u4e58\u79ef\uff0c\u5f97\u5230\u6ce8\u610f\u529b\u5206\u6570 e_{ij} e_{ij} $$ e_{ij} = q_i^Tk_j $$ \uff082\uff09\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd \u03b1 \u03b1 $$ \u03b1_{ij} = softmax(e_{ij}) $$ \uff083\uff09\u8ba1\u7b97\u8f93\u51fa $$ output_i = \\sum_j{\u03b1_{ij}v_j} $$ 1.3 \u81ea\u6ce8\u610f\u529b\u6a21\u5757\u7684\u5904\u7406 \u00b6 1.3.1 \u4f4d\u7f6e\u7f16\u7801 \u00b6 \u56e0\u4e3a\u81ea\u6ce8\u610f\u529b\u6ca1\u6709\u8003\u8651\u4f4d\u7f6e\u4fe1\u606f\uff0c\u6240\u4ee5\u9700\u8981\u5c06\u5e8f\u5217\u7684\u4f4d\u7f6e\u7f16\u7801\u5230keys\uff0cqueries\uff0cvalues\u5411\u91cf\u4e2d\u3002 \u8003\u8651\u5c06\u5e8f\u5217\u7d22\u5f15\uff08sequence index\uff09\u7528\u5411\u91cf\uff08vector\uff09\u8868\u793a\uff1a $$ p_i\u2208R^d, for\\; i\u2208{1,2,...,T} $$ p_i p_i \u5373\u4f4d\u7f6e\u5411\u91cf\uff08positional vector\uff09\u3002 \u5f97\u5230\u4f4d\u7f6e\u5411\u91cf\u540e\uff0c\u6211\u4eec\u5c06\u5176\u52a0\u5230\u8f93\u5165\u91cc\u3002\u5047\u8bbe \\widetilde q \\widetilde q \uff0c \\widetilde k \\widetilde k \uff0c \\widetilde v \\widetilde v \u662f\u4e4b\u524d\u7684\u5411\u91cf\uff0c\u5219\uff1a $$ q_i =\\widetilde q_i + p_i\\ k_i =\\widetilde k_i + p_i\\ v_i =\\widetilde v_i + p_i\\ $$ [?]\u4f4d\u7f6e\u5411\u91cf\u4e0d\u662f\u52a0\u5230\u8f93\u5165\u91cc\u5417\uff1f\u4e3a\u4ec0\u4e48\u8fd9\u91cc\u662f\u52a0\u5165\u5230q,k,v\uff1f \u4f4d\u7f6e\u5411\u91cf\u6709\u591a\u79cd\u8ba1\u7b97\u65b9\u5f0f\uff0c\u5982\u6b63\u5f26\u4f4d\u7f6e\u8868\u793a\u7b49\uff0c\u6700\u5e38\u7528\u7684\u662f\u7edd\u5bf9\u4f4d\u7f6e\u8868\u793a\uff08absolute position representations\uff09\u3002\uff08\u6ce8\uff1a\u6709\u70b9\u50cf\u72ec\u70ed\u7f16\u7801\uff09 1.3.2 \u906e\u7f69 \u00b6 \u8fdb\u884c\u5e8f\u5217\u9884\u6d4b\u7684\u65f6\u5019\uff0c\u4e0d\u80fd\u770b\u5230\u540e\u9762\u7684\u4fe1\u606f\uff0c\u56e0\u6b64\u91c7\u7528\u4e86\u906e\u7f69\uff08Masking\uff09\u5904\u7406\uff0c\u5373\u5c06\u540e\u9762\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u6570\u8bbe\u7f6e\u4e3a -\\infty -\\infty \uff1a e_{ij} = \\left\\{ \\begin{aligned} q^T_ik_j, j<i\\\\ -\\infty, j\u2265i \\end{aligned} \\right. e_{ij} = \\left\\{ \\begin{aligned} q^T_ik_j, j<i\\\\ -\\infty, j\u2265i \\end{aligned} \\right. 2 Transformer\u6a21\u578b\u4ecb\u7ecd \u00b6 2.1 Transformer\u6982\u89c8 \u00b6 2.2 Transformer\u7f16\u7801\u5668 \u00b6 \u7f16\u7801\u5668\uff08Encoder\uff09\u5305\u542b\u4ee5\u4e0b\u6a21\u5757\uff1a Q-K-V\u5411\u91cf \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff08Multi-head attention\uff09 \u5176\u4ed6\u8bad\u7ec3\u6280\u5de7\uff08\u8fd9\u4e9b\u6280\u5de7\u4e0d\u80fd\u63d0\u5347\u6a21\u578b\u80fd\u505a\u4ec0\u4e48\uff0c\u800c\u662f\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\uff09 \u6b8b\u5dee\u8fde\u63a5\uff08Residual connections\uff09 \u5f52\u4e00\u5316\uff08Layer normalization\uff09 \u7f29\u653e\u70b9\u79ef\u8fd0\u7b97\uff08Scaled Dot Product\uff09 2.2.1 Query-Key-Value\u5411\u91cf\u77e9\u9635 \u00b6 \u7528 x_1,...,x_T x_1,...,x_T (x_i\u2208R^d) (x_i\u2208R^d) \u8868\u793aTransformer\u7f16\u7801\u5668\u7684\u8f93\u5165\u5411\u91cf\uff0c\u5219queries, keys, values\u7684\u8ba1\u7b97\u5982\u4e0b\uff1a q_i=Qx_i q_i=Qx_i \uff0c Q\u2208R^{d\u00d7d} Q\u2208R^{d\u00d7d} \u662f\u6743\u91cd\u77e9\u9635 k_i=Kx_i k_i=Kx_i \uff0c K\u2208R^{d\u00d7d} K\u2208R^{d\u00d7d} \u662f\u6743\u91cd\u77e9\u9635 v_i=Vx_i v_i=Vx_i \uff0c V\u2208R^{d\u00d7d} V\u2208R^{d\u00d7d} \u662f\u6743\u91cd\u77e9\u9635 \u5bf9\u4e0d\u540c\u7684\u53c2\u6570\u77e9\u9635\u5bf9\u539f\u59cb\u8f93\u5165\u5411\u91cf\u505a\u7ebf\u6027\u53d8\u6362\uff0c\u4ece\u800c\u8ba9\u4e0d\u540c\u7684\u53d8\u6362\u7ed3\u679c\u627f\u62c5\u4e0d\u540c\u89d2\u8272\u3002 \u8ba9\u6211\u4eec\u901a\u8fc7\u77e9\u9635\u7684\u89c6\u89d2\u6765\u770bQ\uff0cK\uff0cV\u662f\u5982\u4f55\u8ba1\u7b97\u7684\uff1a \u9996\u5148\uff0c\u7528 X=[x_1;...;x_T]\u2208R^{T\u00d7d} X=[x_1;...;x_T]\u2208R^{T\u00d7d} \u8868\u793a\u8f93\u5165\u5411\u91cf\u7684\u62fc\u63a5\u77e9\u9635\uff0c\u90a3\u4e48 XQ\u2208R^{T\u00d7d} XQ\u2208R^{T\u00d7d} \uff0c XK\u2208R^{T\u00d7d} XK\u2208R^{T\u00d7d} \uff0c XV\u2208R^{T\u00d7d} XV\u2208R^{T\u00d7d} \u3002\u8f93\u51fa\u5c31\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a $$ output = softmax(XQ(XK)^T)\u00d7XV $$ 2.2.2 \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236 \u00b6 \u5bf9\u4e8e\u5355\u8bcd i i \uff0c\u81ea\u6ce8\u610f\u529b\u53ea\u6ce8\u610f\u5230 x^T_iQ^TKx_j x^T_iQ^TKx_j \u9ad8\u7684\u5730\u65b9\uff0c\u4f46\u662f\u6211\u4eec\u5982\u4f55\u5173\u6ce8\u5230\u4e0d\u540c\u7684 j j \u5462\uff1f \u8fd9\u91cc\u5bf9\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528\u591a\u5934\u7684\u89e3\u91ca\u5e76\u4e0d\u6e05\u695a\uff0c\u53ef\u4ee5\u53c2\u8003\uff1a\u4e3a\u4ec0\u4e48Transformer \u9700\u8981\u8fdb\u884c Multi-head Attention\uff1f - \u77e5\u4e4e https://www.zhihu.com/question/341222779 \u300a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u2014\u2014\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\u300bP93\u4e2d\u7684\u89e3\u91ca\u662f\uff1a \u201c\u7531\u4e8e\u81ea\u6ce8\u610f\u529b\u7ed3\u679c\u9700\u8981\u7ecf\u8fc7\u5f52\u4e00\u5316\uff0c\u5bfc\u81f4\u5373\u4f7f\u4e00\u4e2a\u8f93\u5165\u548c\u591a\u4e2a\u5176\u4ed6\u7684\u8f93\u5165\u76f8\u5173\uff0c\u4e5f\u65e0\u6cd5\u540c\u65f6\u4e3a\u8fd9\u4e9b\u8f93\u5165\u8d4b\u4e88\u8f83\u5927\u7684\u6ce8\u610f\u529b\u503c\uff0c\u5373\u81ea\u6ce8\u610f\u529b\u7ed3\u679c\u4e4b\u95f4\u662f\u4e92\u65a5\u7684\uff0c\u65e0\u6cd5\u540c\u65f6\u5173\u6ce8\u591a\u4e2a\u8f93\u5165\u3002\u56e0\u6b64\uff0c\u5982\u679c\u80fd\u4f7f\u7528\u591a\u7ec4\u6ce8\u610f\u529b\u6a21\u578b\u4ea7\u751f\u591a\u7ec4\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u7ed3\u679c\uff0c\u5219\u4e0d\u540c\u7ec4\u6ce8\u610f\u529b\u6a21\u578b\u53ef\u80fd\u5173\u6ce8\u5230\u4e0d\u540c\u7684\u8f93\u5165\u4e0a\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002\u201c \u6211\u4eec\u901a\u8fc7\u591a\u4e2a Q, K, V Q, K, V \u77e9\u9635\u5b9a\u4e49\u591a\u5934\u6ce8\u610f\u529b\uff08Multi-headed Attention\uff09\u3002 \u7528 Q_\u2113,K_\u2113,V_\u2113\u2208R^{d\u00d7d/h} Q_\u2113,K_\u2113,V_\u2113\u2208R^{d\u00d7d/h} \u8868\u793a\u4e0d\u540c\u7684\u53c2\u6570\u77e9\u9635\uff0c\u5176\u4e2d h h \u8868\u793a\u6ce8\u610f\u529b\u5934\u7684\u5e8f\u53f7\uff0c \u2113 \u2113 \u7684\u53d6\u503c\u8303\u56f4\u662f\u4ece 1 1 \u5230 h h \u3002 ( R^{d\u00d7d/h} R^{d\u00d7d/h} ) \u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u72ec\u7acb\u8fd0\u7b97\uff1a $$ output_\u2113 = softmax(XQ_\u2113K T_\u2113X T)*XV_\u2113 $$ \u5176\u4e2d output_\u2113\u2208R^{d/h} output_\u2113\u2208R^{d/h} \u3002 \u7136\u540e\uff0c\u5c06\u6240\u6709\u7684\u8f93\u51fa\u6df7\u5408\uff1a $$ output = Y[output_1;...;output_h], Y\u2208R^{d\u00d7d} $$ \u4e0b\u56fe\u662f\u5355\u5934\u6ce8\u610f\u529b\u548c\u591a\u5934\u6ce8\u610f\u529b\u7684\u7b80\u5355\u793a\u610f\u56fe\uff1a \u53ef\u4ee5\u770b\u51fa\uff0c\u591a\u5934\u6ce8\u610f\u529b\u548c\u5355\u4e2a\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u91cf\u662f\u4e00\u6837\u7684\u3002 \uff08\u90fd\u662f\u628a\u77e9\u9635\u62fc\u8d77\u6765\u8ba1\u7b97\u4e00\u6b21\uff09 2.2.3 \u6b8b\u5dee\u8fde\u63a5 \u00b6 \u6b8b\u5dee\u8fde\u63a5\uff08Residual connections\uff09\u662f\u4e00\u79cd\u63d0\u5347\u6a21\u578b\u8bad\u7ec3\u6548\u679c\u7684\u6280\u5de7\u3002 \u6b63\u5e38\u60c5\u51b5\uff1a X^{(i)} = Layer(X^{(i-1)}) X^{(i)} = Layer(X^{(i-1)}) \u6b8b\u5dee\u8fde\u63a5\uff1a X^{(i)} = X^{(i-1)}+Layer(X^{(i-1)}) X^{(i)} = X^{(i-1)}+Layer(X^{(i-1)}) 2.2.4 \u5f52\u4e00\u5316 \u00b6 \u5f52\u4e00\u5316\uff08Layer normalization\uff09\u662f\u4e00\u79cd\u63d0\u5347\u6a21\u578b\u8bad\u7ec3\u901f\u5ea6\u7684\u6280\u5de7\u3002 $$ output = \\frac{x-\u03bc}{\\sqrt{\ud835\udf0e}+\ud835\udf16}*\ud835\udefe+\ud835\udefd $$ \u5176\u4e2d\uff0c\ud835\udf07\u662f\u5747\u503c\uff0c\ud835\udf0e\u662f\u6807\u51c6\u5dee\u3002\ud835\udefe\u548c\ud835\udefd\u662fgain\u548cbias\u53c2\u6570[?]\u3002 2.2.5 \u7f29\u653e\u70b9\u79ef\u8fd0\u7b97 \u00b6 \u7f29\u653e\u70b9\u79ef\u8fd0\u7b97\u662f\u4e3a\u4e86\u9632\u6b62\u5728\u7ef4\u6570\u8fc7\u5927\u65f6\uff0c\u68af\u5ea6\u53d8\u5c0f\u6216\u6d88\u5931\u3002 \u6b63\u5e38\u60c5\u51b5\uff1a output_\u2113 = softmax(XQ_\u2113K_\u2113^TX^T)*XV_\u2113 output_\u2113 = softmax(XQ_\u2113K_\u2113^TX^T)*XV_\u2113 \u7f29\u653e\u70b9\u79ef\u8fd0\u7b97\uff1a output_\u2113 = softmax(\\frac{XQ_\u2113K_\u2113^TX^T}{\\sqrt{d/h}})*XV_\u2113 output_\u2113 = softmax(\\frac{XQ_\u2113K_\u2113^TX^T}{\\sqrt{d/h}})*XV_\u2113 \u53ef\u4ee5\u770b\u51fa\uff0c\u5c31\u662f\u5c06\u6ce8\u610f\u529b\u5206\u6570\u9664\u4ee5\u7ef4\u6570 d d \u9664\u4ee5\u6ce8\u610f\u529b\u5934\u7684\u6570\u91cf h h \u7684\u6839\u3002 2.2.6 \u5c0f\u7ed3 \u00b6 2.1\u4e2d\u7ed9\u51fa\u4e86Tranformer\u7f16\u7801\u5668\u7684\u6574\u4f53\u6982\u89c8\uff0c\u7ecf\u8fc7\u5bf9\u6a21\u5757\u7ec4\u6210\u7684\u5206\u89e3\uff0c\u7f16\u7801\u5668\u66f4\u5177\u4f53\u7684\u7ed3\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a 2.3 Transformer\u89e3\u7801\u5668 \u00b6 \u89e3\u7801\u5668\u7684\u7ed3\u6784\u4e0e\u7f16\u7801\u5668\u7c7b\u4f3c\uff0c\u5982\u56fe\uff1a \u53ef\u4ee5\u770b\u51fa\uff0c\u7a0d\u5fae\u4e0d\u4e00\u6837\u7684\u5730\u65b9\u5728\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\uff08Cross attention\uff09\u3002 \u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236 \u5047\u8bbe h_1,...,h_T h_1,...,h_T \u662fTransformer\u7f16\u7801\u5668\u7684\u8f93\u51fa\u5411\u91cf\uff0c z_1,...,z_T z_1,...,z_T \u662fTransformer\u89e3\u7801\u5668\u7684\u8f93\u5165\u5411\u91cf\uff0c\u90a3\u4e48\uff0c keys\u548cvalues\u6765\u81ea\u7f16\u7801\u5668\uff1a k_i = Kh_i,v_i = Vh_i k_i = Kh_i,v_i = Vh_i queries\u6765\u81ea\u89e3\u7801\u5668\uff1a q_i=Qz_i q_i=Qz_i . \u5047\u8bbe H = [h_1;...;h_T] H = [h_1;...;h_T] \u662f\u7f16\u7801\u5668\u5411\u91cf\u7684\u62fc\u63a5\uff0c Z = [z_1;...;z_T] Z = [z_1;...;z_T] \u662f\u89e3\u7801\u5668\u5411\u91cf\u7684\u62fc\u63a5\uff0c\u90a3\u4e48\u8f93\u51fa\u53ef\u4ee5\u5b9a\u4e49\u4e3a\uff1a $$ output = softmax(ZQ(HK)^T)\u00d7HV $$","title":"Lecture09 \u81ea\u6ce8\u610f\u529b\u6a21\u578b\u3001Transformers"},{"location":"Lecture09/#lecture09-transformers","text":"","title":"Lecture09: \u81ea\u6ce8\u610f\u529b\u6a21\u578b\u3001Transformers"},{"location":"Lecture09/#_1","text":"\u4eceRNN\u5230\u57fa\u4e8e\u6ce8\u610f\u529b\u7684NLP\u6a21\u578b Transformer\u6a21\u578b","title":"\u672c\u8282\u4e3b\u8981\u5185\u5bb9"},{"location":"Lecture09/#1-rnnnlp","text":"","title":"1 \u4eceRNN\u5230\u57fa\u4e8e\u6ce8\u610f\u529b\u7684NLP\u6a21\u578b"},{"location":"Lecture09/#11-rnn","text":"\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u8ddd\u79bb\uff08Linear interaction distance\uff09\uff0c\u5373\u957f\u8ddd\u79bb\u4f9d\u8d56\u95ee\u9898\u3002 \u7f3a\u5c11\u5e76\u884c\u6027\uff08parallelizability\uff09\u3002 \u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u4eba\u4eec\u8003\u8651\u5230\u4e86\u6ce8\u610f\u529b\u673a\u5236\u3002 [\u5b66\u751f\u63d0\u95ee]\u6ce8\u610f\u529b\u548c\u5168\u8fde\u63a5\u7f51\u7edc\u7684\u533a\u522b\u662f\u4ec0\u4e48\uff1f 1.\u6ce8\u610f\u529b\u7684\u6743\u91cd\u662f\u52a8\u6001\u7684 2.\u53c2\u6570\u7684\u8ba1\u7b97\u4e0d\u540c","title":"1.1 RNN\u6a21\u578b\u5b58\u5728\u7684\u95ee\u9898"},{"location":"Lecture09/#12","text":"\u6ce8\u610f\u529b\u673a\u5236\u7684\u8fd0\u4f5c\u9700\u8981queris, keys, values\u5411\u91cf\uff1a queries q_1, q_2,..., q_T q_1, q_2,..., q_T \uff0c q_i\u2208R^d q_i\u2208R^d keys k_1, k_2,..., k_T k_1, k_2,..., k_T \uff0c k_i\u2208R^d k_i\u2208R^d values v_1, v_2,..., v_T v_1, v_2,..., v_T \uff0c v_i\u2208R^d v_i\u2208R^d \u5728\u81ea\u6ce8\u610f\u529b\u6a21\u578b\uff08self-attention\uff09\u4e2d\uff0cqueries\uff0ckeys\uff0cvalues\u6765\u6e90\u76f8\u540c\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u67d0\u5c42\u7684\u8f93\u51fa\u662f x_1, x_2,..., x_T x_1, x_2,..., x_T \uff0c\u90a3\u4e48\u53ef\u4ee5\u4f7f v_i = k_i = q_i = x_i v_i = k_i = q_i = x_i [?] \u90a3\u4e48\uff0c\u81ea\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\uff08\u4ee5\u70b9\u79ef\u4e3a\u4f8b\uff09\u5982\u4e0b\uff1a \uff081\uff09\u8ba1\u7b97query-key\u4e58\u79ef\uff0c\u5f97\u5230\u6ce8\u610f\u529b\u5206\u6570 e_{ij} e_{ij} $$ e_{ij} = q_i^Tk_j $$ \uff082\uff09\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd \u03b1 \u03b1 $$ \u03b1_{ij} = softmax(e_{ij}) $$ \uff083\uff09\u8ba1\u7b97\u8f93\u51fa $$ output_i = \\sum_j{\u03b1_{ij}v_j} $$","title":"1.2 \u81ea\u6ce8\u610f\u529b\u6a21\u578b\u4ecb\u7ecd"},{"location":"Lecture09/#13","text":"","title":"1.3 \u81ea\u6ce8\u610f\u529b\u6a21\u5757\u7684\u5904\u7406"},{"location":"Lecture09/#131","text":"\u56e0\u4e3a\u81ea\u6ce8\u610f\u529b\u6ca1\u6709\u8003\u8651\u4f4d\u7f6e\u4fe1\u606f\uff0c\u6240\u4ee5\u9700\u8981\u5c06\u5e8f\u5217\u7684\u4f4d\u7f6e\u7f16\u7801\u5230keys\uff0cqueries\uff0cvalues\u5411\u91cf\u4e2d\u3002 \u8003\u8651\u5c06\u5e8f\u5217\u7d22\u5f15\uff08sequence index\uff09\u7528\u5411\u91cf\uff08vector\uff09\u8868\u793a\uff1a $$ p_i\u2208R^d, for\\; i\u2208{1,2,...,T} $$ p_i p_i \u5373\u4f4d\u7f6e\u5411\u91cf\uff08positional vector\uff09\u3002 \u5f97\u5230\u4f4d\u7f6e\u5411\u91cf\u540e\uff0c\u6211\u4eec\u5c06\u5176\u52a0\u5230\u8f93\u5165\u91cc\u3002\u5047\u8bbe \\widetilde q \\widetilde q \uff0c \\widetilde k \\widetilde k \uff0c \\widetilde v \\widetilde v \u662f\u4e4b\u524d\u7684\u5411\u91cf\uff0c\u5219\uff1a $$ q_i =\\widetilde q_i + p_i\\ k_i =\\widetilde k_i + p_i\\ v_i =\\widetilde v_i + p_i\\ $$ [?]\u4f4d\u7f6e\u5411\u91cf\u4e0d\u662f\u52a0\u5230\u8f93\u5165\u91cc\u5417\uff1f\u4e3a\u4ec0\u4e48\u8fd9\u91cc\u662f\u52a0\u5165\u5230q,k,v\uff1f \u4f4d\u7f6e\u5411\u91cf\u6709\u591a\u79cd\u8ba1\u7b97\u65b9\u5f0f\uff0c\u5982\u6b63\u5f26\u4f4d\u7f6e\u8868\u793a\u7b49\uff0c\u6700\u5e38\u7528\u7684\u662f\u7edd\u5bf9\u4f4d\u7f6e\u8868\u793a\uff08absolute position representations\uff09\u3002\uff08\u6ce8\uff1a\u6709\u70b9\u50cf\u72ec\u70ed\u7f16\u7801\uff09","title":"1.3.1 \u4f4d\u7f6e\u7f16\u7801"},{"location":"Lecture09/#132","text":"\u8fdb\u884c\u5e8f\u5217\u9884\u6d4b\u7684\u65f6\u5019\uff0c\u4e0d\u80fd\u770b\u5230\u540e\u9762\u7684\u4fe1\u606f\uff0c\u56e0\u6b64\u91c7\u7528\u4e86\u906e\u7f69\uff08Masking\uff09\u5904\u7406\uff0c\u5373\u5c06\u540e\u9762\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u6570\u8bbe\u7f6e\u4e3a -\\infty -\\infty \uff1a e_{ij} = \\left\\{ \\begin{aligned} q^T_ik_j, j<i\\\\ -\\infty, j\u2265i \\end{aligned} \\right. e_{ij} = \\left\\{ \\begin{aligned} q^T_ik_j, j<i\\\\ -\\infty, j\u2265i \\end{aligned} \\right.","title":"1.3.2 \u906e\u7f69"},{"location":"Lecture09/#2-transformer","text":"","title":"2 Transformer\u6a21\u578b\u4ecb\u7ecd"},{"location":"Lecture09/#21-transformer","text":"","title":"2.1 Transformer\u6982\u89c8"},{"location":"Lecture09/#22-transformer","text":"\u7f16\u7801\u5668\uff08Encoder\uff09\u5305\u542b\u4ee5\u4e0b\u6a21\u5757\uff1a Q-K-V\u5411\u91cf \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff08Multi-head attention\uff09 \u5176\u4ed6\u8bad\u7ec3\u6280\u5de7\uff08\u8fd9\u4e9b\u6280\u5de7\u4e0d\u80fd\u63d0\u5347\u6a21\u578b\u80fd\u505a\u4ec0\u4e48\uff0c\u800c\u662f\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\uff09 \u6b8b\u5dee\u8fde\u63a5\uff08Residual connections\uff09 \u5f52\u4e00\u5316\uff08Layer normalization\uff09 \u7f29\u653e\u70b9\u79ef\u8fd0\u7b97\uff08Scaled Dot Product\uff09","title":"2.2 Transformer\u7f16\u7801\u5668"},{"location":"Lecture09/#221-query-key-value","text":"\u7528 x_1,...,x_T x_1,...,x_T (x_i\u2208R^d) (x_i\u2208R^d) \u8868\u793aTransformer\u7f16\u7801\u5668\u7684\u8f93\u5165\u5411\u91cf\uff0c\u5219queries, keys, values\u7684\u8ba1\u7b97\u5982\u4e0b\uff1a q_i=Qx_i q_i=Qx_i \uff0c Q\u2208R^{d\u00d7d} Q\u2208R^{d\u00d7d} \u662f\u6743\u91cd\u77e9\u9635 k_i=Kx_i k_i=Kx_i \uff0c K\u2208R^{d\u00d7d} K\u2208R^{d\u00d7d} \u662f\u6743\u91cd\u77e9\u9635 v_i=Vx_i v_i=Vx_i \uff0c V\u2208R^{d\u00d7d} V\u2208R^{d\u00d7d} \u662f\u6743\u91cd\u77e9\u9635 \u5bf9\u4e0d\u540c\u7684\u53c2\u6570\u77e9\u9635\u5bf9\u539f\u59cb\u8f93\u5165\u5411\u91cf\u505a\u7ebf\u6027\u53d8\u6362\uff0c\u4ece\u800c\u8ba9\u4e0d\u540c\u7684\u53d8\u6362\u7ed3\u679c\u627f\u62c5\u4e0d\u540c\u89d2\u8272\u3002 \u8ba9\u6211\u4eec\u901a\u8fc7\u77e9\u9635\u7684\u89c6\u89d2\u6765\u770bQ\uff0cK\uff0cV\u662f\u5982\u4f55\u8ba1\u7b97\u7684\uff1a \u9996\u5148\uff0c\u7528 X=[x_1;...;x_T]\u2208R^{T\u00d7d} X=[x_1;...;x_T]\u2208R^{T\u00d7d} \u8868\u793a\u8f93\u5165\u5411\u91cf\u7684\u62fc\u63a5\u77e9\u9635\uff0c\u90a3\u4e48 XQ\u2208R^{T\u00d7d} XQ\u2208R^{T\u00d7d} \uff0c XK\u2208R^{T\u00d7d} XK\u2208R^{T\u00d7d} \uff0c XV\u2208R^{T\u00d7d} XV\u2208R^{T\u00d7d} \u3002\u8f93\u51fa\u5c31\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a $$ output = softmax(XQ(XK)^T)\u00d7XV $$","title":"2.2.1 Query-Key-Value\u5411\u91cf\u77e9\u9635"},{"location":"Lecture09/#222","text":"\u5bf9\u4e8e\u5355\u8bcd i i \uff0c\u81ea\u6ce8\u610f\u529b\u53ea\u6ce8\u610f\u5230 x^T_iQ^TKx_j x^T_iQ^TKx_j \u9ad8\u7684\u5730\u65b9\uff0c\u4f46\u662f\u6211\u4eec\u5982\u4f55\u5173\u6ce8\u5230\u4e0d\u540c\u7684 j j \u5462\uff1f \u8fd9\u91cc\u5bf9\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528\u591a\u5934\u7684\u89e3\u91ca\u5e76\u4e0d\u6e05\u695a\uff0c\u53ef\u4ee5\u53c2\u8003\uff1a\u4e3a\u4ec0\u4e48Transformer \u9700\u8981\u8fdb\u884c Multi-head Attention\uff1f - \u77e5\u4e4e https://www.zhihu.com/question/341222779 \u300a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u2014\u2014\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\u300bP93\u4e2d\u7684\u89e3\u91ca\u662f\uff1a \u201c\u7531\u4e8e\u81ea\u6ce8\u610f\u529b\u7ed3\u679c\u9700\u8981\u7ecf\u8fc7\u5f52\u4e00\u5316\uff0c\u5bfc\u81f4\u5373\u4f7f\u4e00\u4e2a\u8f93\u5165\u548c\u591a\u4e2a\u5176\u4ed6\u7684\u8f93\u5165\u76f8\u5173\uff0c\u4e5f\u65e0\u6cd5\u540c\u65f6\u4e3a\u8fd9\u4e9b\u8f93\u5165\u8d4b\u4e88\u8f83\u5927\u7684\u6ce8\u610f\u529b\u503c\uff0c\u5373\u81ea\u6ce8\u610f\u529b\u7ed3\u679c\u4e4b\u95f4\u662f\u4e92\u65a5\u7684\uff0c\u65e0\u6cd5\u540c\u65f6\u5173\u6ce8\u591a\u4e2a\u8f93\u5165\u3002\u56e0\u6b64\uff0c\u5982\u679c\u80fd\u4f7f\u7528\u591a\u7ec4\u6ce8\u610f\u529b\u6a21\u578b\u4ea7\u751f\u591a\u7ec4\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u7ed3\u679c\uff0c\u5219\u4e0d\u540c\u7ec4\u6ce8\u610f\u529b\u6a21\u578b\u53ef\u80fd\u5173\u6ce8\u5230\u4e0d\u540c\u7684\u8f93\u5165\u4e0a\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002\u201c \u6211\u4eec\u901a\u8fc7\u591a\u4e2a Q, K, V Q, K, V \u77e9\u9635\u5b9a\u4e49\u591a\u5934\u6ce8\u610f\u529b\uff08Multi-headed Attention\uff09\u3002 \u7528 Q_\u2113,K_\u2113,V_\u2113\u2208R^{d\u00d7d/h} Q_\u2113,K_\u2113,V_\u2113\u2208R^{d\u00d7d/h} \u8868\u793a\u4e0d\u540c\u7684\u53c2\u6570\u77e9\u9635\uff0c\u5176\u4e2d h h \u8868\u793a\u6ce8\u610f\u529b\u5934\u7684\u5e8f\u53f7\uff0c \u2113 \u2113 \u7684\u53d6\u503c\u8303\u56f4\u662f\u4ece 1 1 \u5230 h h \u3002 ( R^{d\u00d7d/h} R^{d\u00d7d/h} ) \u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u72ec\u7acb\u8fd0\u7b97\uff1a $$ output_\u2113 = softmax(XQ_\u2113K T_\u2113X T)*XV_\u2113 $$ \u5176\u4e2d output_\u2113\u2208R^{d/h} output_\u2113\u2208R^{d/h} \u3002 \u7136\u540e\uff0c\u5c06\u6240\u6709\u7684\u8f93\u51fa\u6df7\u5408\uff1a $$ output = Y[output_1;...;output_h], Y\u2208R^{d\u00d7d} $$ \u4e0b\u56fe\u662f\u5355\u5934\u6ce8\u610f\u529b\u548c\u591a\u5934\u6ce8\u610f\u529b\u7684\u7b80\u5355\u793a\u610f\u56fe\uff1a \u53ef\u4ee5\u770b\u51fa\uff0c\u591a\u5934\u6ce8\u610f\u529b\u548c\u5355\u4e2a\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u91cf\u662f\u4e00\u6837\u7684\u3002 \uff08\u90fd\u662f\u628a\u77e9\u9635\u62fc\u8d77\u6765\u8ba1\u7b97\u4e00\u6b21\uff09","title":"2.2.2 \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236"},{"location":"Lecture09/#223","text":"\u6b8b\u5dee\u8fde\u63a5\uff08Residual connections\uff09\u662f\u4e00\u79cd\u63d0\u5347\u6a21\u578b\u8bad\u7ec3\u6548\u679c\u7684\u6280\u5de7\u3002 \u6b63\u5e38\u60c5\u51b5\uff1a X^{(i)} = Layer(X^{(i-1)}) X^{(i)} = Layer(X^{(i-1)}) \u6b8b\u5dee\u8fde\u63a5\uff1a X^{(i)} = X^{(i-1)}+Layer(X^{(i-1)}) X^{(i)} = X^{(i-1)}+Layer(X^{(i-1)})","title":"2.2.3 \u6b8b\u5dee\u8fde\u63a5"},{"location":"Lecture09/#224","text":"\u5f52\u4e00\u5316\uff08Layer normalization\uff09\u662f\u4e00\u79cd\u63d0\u5347\u6a21\u578b\u8bad\u7ec3\u901f\u5ea6\u7684\u6280\u5de7\u3002 $$ output = \\frac{x-\u03bc}{\\sqrt{\ud835\udf0e}+\ud835\udf16}*\ud835\udefe+\ud835\udefd $$ \u5176\u4e2d\uff0c\ud835\udf07\u662f\u5747\u503c\uff0c\ud835\udf0e\u662f\u6807\u51c6\u5dee\u3002\ud835\udefe\u548c\ud835\udefd\u662fgain\u548cbias\u53c2\u6570[?]\u3002","title":"2.2.4 \u5f52\u4e00\u5316"},{"location":"Lecture09/#225","text":"\u7f29\u653e\u70b9\u79ef\u8fd0\u7b97\u662f\u4e3a\u4e86\u9632\u6b62\u5728\u7ef4\u6570\u8fc7\u5927\u65f6\uff0c\u68af\u5ea6\u53d8\u5c0f\u6216\u6d88\u5931\u3002 \u6b63\u5e38\u60c5\u51b5\uff1a output_\u2113 = softmax(XQ_\u2113K_\u2113^TX^T)*XV_\u2113 output_\u2113 = softmax(XQ_\u2113K_\u2113^TX^T)*XV_\u2113 \u7f29\u653e\u70b9\u79ef\u8fd0\u7b97\uff1a output_\u2113 = softmax(\\frac{XQ_\u2113K_\u2113^TX^T}{\\sqrt{d/h}})*XV_\u2113 output_\u2113 = softmax(\\frac{XQ_\u2113K_\u2113^TX^T}{\\sqrt{d/h}})*XV_\u2113 \u53ef\u4ee5\u770b\u51fa\uff0c\u5c31\u662f\u5c06\u6ce8\u610f\u529b\u5206\u6570\u9664\u4ee5\u7ef4\u6570 d d \u9664\u4ee5\u6ce8\u610f\u529b\u5934\u7684\u6570\u91cf h h \u7684\u6839\u3002","title":"2.2.5 \u7f29\u653e\u70b9\u79ef\u8fd0\u7b97"},{"location":"Lecture09/#226","text":"2.1\u4e2d\u7ed9\u51fa\u4e86Tranformer\u7f16\u7801\u5668\u7684\u6574\u4f53\u6982\u89c8\uff0c\u7ecf\u8fc7\u5bf9\u6a21\u5757\u7ec4\u6210\u7684\u5206\u89e3\uff0c\u7f16\u7801\u5668\u66f4\u5177\u4f53\u7684\u7ed3\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a","title":"2.2.6 \u5c0f\u7ed3"},{"location":"Lecture09/#23-transformer","text":"\u89e3\u7801\u5668\u7684\u7ed3\u6784\u4e0e\u7f16\u7801\u5668\u7c7b\u4f3c\uff0c\u5982\u56fe\uff1a \u53ef\u4ee5\u770b\u51fa\uff0c\u7a0d\u5fae\u4e0d\u4e00\u6837\u7684\u5730\u65b9\u5728\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\uff08Cross attention\uff09\u3002 \u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236 \u5047\u8bbe h_1,...,h_T h_1,...,h_T \u662fTransformer\u7f16\u7801\u5668\u7684\u8f93\u51fa\u5411\u91cf\uff0c z_1,...,z_T z_1,...,z_T \u662fTransformer\u89e3\u7801\u5668\u7684\u8f93\u5165\u5411\u91cf\uff0c\u90a3\u4e48\uff0c keys\u548cvalues\u6765\u81ea\u7f16\u7801\u5668\uff1a k_i = Kh_i,v_i = Vh_i k_i = Kh_i,v_i = Vh_i queries\u6765\u81ea\u89e3\u7801\u5668\uff1a q_i=Qz_i q_i=Qz_i . \u5047\u8bbe H = [h_1;...;h_T] H = [h_1;...;h_T] \u662f\u7f16\u7801\u5668\u5411\u91cf\u7684\u62fc\u63a5\uff0c Z = [z_1;...;z_T] Z = [z_1;...;z_T] \u662f\u89e3\u7801\u5668\u5411\u91cf\u7684\u62fc\u63a5\uff0c\u90a3\u4e48\u8f93\u51fa\u53ef\u4ee5\u5b9a\u4e49\u4e3a\uff1a $$ output = softmax(ZQ(HK)^T)\u00d7HV $$","title":"2.3 Transformer\u89e3\u7801\u5668"},{"location":"Lecture10/","text":"Lecture10: \u9884\u8bad\u7ec3 \u00b6 \u672c\u8282\u4e3b\u8981\u5185\u5bb9 \u00b6 \u5b50\u8bcd\u6a21\u578b\u56de\u987e \u9884\u8bad\u7ec3\u76843\u79cd\u65b9\u6cd5 1 \u5b50\u8bcd\u6a21\u578b\u56de\u987e \u00b6 1.1 \u72ec\u70ed\u7f16\u7801 \u00b6 \u5047\u8bbe\u6211\u4eec\u6709\u4e00\u4e2a\u56fa\u5b9a\u7684\u8bcd\u8868\uff0c\u90a3\u4e48\uff1a \u8fd9\u79cd\u8bcd\u8868\u793a\u65b9\u6cd5\u7684\u6548\u679c\u4e0d\u662f\u5f88\u597d\uff0c\u56e0\u4e3a\u5355\u8bcd\u6709\u975e\u5e38\u591a\u7684\u5f62\u6001\u3002 1.2 \u5b57\u8282\u5bf9\u7f16\u7801\u7b97\u6cd5 \u00b6 \u4e00\u79cd\u89e3\u51b3\u601d\u8def\u662f\u5b57\u8282\u5bf9\u7f16\u7801\uff08byte-pair encoding algorithm\uff09\uff0c\u5b83\u628a\u5355\u8bcd\u5206\u89e3\u6210\u4e00\u7cfb\u5217\u7684\u5b50\u8bcd\uff08subwords\uff09\u7ec4\u6210\u7684\u5e8f\u5217\uff0c\u8fd9\u6837\u7a00\u6709\u8bcd\u5c31\u53ef\u4ee5\u88ab\u8868\u793a\u3002 2 \u4ece\u8bcd\u5411\u91cf\u4e2d\u83b7\u5f97\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u542f\u53d1 \u00b6 2.1 \u9884\u8bad\u7ec3\u8bcd\u5411\u91cf \u00b6 \u5927\u7ea62017\u5f00\u59cb\uff0c\u5f00\u59cb\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\uff08\u6ca1\u6709\u80cc\u666f\u4fe1\u606f\uff09\uff1a 2.2 \u9884\u8bad\u7ec3\u6a21\u578b\u6574\u4f53 \u00b6 \u5728\u73b0\u5728\u7684NLP\u4e2d\uff0c\u6240\u6709\u53c2\u6570\u90fd\u901a\u8fc7\u9884\u8bad\u7ec3\u8fdb\u884c\u521d\u59cb\u5316\u3002 2.3 \u9884\u8bad\u7ec3/\u5fae\u8c03\u8303\u5f0f \u00b6 \u9884\u8bad\u7ec3\u901a\u8fc7\u6a21\u578b\u521d\u59cb\u5316\u6765\u63d0\u5347NLP\u4efb\u52a1\u7684\u5e94\u7528\u6548\u679c\u3002 2.4 \u968f\u673a\u68af\u5ea6\u4e0b\u964d\u548c\u9884\u8bad\u7ec3/\u5fae\u8c03 \u00b6 \u4ece\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u89d2\u5ea6\u770b\uff0c\u4e3a\u4ec0\u4e48\u9884\u8bad\u7ec3/\u5fae\u8c03\u662f\u6709\u6548\u7684\uff1f \u5047\u8bbe\u53c2\u6570 \\hat{\ud835\udf03} \\hat{\ud835\udf03} \u53ef\u4ee5\u6700\u5c0f\u5316\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u635f\u5931\u51fd\u6570 min_\ud835\udf03\u2112_{pretrain}(\ud835\udf03) min_\ud835\udf03\u2112_{pretrain}(\ud835\udf03) \uff0c\u90a3\u4e48\u5fae\u8c03\u9636\u6bb5\u7684\u635f\u5931\u51fd\u6570 min_\ud835\udf03\u2112_{fineturn}(\ud835\udf03) min_\ud835\udf03\u2112_{fineturn}(\ud835\udf03) \u4ece \\hat{\ud835\udf03} \\hat{\ud835\udf03} \u5f00\u59cb\u8fdb\u884c\u521d\u59cb\u5316\u3002 \u8fd9\u4e24\u53e5\u8bdd\u4e0d\u662f\u5f88\u7406\u89e3 \u3002 3 \u4e09\u79cd\u9884\u8bad\u7ec3\u65b9\u6cd5 \u00b6 3.1 \u89e3\u7801\u5668 \u00b6 3.2 \u7f16\u7801\u5668 \u00b6 3.3 \u7f16\u7801\u5668-\u89e3\u7801\u5668 \u00b6","title":"Lecture10 \u9884\u8bad\u7ec3"},{"location":"Lecture10/#lecture10","text":"","title":"Lecture10: \u9884\u8bad\u7ec3"},{"location":"Lecture10/#_1","text":"\u5b50\u8bcd\u6a21\u578b\u56de\u987e \u9884\u8bad\u7ec3\u76843\u79cd\u65b9\u6cd5","title":"\u672c\u8282\u4e3b\u8981\u5185\u5bb9"},{"location":"Lecture10/#1","text":"","title":"1 \u5b50\u8bcd\u6a21\u578b\u56de\u987e"},{"location":"Lecture10/#11","text":"\u5047\u8bbe\u6211\u4eec\u6709\u4e00\u4e2a\u56fa\u5b9a\u7684\u8bcd\u8868\uff0c\u90a3\u4e48\uff1a \u8fd9\u79cd\u8bcd\u8868\u793a\u65b9\u6cd5\u7684\u6548\u679c\u4e0d\u662f\u5f88\u597d\uff0c\u56e0\u4e3a\u5355\u8bcd\u6709\u975e\u5e38\u591a\u7684\u5f62\u6001\u3002","title":"1.1 \u72ec\u70ed\u7f16\u7801"},{"location":"Lecture10/#12","text":"\u4e00\u79cd\u89e3\u51b3\u601d\u8def\u662f\u5b57\u8282\u5bf9\u7f16\u7801\uff08byte-pair encoding algorithm\uff09\uff0c\u5b83\u628a\u5355\u8bcd\u5206\u89e3\u6210\u4e00\u7cfb\u5217\u7684\u5b50\u8bcd\uff08subwords\uff09\u7ec4\u6210\u7684\u5e8f\u5217\uff0c\u8fd9\u6837\u7a00\u6709\u8bcd\u5c31\u53ef\u4ee5\u88ab\u8868\u793a\u3002","title":"1.2 \u5b57\u8282\u5bf9\u7f16\u7801\u7b97\u6cd5"},{"location":"Lecture10/#2","text":"","title":"2 \u4ece\u8bcd\u5411\u91cf\u4e2d\u83b7\u5f97\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u542f\u53d1"},{"location":"Lecture10/#21","text":"\u5927\u7ea62017\u5f00\u59cb\uff0c\u5f00\u59cb\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\uff08\u6ca1\u6709\u80cc\u666f\u4fe1\u606f\uff09\uff1a","title":"2.1 \u9884\u8bad\u7ec3\u8bcd\u5411\u91cf"},{"location":"Lecture10/#22","text":"\u5728\u73b0\u5728\u7684NLP\u4e2d\uff0c\u6240\u6709\u53c2\u6570\u90fd\u901a\u8fc7\u9884\u8bad\u7ec3\u8fdb\u884c\u521d\u59cb\u5316\u3002","title":"2.2 \u9884\u8bad\u7ec3\u6a21\u578b\u6574\u4f53"},{"location":"Lecture10/#23","text":"\u9884\u8bad\u7ec3\u901a\u8fc7\u6a21\u578b\u521d\u59cb\u5316\u6765\u63d0\u5347NLP\u4efb\u52a1\u7684\u5e94\u7528\u6548\u679c\u3002","title":"2.3 \u9884\u8bad\u7ec3/\u5fae\u8c03\u8303\u5f0f"},{"location":"Lecture10/#24","text":"\u4ece\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u89d2\u5ea6\u770b\uff0c\u4e3a\u4ec0\u4e48\u9884\u8bad\u7ec3/\u5fae\u8c03\u662f\u6709\u6548\u7684\uff1f \u5047\u8bbe\u53c2\u6570 \\hat{\ud835\udf03} \\hat{\ud835\udf03} \u53ef\u4ee5\u6700\u5c0f\u5316\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u635f\u5931\u51fd\u6570 min_\ud835\udf03\u2112_{pretrain}(\ud835\udf03) min_\ud835\udf03\u2112_{pretrain}(\ud835\udf03) \uff0c\u90a3\u4e48\u5fae\u8c03\u9636\u6bb5\u7684\u635f\u5931\u51fd\u6570 min_\ud835\udf03\u2112_{fineturn}(\ud835\udf03) min_\ud835\udf03\u2112_{fineturn}(\ud835\udf03) \u4ece \\hat{\ud835\udf03} \\hat{\ud835\udf03} \u5f00\u59cb\u8fdb\u884c\u521d\u59cb\u5316\u3002 \u8fd9\u4e24\u53e5\u8bdd\u4e0d\u662f\u5f88\u7406\u89e3 \u3002","title":"2.4 \u968f\u673a\u68af\u5ea6\u4e0b\u964d\u548c\u9884\u8bad\u7ec3/\u5fae\u8c03"},{"location":"Lecture10/#3","text":"","title":"3 \u4e09\u79cd\u9884\u8bad\u7ec3\u65b9\u6cd5"},{"location":"Lecture10/#31","text":"","title":"3.1 \u89e3\u7801\u5668"},{"location":"Lecture10/#32","text":"","title":"3.2 \u7f16\u7801\u5668"},{"location":"Lecture10/#33-","text":"","title":"3.3 \u7f16\u7801\u5668-\u89e3\u7801\u5668"},{"location":"Lecture11/","text":"Lecture11: \u95ee\u7b54\u7cfb\u7edf \u00b6 \u672c\u8282\u4e3b\u8981\u5185\u5bb9 \u00b6 \u95ee\u7b54\u7cfb\u7edf\u7b80\u4ecb \u9605\u8bfb\u7406\u89e3\u4efb\u52a1 \u5f00\u653e\u9886\u57df\u7684\u95ee\u7b54\u4efb\u52a1 \u6559\u6388\uff1a Danqi Chen 1 \u95ee\u7b54\u7b80\u4ecb \u00b6 1.1 \u95ee\u7b54\u7684\u5b9a\u4e49 \u00b6 1.2 \u95ee\u7b54\u7684\u5206\u7c7b \u00b6 \u6309\u95ee\u7b54\u7cfb\u7edf\u4f7f\u7528\u7684\u4fe1\u606f\u6765\u6e90\u6765\u770b\uff0c\u6709\u6587\u7ae0\u6587\u672c\u3001\u7f51\u9875\u6587\u672c\u3001\u77e5\u8bc6\u5e93\u3001\u8868\u683c\u3001\u56fe\u50cf\u7b49\uff1b \u6309\u95ee\u9898\u7c7b\u578b\u6765\u770b\uff0c\u6709\u4e8b\u5b9e\u6027\u6216\u975e\u4e8b\u5b9e\u6027\u95ee\u9898\u3001\u5f00\u653e\u57df\u6216\u5c01\u95ed\u57df\u95ee\u9898\u3001\u5355\u4e00\u95ee\u9898\u6216\u7ec4\u5408\u95ee\u9898\uff1b \u4ece\u56de\u7b54\u7c7b\u578b\u6765\u770b\uff0c\u6709\u6458\u8981\u3001\u6bb5\u843d\u3001\u5217\u8868\u3001\u5bf9\u9519\u7b49\u3002 1.3 \u95ee\u7b54\u7684\u5e94\u7528 \u00b6 \u641c\u7d22\u5f15\u64ce \u8bed\u97f3\u52a9\u624b \u8fdb\u5165\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\u540e\uff0c\u51e0\u4e4e\u6240\u6709\u6700\u5148\u8fdb\u7684\u95ee\u7b54\u7cfb\u7edf\u90fd\u5efa\u7acb\u5728\u7aef\u5230\u7aef\u8bad\u7ec3\u548c\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u4e4b\u4e0a\uff08\u4f8b\u5982\uff0cBERT\uff09\u3002 1.4 \u6587\u672c\u95ee\u7b54\u4e4b\u5916 \u00b6 \u672c\u8282\u91cd\u70b9\u662f\u57fa\u4e8e\u975e\u7ed3\u6784\u5316\u6587\u672c\u7684\u95ee\u7b54\u7cfb\u7edf\u3002 \u975e\u7ed3\u6784\u5316\u6587\u672c\u4e4b\u5916\uff0c\u8fd8\u6709\u57fa\u4e8e\u6570\u636e\u5e93/\u77e5\u8bc6\u5e93\u7684\u95ee\u7b54\u3001\u89c6\u89c9\u95ee\u7b54\u7b49\u9886\u57df\u3002 2 \u9605\u8bfb\u7406\u89e3\u4efb\u52a1 \u00b6 2.1 \u9605\u8bfb\u7406\u89e3\u5b9a\u4e49 \u00b6 \u9605\u8bfb\u7406\u89e3\u4efb\u52a1\u662f\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7684\u91cd\u8981\u5e94\u7528\uff0c\u5f88\u591aNLP\u4efb\u52a1\u53ef\u4ee5\u8f6c\u5316\u4e3a\u9605\u8bfb\u7406\u89e3\uff0c\u4f8b\u5982\u4fe1\u606f\u62bd\u53d6\u3001\u8bed\u4e49\u6807\u6ce8\u7b49\u3002 2.2 \u65af\u5766\u798fQA\u6570\u636e\u96c6\uff08SQuAD\uff09 \u00b6 SQuAD\u662f2016\u5e74\u7531\u65af\u5766\u798f\u5927\u5b66\u7684\u7814\u7a76\u8005\u5efa\u7acb\u7684\u95ee\u7b54\u6570\u636e\u96c6\u3002 \uff08\u6240\u4ee5\u53ea\u7528\u4e86\u90a3\u4e9b\u80fd\u7528\u539f\u6587\u56de\u7b54\u7684\u95ee\u9898\uff09 SQuAD\u6709\u4e24\u4e2a\u8bc4\u4f30\u6307\u6807\uff1a EM\uff08\u7cbe\u786e\u5339\u914d\uff0cExact Match\uff09\uff080 \u6216 1\uff09 F1\uff08\u5177\u4f53\u8ba1\u7b97\u65b9\u5f0f\u53c2\u8003\u539f\u8bba\u6587\uff09 2.3 \u9605\u8bfb\u7406\u89e3\u5efa\u6a21 \u00b6 \u89e3\u51b3\u8be5\u4efb\u52a1\u4e3b\u8981\u6709\u4e24\u7c7b\u6a21\u578b\uff1a 2.4 seq2seq\u56de\u987e\uff1a\u673a\u5668\u7ffb\u8bd1\u4e0e\u9605\u8bfb\u7406\u89e3\u7684\u5f02\u540c \u00b6 \u673a\u5668\u7ffb\u8bd1\u4e2d\uff0c\u6709source sentence\u548ctarget sentence\uff1b\u9605\u8bfb\u7406\u89e3\u4e5f\u6709\u4e24\u6bb5\u6587\u672c\uff1apassage\u548cquestion\uff08\u5b57\u6570\u4e0d\u5747\u8861\uff09\u3002 \u673a\u5668\u7ffb\u8bd1\u4e2d\uff0c\u9700\u8981\u627e\u5230\u4e0etarget word\u76f8\u5173\u7684source word\uff1b\u9605\u8bfb\u7406\u89e3\u4e2d\uff0c\u9700\u8981\u627e\u5230\u4e0equestion\u6700\u76f8\u5173\u7684passage\u4e2d\u7684\u5355\u8bcd\u3002\uff08\u90fd\u9700\u8981\u6ce8\u610f\u529b\u673a\u5236\u6765\u5b9e\u73b0\uff09 \u673a\u5668\u7ffb\u8bd1\u9700\u8981\u89e3\u7801\u5668\u751f\u6210\u7b54\u6848\uff1b\u4f46\u662f\u9605\u8bfb\u7406\u89e3\u4efb\u52a1\u9700\u8981\u4e24\u4e2a\u5206\u7c7b\u5668\u9884\u6d4b\u7b54\u6848\u7684\u8d77\u59cb\u4f4d\u7f6e\u548c\u7ed3\u675f\u4f4d\u7f6e\u3002 2.5 BiDAF\u6a21\u578b \u00b6 BiDAF: the Bidirectional Attention Flow model \u6574\u4f53\u67b6\u6784\u5982\u4e0b\uff1a 2.5.1 \u7f16\u7801\u5c42\uff1aEncoding \u00b6 2.5.2 \u6ce8\u610f\u529b\u5c42\uff1aAttention \u00b6 \u4e24\u79cd\u6ce8\u610f\u529b\uff1a Context-to-query attention: \u5bf9\u4e8e\u6bcf\u4e2acontext word\uff0c\u4ecequery words\u4e2d\u9009\u62e9\u6700\u76f8\u5173\u7684\u8bcd\u3002 Query-to-context attention: \u9009\u62e9\u4e0e query words\u6700\u76f8\u5173\u7684context words\u3002 \u6ce8\u610f\u529b\u53ef\u89c6\u5316\uff1a 2.5.3 \u8f93\u51fa\u5c42\uff1aModeling and output layers \u00b6 Modeling layer: \u4e24\u5c42\u53cc\u5411LSTM\u7f51\u7edc\u3002 Output layer\uff1a\u4e24\u4e2a\u5206\u7c7b\u5668\uff0c\u5206\u522b\u9884\u6d4b\u8d77\u59cb\u4f4d\u7f6e\u548c\u7ed3\u675f\u4f4d\u7f6e\u3002 2.6 BERT\u6a21\u578b \u00b6 BERT\u505a\u9605\u8bfb\u7406\u89e3\u4efb\u52a1\uff0c\u601d\u8def\u5982\u4e0b\uff1a 2.7 BiDAF\u548cBERT\u7684\u5f02\u540c \u00b6 \u4e0d\u540c\u70b9\uff1a BERT\u53c2\u6570\u66f4\u591a BiDAF\u57fa\u4e8eBiLSTM\uff0cBERT\u57fa\u4e8eTransformer BERT\u7ecf\u8fc7\u9884\u8bad\u7ec3\uff0c\u800cBiDAF\u9664\u4e86\u7528GloVe\u8fdb\u884c\u7f16\u7801\u5916\uff0c\u5176\u4ed6\u53c2\u6570\u90fd\u53ea\u662f\u4ece\u8be5\u4efb\u52a1\u7684\u76d1\u7763\u6570\u636e\u96c6\u4e0a\u5b66\u5230\u7684 \u76f8\u540c\u70b9\uff1a \uff08BERT\u53e0\u52a0\u4e86\u66f4\u591a\u7684attention\uff08\u81ea\u6ce8\u610f\u529b\uff09\uff0c\u7814\u7a76\u8868\u660e\uff0c\u5411BiDAF\u589e\u52a0\u81ea\u6ce8\u610f\u529b\u5c42\u4e5f\u80fd\u63d0\u5347\u6548\u679c\u3002\uff09 \u6b64\u5916\uff0cSpanBERT\uff08\u5982\u4e0b\u56fe\uff09\u901a\u8fc7\u6539\u53d8mask\u6570\u636e\u548c\u8bad\u7ec3\u4efb\u52a1\u4e5f\u80fd\u63d0\u5347\u6548\u679c\uff1a \u7528\u8fde\u7eedmask\u53d6\u4ee3\u968f\u673amask \u7528span\u7684\u4e24\u4e2a\u8282\u70b9\u9884\u6d4b\u6240\u6709mask\u7684\u5355\u8bcd\u3002\uff08\u76f8\u5f53\u4e8e\u628aspan\u7684\u4fe1\u606f\u538b\u7f29\u5230\u4e24\u4e2a\u8282\u70b9\u4e2d\uff09 3 \u5f00\u653e\u9886\u57df\u7684\u95ee\u7b54\u4efb\u52a1 \u00b6 3.1 \u5f00\u653e\u57df\u95ee\u7b54\u7684\u7279\u70b9 \u00b6 3.2 Retriever-reader\u67b6\u6784 \u00b6 \u53c2\u8003\u6587\u732e\uff1aChen et al., 2017. Reading Wikipedia to Answer Open-domain Questions 3.3 \u5f00\u653e\u57df\u95ee\u7b54\u6700\u65b0\u8fdb\u5c55 \u00b6 Recent work[1] shows that it is beneficial to generate answers instead of to extract answers. Large language models can do open-domain QA well without an explicit retriever stage[2]. Maybe the reader model is not necessary too[3-4]. It is possible to encode all the phrases (60 billion phrases in Wikipedia) using dense vectors and only do nearest neighbor search without a BERT model at inference time! \u53c2\u8003\u6587\u732e [1] Izacard and Grave 2020. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering [2] Roberts et al., 2020. How Much Knowledge Can You Pack Into the Parameters of a Language Model? [3] Seo et al., 2019. Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index [4] Lee et al., 2020. Learning Dense Representations of Phrases at Scale \u6df1\u5165\u4e86\u89e3\u7684\u8bdd\u89c1 Danqi Chen \u7684\u4ee3\u8868\u4f5c\u3002","title":"Lecture11 \u95ee\u7b54\u7cfb\u7edf"},{"location":"Lecture11/#lecture11","text":"","title":"Lecture11: \u95ee\u7b54\u7cfb\u7edf"},{"location":"Lecture11/#_1","text":"\u95ee\u7b54\u7cfb\u7edf\u7b80\u4ecb \u9605\u8bfb\u7406\u89e3\u4efb\u52a1 \u5f00\u653e\u9886\u57df\u7684\u95ee\u7b54\u4efb\u52a1 \u6559\u6388\uff1a Danqi Chen","title":"\u672c\u8282\u4e3b\u8981\u5185\u5bb9"},{"location":"Lecture11/#1","text":"","title":"1 \u95ee\u7b54\u7b80\u4ecb"},{"location":"Lecture11/#11","text":"","title":"1.1 \u95ee\u7b54\u7684\u5b9a\u4e49"},{"location":"Lecture11/#12","text":"\u6309\u95ee\u7b54\u7cfb\u7edf\u4f7f\u7528\u7684\u4fe1\u606f\u6765\u6e90\u6765\u770b\uff0c\u6709\u6587\u7ae0\u6587\u672c\u3001\u7f51\u9875\u6587\u672c\u3001\u77e5\u8bc6\u5e93\u3001\u8868\u683c\u3001\u56fe\u50cf\u7b49\uff1b \u6309\u95ee\u9898\u7c7b\u578b\u6765\u770b\uff0c\u6709\u4e8b\u5b9e\u6027\u6216\u975e\u4e8b\u5b9e\u6027\u95ee\u9898\u3001\u5f00\u653e\u57df\u6216\u5c01\u95ed\u57df\u95ee\u9898\u3001\u5355\u4e00\u95ee\u9898\u6216\u7ec4\u5408\u95ee\u9898\uff1b \u4ece\u56de\u7b54\u7c7b\u578b\u6765\u770b\uff0c\u6709\u6458\u8981\u3001\u6bb5\u843d\u3001\u5217\u8868\u3001\u5bf9\u9519\u7b49\u3002","title":"1.2 \u95ee\u7b54\u7684\u5206\u7c7b"},{"location":"Lecture11/#13","text":"\u641c\u7d22\u5f15\u64ce \u8bed\u97f3\u52a9\u624b \u8fdb\u5165\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\u540e\uff0c\u51e0\u4e4e\u6240\u6709\u6700\u5148\u8fdb\u7684\u95ee\u7b54\u7cfb\u7edf\u90fd\u5efa\u7acb\u5728\u7aef\u5230\u7aef\u8bad\u7ec3\u548c\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u4e4b\u4e0a\uff08\u4f8b\u5982\uff0cBERT\uff09\u3002","title":"1.3 \u95ee\u7b54\u7684\u5e94\u7528"},{"location":"Lecture11/#14","text":"\u672c\u8282\u91cd\u70b9\u662f\u57fa\u4e8e\u975e\u7ed3\u6784\u5316\u6587\u672c\u7684\u95ee\u7b54\u7cfb\u7edf\u3002 \u975e\u7ed3\u6784\u5316\u6587\u672c\u4e4b\u5916\uff0c\u8fd8\u6709\u57fa\u4e8e\u6570\u636e\u5e93/\u77e5\u8bc6\u5e93\u7684\u95ee\u7b54\u3001\u89c6\u89c9\u95ee\u7b54\u7b49\u9886\u57df\u3002","title":"1.4 \u6587\u672c\u95ee\u7b54\u4e4b\u5916"},{"location":"Lecture11/#2","text":"","title":"2 \u9605\u8bfb\u7406\u89e3\u4efb\u52a1"},{"location":"Lecture11/#21","text":"\u9605\u8bfb\u7406\u89e3\u4efb\u52a1\u662f\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7684\u91cd\u8981\u5e94\u7528\uff0c\u5f88\u591aNLP\u4efb\u52a1\u53ef\u4ee5\u8f6c\u5316\u4e3a\u9605\u8bfb\u7406\u89e3\uff0c\u4f8b\u5982\u4fe1\u606f\u62bd\u53d6\u3001\u8bed\u4e49\u6807\u6ce8\u7b49\u3002","title":"2.1 \u9605\u8bfb\u7406\u89e3\u5b9a\u4e49"},{"location":"Lecture11/#22-qasquad","text":"SQuAD\u662f2016\u5e74\u7531\u65af\u5766\u798f\u5927\u5b66\u7684\u7814\u7a76\u8005\u5efa\u7acb\u7684\u95ee\u7b54\u6570\u636e\u96c6\u3002 \uff08\u6240\u4ee5\u53ea\u7528\u4e86\u90a3\u4e9b\u80fd\u7528\u539f\u6587\u56de\u7b54\u7684\u95ee\u9898\uff09 SQuAD\u6709\u4e24\u4e2a\u8bc4\u4f30\u6307\u6807\uff1a EM\uff08\u7cbe\u786e\u5339\u914d\uff0cExact Match\uff09\uff080 \u6216 1\uff09 F1\uff08\u5177\u4f53\u8ba1\u7b97\u65b9\u5f0f\u53c2\u8003\u539f\u8bba\u6587\uff09","title":"2.2 \u65af\u5766\u798fQA\u6570\u636e\u96c6\uff08SQuAD\uff09"},{"location":"Lecture11/#23","text":"\u89e3\u51b3\u8be5\u4efb\u52a1\u4e3b\u8981\u6709\u4e24\u7c7b\u6a21\u578b\uff1a","title":"2.3 \u9605\u8bfb\u7406\u89e3\u5efa\u6a21"},{"location":"Lecture11/#24-seq2seq","text":"\u673a\u5668\u7ffb\u8bd1\u4e2d\uff0c\u6709source sentence\u548ctarget sentence\uff1b\u9605\u8bfb\u7406\u89e3\u4e5f\u6709\u4e24\u6bb5\u6587\u672c\uff1apassage\u548cquestion\uff08\u5b57\u6570\u4e0d\u5747\u8861\uff09\u3002 \u673a\u5668\u7ffb\u8bd1\u4e2d\uff0c\u9700\u8981\u627e\u5230\u4e0etarget word\u76f8\u5173\u7684source word\uff1b\u9605\u8bfb\u7406\u89e3\u4e2d\uff0c\u9700\u8981\u627e\u5230\u4e0equestion\u6700\u76f8\u5173\u7684passage\u4e2d\u7684\u5355\u8bcd\u3002\uff08\u90fd\u9700\u8981\u6ce8\u610f\u529b\u673a\u5236\u6765\u5b9e\u73b0\uff09 \u673a\u5668\u7ffb\u8bd1\u9700\u8981\u89e3\u7801\u5668\u751f\u6210\u7b54\u6848\uff1b\u4f46\u662f\u9605\u8bfb\u7406\u89e3\u4efb\u52a1\u9700\u8981\u4e24\u4e2a\u5206\u7c7b\u5668\u9884\u6d4b\u7b54\u6848\u7684\u8d77\u59cb\u4f4d\u7f6e\u548c\u7ed3\u675f\u4f4d\u7f6e\u3002","title":"2.4 seq2seq\u56de\u987e\uff1a\u673a\u5668\u7ffb\u8bd1\u4e0e\u9605\u8bfb\u7406\u89e3\u7684\u5f02\u540c"},{"location":"Lecture11/#25-bidaf","text":"BiDAF: the Bidirectional Attention Flow model \u6574\u4f53\u67b6\u6784\u5982\u4e0b\uff1a","title":"2.5 BiDAF\u6a21\u578b"},{"location":"Lecture11/#251-encoding","text":"","title":"2.5.1 \u7f16\u7801\u5c42\uff1aEncoding"},{"location":"Lecture11/#252-attention","text":"\u4e24\u79cd\u6ce8\u610f\u529b\uff1a Context-to-query attention: \u5bf9\u4e8e\u6bcf\u4e2acontext word\uff0c\u4ecequery words\u4e2d\u9009\u62e9\u6700\u76f8\u5173\u7684\u8bcd\u3002 Query-to-context attention: \u9009\u62e9\u4e0e query words\u6700\u76f8\u5173\u7684context words\u3002 \u6ce8\u610f\u529b\u53ef\u89c6\u5316\uff1a","title":"2.5.2 \u6ce8\u610f\u529b\u5c42\uff1aAttention"},{"location":"Lecture11/#253-modeling-and-output-layers","text":"Modeling layer: \u4e24\u5c42\u53cc\u5411LSTM\u7f51\u7edc\u3002 Output layer\uff1a\u4e24\u4e2a\u5206\u7c7b\u5668\uff0c\u5206\u522b\u9884\u6d4b\u8d77\u59cb\u4f4d\u7f6e\u548c\u7ed3\u675f\u4f4d\u7f6e\u3002","title":"2.5.3 \u8f93\u51fa\u5c42\uff1aModeling and output layers"},{"location":"Lecture11/#26-bert","text":"BERT\u505a\u9605\u8bfb\u7406\u89e3\u4efb\u52a1\uff0c\u601d\u8def\u5982\u4e0b\uff1a","title":"2.6 BERT\u6a21\u578b"},{"location":"Lecture11/#27-bidafbert","text":"\u4e0d\u540c\u70b9\uff1a BERT\u53c2\u6570\u66f4\u591a BiDAF\u57fa\u4e8eBiLSTM\uff0cBERT\u57fa\u4e8eTransformer BERT\u7ecf\u8fc7\u9884\u8bad\u7ec3\uff0c\u800cBiDAF\u9664\u4e86\u7528GloVe\u8fdb\u884c\u7f16\u7801\u5916\uff0c\u5176\u4ed6\u53c2\u6570\u90fd\u53ea\u662f\u4ece\u8be5\u4efb\u52a1\u7684\u76d1\u7763\u6570\u636e\u96c6\u4e0a\u5b66\u5230\u7684 \u76f8\u540c\u70b9\uff1a \uff08BERT\u53e0\u52a0\u4e86\u66f4\u591a\u7684attention\uff08\u81ea\u6ce8\u610f\u529b\uff09\uff0c\u7814\u7a76\u8868\u660e\uff0c\u5411BiDAF\u589e\u52a0\u81ea\u6ce8\u610f\u529b\u5c42\u4e5f\u80fd\u63d0\u5347\u6548\u679c\u3002\uff09 \u6b64\u5916\uff0cSpanBERT\uff08\u5982\u4e0b\u56fe\uff09\u901a\u8fc7\u6539\u53d8mask\u6570\u636e\u548c\u8bad\u7ec3\u4efb\u52a1\u4e5f\u80fd\u63d0\u5347\u6548\u679c\uff1a \u7528\u8fde\u7eedmask\u53d6\u4ee3\u968f\u673amask \u7528span\u7684\u4e24\u4e2a\u8282\u70b9\u9884\u6d4b\u6240\u6709mask\u7684\u5355\u8bcd\u3002\uff08\u76f8\u5f53\u4e8e\u628aspan\u7684\u4fe1\u606f\u538b\u7f29\u5230\u4e24\u4e2a\u8282\u70b9\u4e2d\uff09","title":"2.7 BiDAF\u548cBERT\u7684\u5f02\u540c"},{"location":"Lecture11/#3","text":"","title":"3 \u5f00\u653e\u9886\u57df\u7684\u95ee\u7b54\u4efb\u52a1"},{"location":"Lecture11/#31","text":"","title":"3.1 \u5f00\u653e\u57df\u95ee\u7b54\u7684\u7279\u70b9"},{"location":"Lecture11/#32-retriever-reader","text":"\u53c2\u8003\u6587\u732e\uff1aChen et al., 2017. Reading Wikipedia to Answer Open-domain Questions","title":"3.2 Retriever-reader\u67b6\u6784"},{"location":"Lecture11/#33","text":"Recent work[1] shows that it is beneficial to generate answers instead of to extract answers. Large language models can do open-domain QA well without an explicit retriever stage[2]. Maybe the reader model is not necessary too[3-4]. It is possible to encode all the phrases (60 billion phrases in Wikipedia) using dense vectors and only do nearest neighbor search without a BERT model at inference time! \u53c2\u8003\u6587\u732e [1] Izacard and Grave 2020. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering [2] Roberts et al., 2020. How Much Knowledge Can You Pack Into the Parameters of a Language Model? [3] Seo et al., 2019. Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index [4] Lee et al., 2020. Learning Dense Representations of Phrases at Scale \u6df1\u5165\u4e86\u89e3\u7684\u8bdd\u89c1 Danqi Chen \u7684\u4ee3\u8868\u4f5c\u3002","title":"3.3 \u5f00\u653e\u57df\u95ee\u7b54\u6700\u65b0\u8fdb\u5c55"},{"location":"computing-and-computing-power/","text":"\u4eba\u5de5\u667a\u80fd\u7684\u57fa\u77f3\uff1a\u7b97\u529b\u3001\u8ba1\u7b97\u8303\u5f0f\u548c\u8ba1\u7b97\u8bbe\u65bd \u00b6 \u652f\u6491\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u7684\u4e09\u4e2a\u8981\u7d20\u5305\u62ec\uff1a\u6570\u636e\u3001\u7b97\u6cd5\u3001\u7b97\u529b\u3002\u53ef\u4ee5\u8bf4\uff0c\u6570\u636e\u662f\u673a\u5668\u5b66\u4e60\u7684\u8f93\u5165\uff0c\u7b97\u6cd5\u51b3\u5b9a\u673a\u5668\u5b66\u4ec0\u4e48\u3001\u600e\u6837\u5b66\uff0c\u7b97\u529b\u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b\u3002\u8fd9\u91cc\u7684\u7b97\u529b\u5e76\u4e0d\u53ea\u662f\u663e\u5361\uff0c\u4e3a\u4e86\u53c8\u597d\u53c8\u5feb\u53c8\u4fbf\u5b9c\u53c8\u5b89\u5168\u7684\u8ba1\u7b97\uff0c\u4eba\u4eec\u60f3\u5c3d\u529e\u6cd5\u4ece\u8f6f\u4ef6\u786c\u4ef6\u5404\u4e2a\u5c42\u9762\u63d0\u5347\u8ba1\u7b97\u80fd\u529b\u3002\u4ece\u7b97\u529b\u3001\u8ba1\u7b97\u65b9\u5f0f\u548c\u8ba1\u7b97\u8bbe\u65bd\u4e09\u4e2a\u65b9\u9762\u770b\uff0c\u76f8\u5173\u6982\u5ff5\u5982\u56fe\u3002 1 \u5b9a\u4e49 \u00b6 \u7b97\u529b\uff08Computing Power\uff09\uff1a\u7b97\u529b\u662f\u901a\u8fc7\u8ba1\u7b97\u8bbe\u5907\u5bf9\u4fe1\u606f\u6216\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u7136\u540e\u8f93\u51fa\u76ee\u6807\u7ed3\u679c\u7684\u8ba1\u7b97\u80fd\u529b\u3002\u5f53\u6211\u4eec\u8c08\u8bba\u7b97\u529b\u65f6\uff0c\u901a\u5e38\u662f\u6307\u7b97\u529b\u7684\u57fa\u672c\u8f7d\u4f53\u5373\u82af\u7247\uff0c\u5982CPU\u3001GPU\u7b49\u3002 \u8ba1\u7b97\uff08Computing\uff09\uff1a\u8ba1\u7b97\u662f\u9488\u5bf9\u4e00\u4e2a\u95ee\u9898\u3001\u8bbe\u8ba1\u51fa\u89e3\u51b3\u95ee\u9898\u7684\u6307\u4ee4\u5e8f\u5217\uff0c\u5e76\u7531\u8ba1\u7b97\u8bbe\u5907\u6765\u6267\u884c\u7684\u8fc7\u7a0b\u3002\u8ba1\u7b97\u673a\u7684\u51fa\u73b0\u5c31\u662f\u4e3a\u4e86\u89e3\u51b3\u8ba1\u7b97\u95ee\u9898\u3002\u9488\u5bf9\u4e0d\u540c\u7684\u8ba1\u7b97\u95ee\u9898\uff0c\u4eba\u4eec\u53d1\u660e\u4e86\u5404\u79cd\u8ba1\u7b97\u8303\u5f0f\uff0c\u5982\u5e76\u884c\u8ba1\u7b97\u3001\u5206\u5e03\u5f0f\u8ba1\u7b97\u7b49\u3002 \u8ba1\u7b97\u8bbe\u65bd\uff08Computing Infrastructure\uff09\uff1a\u8ba1\u7b97\u8bbe\u65bd\u662f\u8ba1\u7b97\u8bbe\u5907\u7684\u96c6\u5408\uff0c\u5305\u62ec\u786c\u4ef6\uff08\u5982\u670d\u52a1\u5668\u3001\u96c6\u7fa4\u7b49\uff09\u548c\u8f6f\u4ef6\uff08\u5982\u64cd\u4f5c\u7cfb\u7edf\u3001\u865a\u62df\u673a\u7b49\uff09\u3002\u672c\u6587\u4e2d\u4e3b\u8981\u6307\u96c6\u7fa4\u3001\u8d85\u7b97\u548c\u6570\u636e\u4e2d\u5fc3\u3002 \u5982\u679c\u7528\u4e00\u4e2a\u4eba\u7684\u5b66\u4e60\u8fc7\u7a0b\u6765\u6bd4\u55bb\uff0c\u90a3\u4e48\u7b97\u529b\u5bf9\u5e94\u5b66\u4e60\u80fd\u529b\uff0c\u82af\u7247\u5c31\u662f\u4eba\u7684\u5927\u8111\uff1b\u8ba1\u7b97\u8303\u5f0f\u5bf9\u5e94\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e0d\u540c\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u63d0\u5347\u5b66\u4e60\u6548\u679c\uff1b\u8ba1\u7b97\u8bbe\u65bd\u5c31\u662f\u5b66\u6821\uff0c\u63d0\u4f9b\u786c\u4ef6\uff08\u5982\u6559\u5ba4\u3001\u56fe\u4e66\u9986\u7b49\uff09\u548c\u8f6f\u4ef6\uff08\u5982\u8001\u5e08\u3001\u73ed\u7ea7\u7b49\uff09\u3002 2 \u7b97\u529b \u00b6 \u82af\u7247\u662f\u7b97\u529b\u7684\u57fa\u672c\u8f7d\u4f53\uff0c\u7531\u534a\u5bfc\u4f53\uff08\u7845\uff09\u548c\u96c6\u6210\u7535\u8def\u7ec4\u6210\u3002\u5e38\u89c1\u7684\u82af\u7247\u7c7b\u578b\u6709CPU\uff0cGPU\uff0cFPGA\uff0cASIC\uff0c\u5b83\u4eec\u7684\u7279\u70b9\u5982\u4e0b\u56fe\u6240\u793a\u3002 \u82af\u7247\u79cd\u7c7b \u5b9a\u5236\u5316\u7a0b\u5ea6 \u53ef\u7f16\u8f91\u6027 \u8ba1\u7b97\u80fd\u529b \u4ef7\u683c \u5e94\u7528 \u4e3b\u8981\u4f9b\u5e94\u5546\u53ca\u5176\u4ea7\u54c1 CPU \u901a\u7528 \u4e0d\u53ef\u7f16\u8f91 \u4f4e \u4f4e \u901a\u7528 Intel (\u5954\u817e\u3001\u9177\u777f\u7cfb\u5217)\u3001AMD (\u9510\u9f99\u7cfb\u5217) GPU \u901a\u7528 \u4e0d\u53ef\u7f16\u8f91 \u4e2d \u4e2d \u6df1\u5ea6\u5b66\u4e60\u3001\u6e38\u620f\u3001\u56fe\u5f62\u8bbe\u8ba1\u7b49 NVIDIA (Tesla\u3001RTX Titan\u7cfb\u5217)\u3001AMD\uff08\u9510\u9f99RX\u7cfb\u5217\uff09 FPGA \u534a\u5b9a\u5236 \u53ef\u4ee5\u7f16\u8f91 \u9ad8 \u9ad8 \u5404\u79cd\u884c\u4e1a\uff0c\u5982\u7f51\u7edc\u901a\u4fe1\u3001\u5de5\u4e1a\u63a7\u5236\u3001\u6d88\u8d39\u7535\u5b50\u548c\u6570\u636e\u4e2d\u5fc3 AMD (Xilinx)\u3001\u82f1\u7279\u5c14(Altera)\u3001Lattice\u3001Microsemi ASIC \u5168\u5b9a\u5236 \u96be\u4ee5\u7f16\u8f91 \u9ad8 \u4f4e \u5b9a\u5236\u5316\u9700\u6c42\uff0c\u5982\u6bd4\u7279\u5e01\u6316\u77ff Intel\u3001Infineon \u7b49\u7b49[4] 2.1 CPU \u00b6 CPU\u5c31\u662f\u7b14\u8bb0\u672c\u7535\u8111\u7684\u4e2d\u592e\u5904\u7406\u5668\u30021971\u5e74\uff0cIntel\u516c\u53f8\u7684\u5de5\u7a0b\u5e08\u53d1\u660e\u4e86\u4e16\u754c\u4e0a\u7b2c\u4e00\u4e2a\u5546\u7528\u5fae\u5904\u7406\u5668\uff0c\u7531\u6b64\u5f00\u542f\u4e86\u8ba1\u7b97\u673a\u548c\u4e92\u8054\u7f51\u9769\u547d\uff0cIntel\u4e5f\u4f34\u968f\u7740\u4e92\u8054\u7f51\u7684\u7b2c\u4e00\u4e2a\u9ec4\u91d1\u65f6\u4ee3\u8d77\u98de\u3002\u7531\u4e8e\u82af\u7247\u7684\u6750\u6599\u4e3b\u8981\u662f\u7845\u8fd9\u79cd\u534a\u5bfc\u4f53\uff0c\u7845\u8c37\u4e5f\u56e0\u6b64\u5f97\u540d\u3002Intel\u521b\u59cb\u4eba\u6469\u5c14\u63d0\u51fa\u8457\u540d\u7684\u6469\u5c14\u5b9a\u5f8b\uff1a\u5f53\u4ef7\u683c\u4e0d\u53d8\u65f6\uff0c\u96c6\u6210\u7535\u8def\u4e0a\u53ef\u5bb9\u7eb3\u7684\u5143\u5668\u4ef6\u7684\u6570\u76ee\uff0c\u7ea6\u6bcf\u969418-24\u4e2a\u6708\u4fbf\u4f1a\u589e\u52a0\u4e00\u500d\uff0c\u6027\u80fd\u4e5f\u5c06\u63d0\u5347\u4e00\u500d\u3002\u7136\u800c\uff0c\u968f\u7740\u82af\u7247\u7ec4\u4ef6\u7684\u89c4\u6a21\u8d8a\u6765\u8d8a\u63a5\u8fd1\u5355\u4e2a\u539f\u5b50\u7684\u89c4\u6a21\uff0c\u6469\u5c14\u5b9a\u5f8b\u5df2\u9010\u6e10\u5931\u6548\uff0c\u534a\u5bfc\u4f53\u884c\u4e1a\u53d1\u5c55\u9010\u6e10\u653e\u7f13\uff0c\u8be5\u884c\u4e1a\u6b63\u91c7\u7528\u5176\u4ed6\u8ba1\u7b97\u65b9\u5f0f\uff08\u5982GPU\uff09\u3001\u5148\u8fdb\u8f6f\u4ef6\u67b6\u6784\u548c\u5de5\u5177\uff0c\u4ee5\u53ca\u65b0\u7684\u82af\u7247\u7535\u8def\u5c01\u88c5\u65b9\u6cd5\u6765\u63d0\u5347\u6574\u4f53\u8ba1\u7b97\u6027\u80fd\u3002 2.2 GPU \u00b6 GPU\u5373\u56fe\u5f62\u5904\u7406\u5668\uff0c\u53c8\u79f0\u663e\u5361\uff0c\u8d1f\u8d23\u8f93\u51fa\u663e\u793a\u56fe\u5f62\u3002\u7531\u4e8eGPU\u6709\u5927\u91cf\u7684\u6838\u5fc3\u548c\u9ad8\u901f\u5185\u5b58\uff0c\u64c5\u957f\u5e76\u884c\u8ba1\u7b97\uff0c\u56e0\u6b64\u88ab\u7528\u6765\u5904\u7406\u9700\u8981\u5927\u91cf\u77e9\u9635\u8fd0\u7b97\u7684\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u3002\u4e3b\u6d41\u663e\u5361\u7684\u663e\u793a\u82af\u7247\u4e3b\u8981\u7531NVIDIA\uff08\u82f1\u4f1f\u8fbe\uff09\u548cAMD\u4e24\u5927\u5382\u5546\u5236\u9020\uff0c\u901a\u5e38NVIDIA\u7684\u663e\u5361\u79f0\u4e3aN\u5361\uff0cAMD\u7684\u663e\u5361\u79f0\u4e3aA\u5361\u3002\u4e3b\u6d41\u6df1\u5ea6\u5b66\u4e60\u6240\u7528\u7684\u663e\u5361\u4e3b\u8981\u662fN\u5361\uff0c\u56e0\u4e3a\u6df1\u5ea6\u5b66\u4e60\u8981\u7528\u5230CUDA\u52a0\u901f\u3002CUDA\u662fNVIDIA\u63a8\u51fa\u7684GPU\u5e76\u884c\u8ba1\u7b97\u6846\u67b6\uff0c\u53ea\u80fd\u5728\u81ea\u5bb6GPU\u4e0a\u8fd0\u884c\uff0c\u5e76\u4e14\u51e0\u4e4e\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u90fd\u9996\u9009CUDA\u4f5c\u4e3a\u5e95\u5c42\u52a0\u901f\u5e93\u3002\u6df1\u53d7\u5e7f\u5927AI\u70bc\u4e39\u5e08\u559c\u7231\u7684\u663e\u5361\u6709GTX1080Ti\u3001GTX3080Ti\u3001Tesla V100\u3001Tesla A100\u7b49\u3002 2.3 FPGA \u00b6 FPGA\u662f\u53ef\u7f16\u7a0b\u96c6\u6210\u7535\u8def\uff0cFPGA\u5185\u90e8\u7684\u7535\u8def\u65e8\u5728\u5b9e\u73b0\u5404\u79cd\u4e0d\u540c\u7684\u529f\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u6839\u636e\u9700\u8981\u91cd\u65b0\u7f16\u7a0b\u4ee5\u6267\u884c\u8fd9\u4e9b\u529f\u80fd\u3002FPGA\u53ef\u4ee5\u5728\u5236\u9020\u540e\u7531\u7528\u6237\u7f16\u7a0b\u7528\u4e8e\u7279\u5b9a\u7528\u9014\u3002FPGA\u5305\u542b\u901a\u8fc7\u53ef\u7f16\u7a0b\u4e92\u8fde\u8fde\u63a5\u7684\u81ea\u9002\u5e94\u903b\u8f91\u6a21\u5757 (ALM) \u548c\u903b\u8f91\u5143\u4ef6(LE)\u3002\u8fd9\u4e9b\u5757\u521b\u5efa\u4e86\u903b\u8f91\u95e8\u7684\u7269\u7406\u9635\u5217\uff0c\u53ef\u4ee5\u5b9a\u5236\u4ee5\u6267\u884c\u7279\u5b9a\u7684\u8ba1\u7b97\u4efb\u52a1\u3002\u76ee\u524d\u5168\u7403FPGA\u5e02\u573a\u7531\u56db\u5927\u5de8\u5934AMD\uff08Xilinx\uff09\u3001\u82f1\u7279\u5c14\u3001Lattice\u3001Microsemi\u5784\u65ad\uff0c\u56db\u5927\u5382\u5546\u7684\u5e02\u573a\u5360\u6709\u7387\u8fbe\u5230\u4e8696%\u3002FPGA\u4f5c\u4e3a\u4ea7\u4e1a\u65b0\u52a8\u80fd\u7684\u5f3a\u5927\u5f15\u64ce\u5bf9\u201c\u65b0\u57fa\u5efa\u201d\u7684\u6838\u5fc3\u9886\u57df5G\u3001\u7269\u8054\u7f51\u3001\u6570\u636e\u4e2d\u5fc3\u7b49\u90fd\u8d77\u5230\u975e\u5e38\u91cd\u8981\u7684\u4f5c\u7528\u3002 2.4 ASIC \u00b6 ASIC\u5c5e\u4e8e\u4e13\u7528\u96c6\u6210\u7535\u8def\uff0c\u662f\u4e3a\u7279\u5b9a\u529f\u80fd\u800c\u4e13\u95e8\u6784\u5efa\u548c\u6279\u91cf\u751f\u4ea7\u7684\u3002\u4e0e FPGA \u4e0d\u540c\uff0c\u5b83\u4eec\u4e0d\u80fd\u91cd\u65b0\u7f16\u7a0b\uff0c\u4f46ASIC\u5177\u6709\u4f53\u79ef\u66f4\u5c0f\u3001\u91cd\u91cf\u66f4\u8f7b\u3001\u529f\u8017\u66f4\u4f4e\u3001\u53ef\u9760\u6027\u63d0\u9ad8\u3001\u6027\u80fd\u63d0\u9ad8\u3001\u4fdd\u5bc6\u6027\u589e\u5f3a\u3001\u6210\u672c\u964d\u4f4e\u7b49\u4f18\u70b9\u3002\u4ee5\u524d\uff0c\u4eba\u4eec\u90fd\u662f\u7528PC\uff08x86\u901a\u7528\u82af\u7247\uff09\u6316\u77ff\uff0c\u540e\u6765\u8d8a\u6316\u96be\u5ea6\u8d8a\u5927\uff0c\u7b97\u529b\u4e0d\u591f\u3002\u4e8e\u662f\uff0c\u5f00\u59cb\u4f7f\u7528\u663e\u5361\uff08GPU\uff09\u53bb\u6316\u77ff\u3002\u518d\u540e\u6765\uff0c\u663e\u5361\u7684\u80fd\u8017\u592a\u9ad8\uff0c\u6316\u51fa\u6765\u7684\u5e01\u503c\u8fd8\u62b5\u4e0d\u4e0a\u7535\u8d39\uff0c\u5c31\u5f00\u59cb\u91c7\u7528FPGA\u548cASIC\u96c6\u7fa4\u9635\u5217\u6316\u77ff[1]\u3002 3 \u8ba1\u7b97\u8303\u5f0f \u00b6 3.1 \u5e76\u884c\u8ba1\u7b97 \u00b6 \u5e76\u884c\u8ba1\u7b97\u88ab\u5b9a\u4e49\u4e3a\u4e00\u79cd\u540c\u65f6\u4f7f\u7528\u591a\u4e2a\u8ba1\u7b97\u673a\u7cfb\u7edf\u7684\u8ba1\u7b97\u7c7b\u578b\u3002\u5728\u8fd9\u91cc\uff0c\u4e00\u4e2a\u95ee\u9898\u88ab\u5206\u89e3\u6210\u5b50\u95ee\u9898\uff0c\u7136\u540e\u8fdb\u4e00\u6b65\u5206\u89e3\u6210\u6307\u4ee4\u3002\u6bcf\u4e2a\u5b50\u95ee\u9898\u7684\u8fd9\u4e9b\u6307\u4ee4\u5728\u4e0d\u540c\u7684\u5904\u7406\u5668\u4e0a\u540c\u65f6\u6267\u884c\u3002\u5728\u4e0b\u56fe\u4e2d\uff0c\u4f60\u53ef\u4ee5\u770b\u5230\u5e76\u884c\u8ba1\u7b97\u7cfb\u7edf\u662f\u5982\u4f55\u7531\u591a\u4e2a\u5904\u7406\u5668\u7ec4\u6210\u7684\uff0c\u8fd9\u4e9b\u5904\u7406\u5668\u76f8\u4e92\u901a\u4fe1\u5e76\u540c\u65f6\u5728\u5171\u4eab\u5185\u5b58\u4e0a\u6267\u884c\u591a\u4e2a\u4efb\u52a1\u3002\u5e76\u884c\u8ba1\u7b97\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u8282\u7701\u65f6\u95f4\u548c\u63d0\u4f9b\u5e76\u53d1\u6027\u3002 3.2 \u5206\u5e03\u5f0f\u8ba1\u7b97 \u00b6 \u5206\u5e03\u5f0f\u8ba1\u7b97\u88ab\u5b9a\u4e49\u4e3a\u4e00\u79cd\u8ba1\u7b97\u7c7b\u578b\uff0c\u5176\u4e2d\u591a\u4e2a\u8ba1\u7b97\u673a\u7cfb\u7edf\u5bf9\u4e00\u4e2a\u95ee\u9898\u8fdb\u884c\u5de5\u4f5c\u3002\u5728\u8fd9\u91cc\uff0c\u6240\u6709\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\u88ab\u8fde\u63a5\u5728\u4e00\u8d77\uff0c\u95ee\u9898\u88ab\u5206\u4e3a\u5b50\u95ee\u9898\uff0c\u6bcf\u4e2a\u90e8\u5206\u7531\u4e0d\u540c\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\u6765\u89e3\u51b3\u3002\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u76ee\u6807\u662f\u63d0\u9ad8\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u786e\u4fdd\u5bb9\u9519\u3002\u5728\u4e0b\u56fe\u4e2d\uff0c\u6bcf\u4e2a\u5904\u7406\u5668\u90fd\u6709\u81ea\u5df1\u7684\u672c\u5730\u5b58\u50a8\u5668\uff0c\u6240\u6709\u5904\u7406\u5668\u901a\u8fc7\u7f51\u7edc\u76f8\u4e92\u901a\u4fe1\u3002 3.2.1 \u7f51\u683c\u8ba1\u7b97 \u00b6 \u7f51\u683c\u8ba1\u7b97\u662f\u7531\u8ba1\u7b97\u673a\u7ec4\u6210\u7684\u7f51\u7edc\uff0c\u5171\u540c\u6267\u884c\u5355\u53f0\u673a\u5668\u53ef\u80fd\u96be\u4ee5\u5904\u7406\u7684\u4efb\u52a1\u3002\u8be5\u7f51\u7edc\u4e0a\u7684\u6240\u6709\u8ba1\u7b97\u673a\u90fd\u5728\u540c\u4e00\u4e2a\u4fdd\u62a4\u4f1e\u4e0b\u5de5\u4f5c\uff0c\u88ab\u79f0\u4e3a\u865a\u62df\u8d85\u7ea7\u8ba1\u7b97\u673a\u3002\u4ed6\u4eec\u5de5\u4f5c\u7684\u4efb\u52a1\u8981\u4e48\u662f\u9ad8\u8ba1\u7b97\u80fd\u529b\uff0c\u8981\u4e48\u662f\u7531\u5927\u578b\u6570\u636e\u96c6\u7ec4\u6210\u3002\u7f51\u683c\u8ba1\u7b97\u4e2d\u8ba1\u7b97\u673a\u7cfb\u7edf\u4e4b\u95f4\u7684\u6240\u6709\u901a\u4fe1\u90fd\u662f\u5728 \"\u6570\u636e\u7f51\u683c \"\u4e0a\u5b8c\u6210\u7684\u3002\u7f51\u683c\u8ba1\u7b97\u7684\u76ee\u6807\u662f\u5728\u66f4\u77ed\u7684\u65f6\u95f4\u5185\u89e3\u51b3\u66f4\u591a\u7684\u9ad8\u8ba1\u7b97\u95ee\u9898\uff0c\u63d0\u9ad8\u751f\u4ea7\u529b\u3002 3.2.2 \u4e91\u8ba1\u7b97 \u00b6 \u4e91\u88ab\u5b9a\u4e49\u4e3a\u4f7f\u7528\u522b\u4eba\u7684\u670d\u52a1\u5668\u6765\u6258\u7ba1\u3001\u5904\u7406\u6216\u5b58\u50a8\u6570\u636e\u3002\u4e91\u8ba1\u7b97\u88ab\u5b9a\u4e49\u4e3a\u4e00\u79cd\u8ba1\u7b97\u7c7b\u578b\uff0c\u5b83\u662f\u5728\u4e92\u8054\u7f51\u4e0a\u4ee5\u968f\u7528\u968f\u4ed8\u7684\u65b9\u5f0f\u63d0\u4f9b\u6309\u9700\u8ba1\u7b97\u670d\u52a1\u3002 3.2.3 \u8fb9\u7f18\u8ba1\u7b97 \u00b6 \u8fb9\u7f18\u8ba1\u7b97\u88ab\u5b9a\u4e49\u4e3a\u4e13\u6ce8\u4e8e\u51cf\u5c11\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u5668\u4e4b\u95f4\u957f\u8ddd\u79bb\u901a\u4fe1\u7684\u8ba1\u7b97\u7c7b\u578b\u3002\u8fd9\u662f\u901a\u8fc7\u5728\u4e91\u4e2d\u8fd0\u884c\u8f83\u5c11\u7684\u8fdb\u7a0b\uff0c\u5e76\u5c06\u8fd9\u4e9b\u8fdb\u7a0b\u8f6c\u79fb\u5230\u7528\u6237\u7684\u8ba1\u7b97\u673a\u3001\u7269\u8054\u7f51\u8bbe\u5907\u6216\u8fb9\u7f18\u8bbe\u5907/\u670d\u52a1\u5668\u4e0a\u3002 3.3 \u96c6\u7fa4\u8ba1\u7b97 \u00b6 \u96c6\u7fa4\u662f\u4e00\u7ec4\u72ec\u7acb\u7684\u8ba1\u7b97\u673a\uff0c\u5b83\u4eec\u4e00\u8d77\u5de5\u4f5c\u4ee5\u6267\u884c\u7ed9\u5b9a\u7684\u4efb\u52a1\u3002\u96c6\u7fa4\u8ba1\u7b97\u88ab\u5b9a\u4e49\u4e3a\u4e00\u79cd\u8ba1\u7b97\u7c7b\u578b\uff0c\u7531\u4e24\u53f0\u6216\u66f4\u591a\u7684\u72ec\u7acb\u8ba1\u7b97\u673a\u7ec4\u6210\uff0c\u88ab\u79f0\u4e3a\u8282\u70b9\uff0c\u4f5c\u4e3a\u4e00\u53f0\u673a\u5668\u4e00\u8d77\u5de5\u4f5c\u6267\u884c\u4efb\u52a1\u3002\u96c6\u7fa4\u8ba1\u7b97\u7684\u76ee\u6807\u662f\u63d0\u9ad8\u7cfb\u7edf\u7684\u6027\u80fd\u3001\u53ef\u6269\u5c55\u6027\u548c\u7b80\u5355\u6027\u3002\u6b63\u5982\u4e0b\u56fe\u4e2d\u770b\u5230\u7684\uff0c\u6240\u6709\u7684\u8282\u70b9\uff0c\uff08\u65e0\u8bba\u5b83\u4eec\u662f\u7236\u8282\u70b9\u8fd8\u662f\u5b50\u8282\u70b9\uff09\uff0c\u90fd\u4f5c\u4e3a\u4e00\u4e2a\u5355\u4e00\u7684\u5b9e\u4f53\u6765\u6267\u884c\u4efb\u52a1\u3002 3.4 \u9ad8\u6027\u80fd\u8ba1\u7b97 \u00b6 \u9ad8\u6027\u80fd\u8ba1\u7b97\u7814\u7a76\u96c6\u7fa4\u67b6\u6784\u3001\u5e76\u884c\u7b97\u6cd5\u548c\u76f8\u5173\u8f6f\u4ef6\u57fa\u7840\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u8ba1\u7b97\u5b9e\u73b0\u5355\u53f0\u8ba1\u7b97\u673a\u65e0\u6cd5\u8fbe\u5230\u7684\u8fd0\u7b97\u901f\u5ea6\u3002\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e3b\u8981\u5e94\u7528\u9886\u57df\u6709\uff1a \u79d1\u5b66\u8ba1\u7b97\u7c7b\uff1a\u7269\u7406\u5316\u5b66\u3001\u6c14\u8c61\u73af\u4fdd\u3001\u751f\u547d\u79d1\u5b66\u3001\u77f3\u6cb9\u52d8\u63a2\u3001\u5929\u6587\u63a2\u6d4b\u7b49\u3002 \u5de5\u7a0b\u8ba1\u7b97\u7c7b\uff1a\u8ba1\u7b97\u673a\u8f85\u52a9\u5de5\u7a0b\u3001\u8ba1\u7b97\u673a\u8f85\u52a9\u5236\u9020\u3001\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\u3001\u7535\u78c1\u4eff\u771f\u7b49\u3002 \u667a\u80fd\u8ba1\u7b97\u7c7b\uff1a\u5373\u4eba\u5de5\u667a\u80fd\uff08AI\uff0cArtificial Intelligence\uff09\u8ba1\u7b97\uff0c\u5305\u62ec\uff1a\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u6570\u636e\u5206\u6790\u7b49\u3002 4 \u8ba1\u7b97\u8bbe\u65bd \u00b6 \u8ba1\u7b97\u8bbe\u65bd\uff08\u6216\u57fa\u7840\u8ba1\u7b97\u8bbe\u65bd\uff09\uff0c\u5c31\u5982\u5b66\u6821\u662f\u6559\u80b2\u8bbe\u65bd\u3001\u533b\u9662\u662f\u533b\u7597\u8bbe\u65bd\u3001\u94c1\u8def\u662f\u4ea4\u901a\u8bbe\u65bd\u4e00\u6837\uff0c\u5f3a\u8c03\u7684\u662f\u5177\u4f53\u7684\u7269\u7406\u5f62\u6001\u3002\u56e0\u6b64\u5c06\u8ba1\u7b97\u8bbe\u65bd\u5206\u4e3a\u96c6\u7fa4\u3001\u8d85\u7b97\u3001\u6570\u636e\u4e2d\u5fc3\u3002 4.1 \u96c6\u7fa4 \u00b6 \u96c6\u7fa4\u5c31\u662f\u6307\u4e00\u7ec4\uff08\u82e5\u5e72\u4e2a\uff09\u76f8\u4e92\u72ec\u7acb\u540c\u65f6\u53ef\u4ee5\u901a\u8fc7\u7f51\u7edc\u5f7c\u6b64\u4e92\u8054\u7684\u8ba1\u7b97\u673a\uff0c\u6bcf\u4e2a\u96c6\u7fa4\u8282\u70b9\uff08\u5373\u96c6\u7fa4\u4e2d\u7684\u6bcf\u53f0\u8ba1\u7b97\u673a\uff09\u90fd\u662f\u8fd0\u884c\u5404\u81ea\u670d\u52a1\u7684\u72ec\u7acb\u670d\u52a1\u5668\uff0c\u8fd9\u4e9b\u670d\u52a1\u5668\u5408\u4f5c\u505a\u540c\u4e00\u4ef6\u4e8b\u3002\u5f53\u7528\u6237\u8bf7\u6c42\u96c6\u7fa4\u7cfb\u7edf\u65f6\uff0c\u96c6\u7fa4\u7ed9\u7528\u6237\u7684\u611f\u89c9\u5c31\u662f\u4e00\u4e2a\u5355\u4e00\u72ec\u7acb\u7684\u670d\u52a1\u5668\uff0c\u800c\u5b9e\u9645\u4e0a\u7528\u6237\u8bf7\u6c42\u7684\u662f\u4e00\u7ec4\u96c6\u7fa4\u670d\u52a1\u5668\u3002 \u5206\u5e03\u5f0f\u4e0e\u96c6\u7fa4\u7684\u533a\u522b\u662f\u4ec0\u4e48\uff1f\u5f15\u7528\u4e00\u4e2a\u4f8b\u5b50\u8bf4\u660e[7]\uff1a\u5c0f\u996d\u5e97\u539f\u6765\u53ea\u6709\u4e00\u4e2a\u53a8\u5e08\uff0c\u5207\u83dc\u6d17\u83dc\u5907\u6599\u7092\u83dc\u5168\u5e72\u3002\u540e\u6765\u5ba2\u4eba\u591a\u4e86\uff0c\u53a8\u623f\u4e00\u4e2a\u53a8\u5e08\u5fd9\u4e0d\u8fc7\u6765\uff0c\u53c8\u8bf7\u4e86\u4e2a\u53a8\u5e08\uff0c\u4e24\u4e2a\u53a8\u5e08\u90fd\u80fd\u7092\u4e00\u6837\u7684\u83dc\uff0c\u8fd9\u4e24\u4e2a\u53a8\u5e08\u7684\u5173\u7cfb\u662f**\u96c6\u7fa4**\u3002\u4e3a\u4e86\u8ba9\u53a8\u5e08\u4e13\u5fc3\u7092\u83dc\uff0c\u628a\u83dc\u505a\u5230\u6781\u81f4\uff0c\u53c8\u8bf7\u4e86\u4e2a\u914d\u83dc\u5e08\u8d1f\u8d23\u5207\u83dc\uff0c\u5907\u83dc\uff0c\u5907\u6599\uff0c\u53a8\u5e08\u548c\u914d\u83dc\u5e08\u7684\u5173\u7cfb\u662f**\u5206\u5e03\u5f0f**\u3002 4.2 \u8d85\u7b97 \u00b6 \u8d85\u7b97\u4e3b\u8981\u63d0\u4f9b\u56fd\u5bb6\u9ad8\u79d1\u6280\u9886\u57df\u548c\u5c16\u7aef\u6280\u672f\u7814\u7a76\u9700\u7684\u8fd0\u7b97\u901f\u5ea6\u548c\u5b58\u50a8\u5bb9\u91cf\uff0c\u5305\u62ec\u822a\u5929\u3001\u56fd\u9632\u3001\u77f3\u6cb9\u52d8\u63a2\u3001\u6c14\u5019\u5efa\u6a21\u548c\u57fa\u56e0\u7ec4\u6d4b\u5e8f\u7b49\u3002 4.3 \u6570\u636e\u4e2d\u5fc3 \u00b6 \u6570\u636e\u4e2d\u5fc3\u4e3b\u8981\u9762\u5411\u9700\u8981\u4fe1\u606f\u6280\u672f\u670d\u52a1\u7684\u5546\u4e1a\u573a\u666f\uff08\u4e91\uff09\u3002\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\uff1a \u53c2\u8003\u8d44\u6599 \u00b6 [1] \u300a\u5173\u4e8e\u201c\u7b97\u529b\u201d\uff0c\u8fd9\u7bc7\u6587\u7ae0\u503c\u5f97\u4e00\u770b\u300b https://36kr.com/p/1848541284191362 [2] \u90a2\u6587\u5a1f, \u96f7\u6ce2, \u8d75\u5029\u9896. \u7b97\u529b\u57fa\u7840\u8bbe\u65bd\u53d1\u5c55\u73b0\u72b6\u4e0e\u8d8b\u52bf\u5c55\u671b[J]. \u7535\u4fe1\u79d1\u5b66, 2022, 38(6): 51-61. http://www.infocomm-journal.com/dxkx/CN/10.11959/j.issn.1000-0801.2022137 [3] \u300a\u79d1\u666e\uff1a\u5e76\u884c\u8ba1\u7b97\u3001\u5206\u5e03\u5f0f\u8ba1\u7b97\u3001\u96c6\u7fa4\u8ba1\u7b97\u548c\u4e91\u8ba1\u7b97\u300b https://max2d.com/archives/976 [4] \u300a\u4ec0\u4e48\u662fASIC\uff1fASIC\u4f9b\u5e94\u5546\u6709\u54ea\u4e9b\uff1f\u300b https://mp.ofweek.com/ee/a356714038087 [5] Different Computing Paradigms https://www.geeksforgeeks.org/different-computing-paradigms/ [6] \u300a2022\u4e2d\u56fd\u4eba\u5de5\u667a\u80fd\u82af\u7247\u884c\u4e1a\u7814\u7a76\u62a5\u544a\u300b https://pdf.dfcfw.com/pdf/H3_AP202204131558974968_1.pdf?1649846951000.pdf [7] \u5206\u5e03\u5f0f\u4e0e\u96c6\u7fa4\u7684\u533a\u522b\u662f\u4ec0\u4e48\uff1f - \u5f20\u9e4f\u98de\u7684\u56de\u7b54 - \u77e5\u4e4e https://www.zhihu.com/question/20004877/answer/112124929 [8] \u6570\u636e\u4e2d\u5fc3\u5b9a\u4e49\u3001\u5206\u7c7b\u53ca\u4ea7\u4e1a\u94fe https://blog.51cto.com/kymdidicom/3140009","title":"\u4eba\u5de5\u667a\u80fd\u7684\u57fa\u77f3\uff1a\u7b97\u529b\u3001\u8ba1\u7b97\u548c\u8ba1\u7b97\u8bbe\u65bd"},{"location":"computing-and-computing-power/#_1","text":"\u652f\u6491\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u7684\u4e09\u4e2a\u8981\u7d20\u5305\u62ec\uff1a\u6570\u636e\u3001\u7b97\u6cd5\u3001\u7b97\u529b\u3002\u53ef\u4ee5\u8bf4\uff0c\u6570\u636e\u662f\u673a\u5668\u5b66\u4e60\u7684\u8f93\u5165\uff0c\u7b97\u6cd5\u51b3\u5b9a\u673a\u5668\u5b66\u4ec0\u4e48\u3001\u600e\u6837\u5b66\uff0c\u7b97\u529b\u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b\u3002\u8fd9\u91cc\u7684\u7b97\u529b\u5e76\u4e0d\u53ea\u662f\u663e\u5361\uff0c\u4e3a\u4e86\u53c8\u597d\u53c8\u5feb\u53c8\u4fbf\u5b9c\u53c8\u5b89\u5168\u7684\u8ba1\u7b97\uff0c\u4eba\u4eec\u60f3\u5c3d\u529e\u6cd5\u4ece\u8f6f\u4ef6\u786c\u4ef6\u5404\u4e2a\u5c42\u9762\u63d0\u5347\u8ba1\u7b97\u80fd\u529b\u3002\u4ece\u7b97\u529b\u3001\u8ba1\u7b97\u65b9\u5f0f\u548c\u8ba1\u7b97\u8bbe\u65bd\u4e09\u4e2a\u65b9\u9762\u770b\uff0c\u76f8\u5173\u6982\u5ff5\u5982\u56fe\u3002","title":"\u4eba\u5de5\u667a\u80fd\u7684\u57fa\u77f3\uff1a\u7b97\u529b\u3001\u8ba1\u7b97\u8303\u5f0f\u548c\u8ba1\u7b97\u8bbe\u65bd"},{"location":"computing-and-computing-power/#1","text":"\u7b97\u529b\uff08Computing Power\uff09\uff1a\u7b97\u529b\u662f\u901a\u8fc7\u8ba1\u7b97\u8bbe\u5907\u5bf9\u4fe1\u606f\u6216\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u7136\u540e\u8f93\u51fa\u76ee\u6807\u7ed3\u679c\u7684\u8ba1\u7b97\u80fd\u529b\u3002\u5f53\u6211\u4eec\u8c08\u8bba\u7b97\u529b\u65f6\uff0c\u901a\u5e38\u662f\u6307\u7b97\u529b\u7684\u57fa\u672c\u8f7d\u4f53\u5373\u82af\u7247\uff0c\u5982CPU\u3001GPU\u7b49\u3002 \u8ba1\u7b97\uff08Computing\uff09\uff1a\u8ba1\u7b97\u662f\u9488\u5bf9\u4e00\u4e2a\u95ee\u9898\u3001\u8bbe\u8ba1\u51fa\u89e3\u51b3\u95ee\u9898\u7684\u6307\u4ee4\u5e8f\u5217\uff0c\u5e76\u7531\u8ba1\u7b97\u8bbe\u5907\u6765\u6267\u884c\u7684\u8fc7\u7a0b\u3002\u8ba1\u7b97\u673a\u7684\u51fa\u73b0\u5c31\u662f\u4e3a\u4e86\u89e3\u51b3\u8ba1\u7b97\u95ee\u9898\u3002\u9488\u5bf9\u4e0d\u540c\u7684\u8ba1\u7b97\u95ee\u9898\uff0c\u4eba\u4eec\u53d1\u660e\u4e86\u5404\u79cd\u8ba1\u7b97\u8303\u5f0f\uff0c\u5982\u5e76\u884c\u8ba1\u7b97\u3001\u5206\u5e03\u5f0f\u8ba1\u7b97\u7b49\u3002 \u8ba1\u7b97\u8bbe\u65bd\uff08Computing Infrastructure\uff09\uff1a\u8ba1\u7b97\u8bbe\u65bd\u662f\u8ba1\u7b97\u8bbe\u5907\u7684\u96c6\u5408\uff0c\u5305\u62ec\u786c\u4ef6\uff08\u5982\u670d\u52a1\u5668\u3001\u96c6\u7fa4\u7b49\uff09\u548c\u8f6f\u4ef6\uff08\u5982\u64cd\u4f5c\u7cfb\u7edf\u3001\u865a\u62df\u673a\u7b49\uff09\u3002\u672c\u6587\u4e2d\u4e3b\u8981\u6307\u96c6\u7fa4\u3001\u8d85\u7b97\u548c\u6570\u636e\u4e2d\u5fc3\u3002 \u5982\u679c\u7528\u4e00\u4e2a\u4eba\u7684\u5b66\u4e60\u8fc7\u7a0b\u6765\u6bd4\u55bb\uff0c\u90a3\u4e48\u7b97\u529b\u5bf9\u5e94\u5b66\u4e60\u80fd\u529b\uff0c\u82af\u7247\u5c31\u662f\u4eba\u7684\u5927\u8111\uff1b\u8ba1\u7b97\u8303\u5f0f\u5bf9\u5e94\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e0d\u540c\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u63d0\u5347\u5b66\u4e60\u6548\u679c\uff1b\u8ba1\u7b97\u8bbe\u65bd\u5c31\u662f\u5b66\u6821\uff0c\u63d0\u4f9b\u786c\u4ef6\uff08\u5982\u6559\u5ba4\u3001\u56fe\u4e66\u9986\u7b49\uff09\u548c\u8f6f\u4ef6\uff08\u5982\u8001\u5e08\u3001\u73ed\u7ea7\u7b49\uff09\u3002","title":"1 \u5b9a\u4e49"},{"location":"computing-and-computing-power/#2","text":"\u82af\u7247\u662f\u7b97\u529b\u7684\u57fa\u672c\u8f7d\u4f53\uff0c\u7531\u534a\u5bfc\u4f53\uff08\u7845\uff09\u548c\u96c6\u6210\u7535\u8def\u7ec4\u6210\u3002\u5e38\u89c1\u7684\u82af\u7247\u7c7b\u578b\u6709CPU\uff0cGPU\uff0cFPGA\uff0cASIC\uff0c\u5b83\u4eec\u7684\u7279\u70b9\u5982\u4e0b\u56fe\u6240\u793a\u3002 \u82af\u7247\u79cd\u7c7b \u5b9a\u5236\u5316\u7a0b\u5ea6 \u53ef\u7f16\u8f91\u6027 \u8ba1\u7b97\u80fd\u529b \u4ef7\u683c \u5e94\u7528 \u4e3b\u8981\u4f9b\u5e94\u5546\u53ca\u5176\u4ea7\u54c1 CPU \u901a\u7528 \u4e0d\u53ef\u7f16\u8f91 \u4f4e \u4f4e \u901a\u7528 Intel (\u5954\u817e\u3001\u9177\u777f\u7cfb\u5217)\u3001AMD (\u9510\u9f99\u7cfb\u5217) GPU \u901a\u7528 \u4e0d\u53ef\u7f16\u8f91 \u4e2d \u4e2d \u6df1\u5ea6\u5b66\u4e60\u3001\u6e38\u620f\u3001\u56fe\u5f62\u8bbe\u8ba1\u7b49 NVIDIA (Tesla\u3001RTX Titan\u7cfb\u5217)\u3001AMD\uff08\u9510\u9f99RX\u7cfb\u5217\uff09 FPGA \u534a\u5b9a\u5236 \u53ef\u4ee5\u7f16\u8f91 \u9ad8 \u9ad8 \u5404\u79cd\u884c\u4e1a\uff0c\u5982\u7f51\u7edc\u901a\u4fe1\u3001\u5de5\u4e1a\u63a7\u5236\u3001\u6d88\u8d39\u7535\u5b50\u548c\u6570\u636e\u4e2d\u5fc3 AMD (Xilinx)\u3001\u82f1\u7279\u5c14(Altera)\u3001Lattice\u3001Microsemi ASIC \u5168\u5b9a\u5236 \u96be\u4ee5\u7f16\u8f91 \u9ad8 \u4f4e \u5b9a\u5236\u5316\u9700\u6c42\uff0c\u5982\u6bd4\u7279\u5e01\u6316\u77ff Intel\u3001Infineon \u7b49\u7b49[4]","title":"2 \u7b97\u529b"},{"location":"computing-and-computing-power/#21-cpu","text":"CPU\u5c31\u662f\u7b14\u8bb0\u672c\u7535\u8111\u7684\u4e2d\u592e\u5904\u7406\u5668\u30021971\u5e74\uff0cIntel\u516c\u53f8\u7684\u5de5\u7a0b\u5e08\u53d1\u660e\u4e86\u4e16\u754c\u4e0a\u7b2c\u4e00\u4e2a\u5546\u7528\u5fae\u5904\u7406\u5668\uff0c\u7531\u6b64\u5f00\u542f\u4e86\u8ba1\u7b97\u673a\u548c\u4e92\u8054\u7f51\u9769\u547d\uff0cIntel\u4e5f\u4f34\u968f\u7740\u4e92\u8054\u7f51\u7684\u7b2c\u4e00\u4e2a\u9ec4\u91d1\u65f6\u4ee3\u8d77\u98de\u3002\u7531\u4e8e\u82af\u7247\u7684\u6750\u6599\u4e3b\u8981\u662f\u7845\u8fd9\u79cd\u534a\u5bfc\u4f53\uff0c\u7845\u8c37\u4e5f\u56e0\u6b64\u5f97\u540d\u3002Intel\u521b\u59cb\u4eba\u6469\u5c14\u63d0\u51fa\u8457\u540d\u7684\u6469\u5c14\u5b9a\u5f8b\uff1a\u5f53\u4ef7\u683c\u4e0d\u53d8\u65f6\uff0c\u96c6\u6210\u7535\u8def\u4e0a\u53ef\u5bb9\u7eb3\u7684\u5143\u5668\u4ef6\u7684\u6570\u76ee\uff0c\u7ea6\u6bcf\u969418-24\u4e2a\u6708\u4fbf\u4f1a\u589e\u52a0\u4e00\u500d\uff0c\u6027\u80fd\u4e5f\u5c06\u63d0\u5347\u4e00\u500d\u3002\u7136\u800c\uff0c\u968f\u7740\u82af\u7247\u7ec4\u4ef6\u7684\u89c4\u6a21\u8d8a\u6765\u8d8a\u63a5\u8fd1\u5355\u4e2a\u539f\u5b50\u7684\u89c4\u6a21\uff0c\u6469\u5c14\u5b9a\u5f8b\u5df2\u9010\u6e10\u5931\u6548\uff0c\u534a\u5bfc\u4f53\u884c\u4e1a\u53d1\u5c55\u9010\u6e10\u653e\u7f13\uff0c\u8be5\u884c\u4e1a\u6b63\u91c7\u7528\u5176\u4ed6\u8ba1\u7b97\u65b9\u5f0f\uff08\u5982GPU\uff09\u3001\u5148\u8fdb\u8f6f\u4ef6\u67b6\u6784\u548c\u5de5\u5177\uff0c\u4ee5\u53ca\u65b0\u7684\u82af\u7247\u7535\u8def\u5c01\u88c5\u65b9\u6cd5\u6765\u63d0\u5347\u6574\u4f53\u8ba1\u7b97\u6027\u80fd\u3002","title":"2.1 CPU"},{"location":"computing-and-computing-power/#22-gpu","text":"GPU\u5373\u56fe\u5f62\u5904\u7406\u5668\uff0c\u53c8\u79f0\u663e\u5361\uff0c\u8d1f\u8d23\u8f93\u51fa\u663e\u793a\u56fe\u5f62\u3002\u7531\u4e8eGPU\u6709\u5927\u91cf\u7684\u6838\u5fc3\u548c\u9ad8\u901f\u5185\u5b58\uff0c\u64c5\u957f\u5e76\u884c\u8ba1\u7b97\uff0c\u56e0\u6b64\u88ab\u7528\u6765\u5904\u7406\u9700\u8981\u5927\u91cf\u77e9\u9635\u8fd0\u7b97\u7684\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u3002\u4e3b\u6d41\u663e\u5361\u7684\u663e\u793a\u82af\u7247\u4e3b\u8981\u7531NVIDIA\uff08\u82f1\u4f1f\u8fbe\uff09\u548cAMD\u4e24\u5927\u5382\u5546\u5236\u9020\uff0c\u901a\u5e38NVIDIA\u7684\u663e\u5361\u79f0\u4e3aN\u5361\uff0cAMD\u7684\u663e\u5361\u79f0\u4e3aA\u5361\u3002\u4e3b\u6d41\u6df1\u5ea6\u5b66\u4e60\u6240\u7528\u7684\u663e\u5361\u4e3b\u8981\u662fN\u5361\uff0c\u56e0\u4e3a\u6df1\u5ea6\u5b66\u4e60\u8981\u7528\u5230CUDA\u52a0\u901f\u3002CUDA\u662fNVIDIA\u63a8\u51fa\u7684GPU\u5e76\u884c\u8ba1\u7b97\u6846\u67b6\uff0c\u53ea\u80fd\u5728\u81ea\u5bb6GPU\u4e0a\u8fd0\u884c\uff0c\u5e76\u4e14\u51e0\u4e4e\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u90fd\u9996\u9009CUDA\u4f5c\u4e3a\u5e95\u5c42\u52a0\u901f\u5e93\u3002\u6df1\u53d7\u5e7f\u5927AI\u70bc\u4e39\u5e08\u559c\u7231\u7684\u663e\u5361\u6709GTX1080Ti\u3001GTX3080Ti\u3001Tesla V100\u3001Tesla A100\u7b49\u3002","title":"2.2 GPU"},{"location":"computing-and-computing-power/#23-fpga","text":"FPGA\u662f\u53ef\u7f16\u7a0b\u96c6\u6210\u7535\u8def\uff0cFPGA\u5185\u90e8\u7684\u7535\u8def\u65e8\u5728\u5b9e\u73b0\u5404\u79cd\u4e0d\u540c\u7684\u529f\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u6839\u636e\u9700\u8981\u91cd\u65b0\u7f16\u7a0b\u4ee5\u6267\u884c\u8fd9\u4e9b\u529f\u80fd\u3002FPGA\u53ef\u4ee5\u5728\u5236\u9020\u540e\u7531\u7528\u6237\u7f16\u7a0b\u7528\u4e8e\u7279\u5b9a\u7528\u9014\u3002FPGA\u5305\u542b\u901a\u8fc7\u53ef\u7f16\u7a0b\u4e92\u8fde\u8fde\u63a5\u7684\u81ea\u9002\u5e94\u903b\u8f91\u6a21\u5757 (ALM) \u548c\u903b\u8f91\u5143\u4ef6(LE)\u3002\u8fd9\u4e9b\u5757\u521b\u5efa\u4e86\u903b\u8f91\u95e8\u7684\u7269\u7406\u9635\u5217\uff0c\u53ef\u4ee5\u5b9a\u5236\u4ee5\u6267\u884c\u7279\u5b9a\u7684\u8ba1\u7b97\u4efb\u52a1\u3002\u76ee\u524d\u5168\u7403FPGA\u5e02\u573a\u7531\u56db\u5927\u5de8\u5934AMD\uff08Xilinx\uff09\u3001\u82f1\u7279\u5c14\u3001Lattice\u3001Microsemi\u5784\u65ad\uff0c\u56db\u5927\u5382\u5546\u7684\u5e02\u573a\u5360\u6709\u7387\u8fbe\u5230\u4e8696%\u3002FPGA\u4f5c\u4e3a\u4ea7\u4e1a\u65b0\u52a8\u80fd\u7684\u5f3a\u5927\u5f15\u64ce\u5bf9\u201c\u65b0\u57fa\u5efa\u201d\u7684\u6838\u5fc3\u9886\u57df5G\u3001\u7269\u8054\u7f51\u3001\u6570\u636e\u4e2d\u5fc3\u7b49\u90fd\u8d77\u5230\u975e\u5e38\u91cd\u8981\u7684\u4f5c\u7528\u3002","title":"2.3 FPGA"},{"location":"computing-and-computing-power/#24-asic","text":"ASIC\u5c5e\u4e8e\u4e13\u7528\u96c6\u6210\u7535\u8def\uff0c\u662f\u4e3a\u7279\u5b9a\u529f\u80fd\u800c\u4e13\u95e8\u6784\u5efa\u548c\u6279\u91cf\u751f\u4ea7\u7684\u3002\u4e0e FPGA \u4e0d\u540c\uff0c\u5b83\u4eec\u4e0d\u80fd\u91cd\u65b0\u7f16\u7a0b\uff0c\u4f46ASIC\u5177\u6709\u4f53\u79ef\u66f4\u5c0f\u3001\u91cd\u91cf\u66f4\u8f7b\u3001\u529f\u8017\u66f4\u4f4e\u3001\u53ef\u9760\u6027\u63d0\u9ad8\u3001\u6027\u80fd\u63d0\u9ad8\u3001\u4fdd\u5bc6\u6027\u589e\u5f3a\u3001\u6210\u672c\u964d\u4f4e\u7b49\u4f18\u70b9\u3002\u4ee5\u524d\uff0c\u4eba\u4eec\u90fd\u662f\u7528PC\uff08x86\u901a\u7528\u82af\u7247\uff09\u6316\u77ff\uff0c\u540e\u6765\u8d8a\u6316\u96be\u5ea6\u8d8a\u5927\uff0c\u7b97\u529b\u4e0d\u591f\u3002\u4e8e\u662f\uff0c\u5f00\u59cb\u4f7f\u7528\u663e\u5361\uff08GPU\uff09\u53bb\u6316\u77ff\u3002\u518d\u540e\u6765\uff0c\u663e\u5361\u7684\u80fd\u8017\u592a\u9ad8\uff0c\u6316\u51fa\u6765\u7684\u5e01\u503c\u8fd8\u62b5\u4e0d\u4e0a\u7535\u8d39\uff0c\u5c31\u5f00\u59cb\u91c7\u7528FPGA\u548cASIC\u96c6\u7fa4\u9635\u5217\u6316\u77ff[1]\u3002","title":"2.4 ASIC"},{"location":"computing-and-computing-power/#3","text":"","title":"3 \u8ba1\u7b97\u8303\u5f0f"},{"location":"computing-and-computing-power/#31","text":"\u5e76\u884c\u8ba1\u7b97\u88ab\u5b9a\u4e49\u4e3a\u4e00\u79cd\u540c\u65f6\u4f7f\u7528\u591a\u4e2a\u8ba1\u7b97\u673a\u7cfb\u7edf\u7684\u8ba1\u7b97\u7c7b\u578b\u3002\u5728\u8fd9\u91cc\uff0c\u4e00\u4e2a\u95ee\u9898\u88ab\u5206\u89e3\u6210\u5b50\u95ee\u9898\uff0c\u7136\u540e\u8fdb\u4e00\u6b65\u5206\u89e3\u6210\u6307\u4ee4\u3002\u6bcf\u4e2a\u5b50\u95ee\u9898\u7684\u8fd9\u4e9b\u6307\u4ee4\u5728\u4e0d\u540c\u7684\u5904\u7406\u5668\u4e0a\u540c\u65f6\u6267\u884c\u3002\u5728\u4e0b\u56fe\u4e2d\uff0c\u4f60\u53ef\u4ee5\u770b\u5230\u5e76\u884c\u8ba1\u7b97\u7cfb\u7edf\u662f\u5982\u4f55\u7531\u591a\u4e2a\u5904\u7406\u5668\u7ec4\u6210\u7684\uff0c\u8fd9\u4e9b\u5904\u7406\u5668\u76f8\u4e92\u901a\u4fe1\u5e76\u540c\u65f6\u5728\u5171\u4eab\u5185\u5b58\u4e0a\u6267\u884c\u591a\u4e2a\u4efb\u52a1\u3002\u5e76\u884c\u8ba1\u7b97\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u8282\u7701\u65f6\u95f4\u548c\u63d0\u4f9b\u5e76\u53d1\u6027\u3002","title":"3.1 \u5e76\u884c\u8ba1\u7b97"},{"location":"computing-and-computing-power/#32","text":"\u5206\u5e03\u5f0f\u8ba1\u7b97\u88ab\u5b9a\u4e49\u4e3a\u4e00\u79cd\u8ba1\u7b97\u7c7b\u578b\uff0c\u5176\u4e2d\u591a\u4e2a\u8ba1\u7b97\u673a\u7cfb\u7edf\u5bf9\u4e00\u4e2a\u95ee\u9898\u8fdb\u884c\u5de5\u4f5c\u3002\u5728\u8fd9\u91cc\uff0c\u6240\u6709\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\u88ab\u8fde\u63a5\u5728\u4e00\u8d77\uff0c\u95ee\u9898\u88ab\u5206\u4e3a\u5b50\u95ee\u9898\uff0c\u6bcf\u4e2a\u90e8\u5206\u7531\u4e0d\u540c\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\u6765\u89e3\u51b3\u3002\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u76ee\u6807\u662f\u63d0\u9ad8\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u786e\u4fdd\u5bb9\u9519\u3002\u5728\u4e0b\u56fe\u4e2d\uff0c\u6bcf\u4e2a\u5904\u7406\u5668\u90fd\u6709\u81ea\u5df1\u7684\u672c\u5730\u5b58\u50a8\u5668\uff0c\u6240\u6709\u5904\u7406\u5668\u901a\u8fc7\u7f51\u7edc\u76f8\u4e92\u901a\u4fe1\u3002","title":"3.2 \u5206\u5e03\u5f0f\u8ba1\u7b97"},{"location":"computing-and-computing-power/#321","text":"\u7f51\u683c\u8ba1\u7b97\u662f\u7531\u8ba1\u7b97\u673a\u7ec4\u6210\u7684\u7f51\u7edc\uff0c\u5171\u540c\u6267\u884c\u5355\u53f0\u673a\u5668\u53ef\u80fd\u96be\u4ee5\u5904\u7406\u7684\u4efb\u52a1\u3002\u8be5\u7f51\u7edc\u4e0a\u7684\u6240\u6709\u8ba1\u7b97\u673a\u90fd\u5728\u540c\u4e00\u4e2a\u4fdd\u62a4\u4f1e\u4e0b\u5de5\u4f5c\uff0c\u88ab\u79f0\u4e3a\u865a\u62df\u8d85\u7ea7\u8ba1\u7b97\u673a\u3002\u4ed6\u4eec\u5de5\u4f5c\u7684\u4efb\u52a1\u8981\u4e48\u662f\u9ad8\u8ba1\u7b97\u80fd\u529b\uff0c\u8981\u4e48\u662f\u7531\u5927\u578b\u6570\u636e\u96c6\u7ec4\u6210\u3002\u7f51\u683c\u8ba1\u7b97\u4e2d\u8ba1\u7b97\u673a\u7cfb\u7edf\u4e4b\u95f4\u7684\u6240\u6709\u901a\u4fe1\u90fd\u662f\u5728 \"\u6570\u636e\u7f51\u683c \"\u4e0a\u5b8c\u6210\u7684\u3002\u7f51\u683c\u8ba1\u7b97\u7684\u76ee\u6807\u662f\u5728\u66f4\u77ed\u7684\u65f6\u95f4\u5185\u89e3\u51b3\u66f4\u591a\u7684\u9ad8\u8ba1\u7b97\u95ee\u9898\uff0c\u63d0\u9ad8\u751f\u4ea7\u529b\u3002","title":"3.2.1 \u7f51\u683c\u8ba1\u7b97"},{"location":"computing-and-computing-power/#322","text":"\u4e91\u88ab\u5b9a\u4e49\u4e3a\u4f7f\u7528\u522b\u4eba\u7684\u670d\u52a1\u5668\u6765\u6258\u7ba1\u3001\u5904\u7406\u6216\u5b58\u50a8\u6570\u636e\u3002\u4e91\u8ba1\u7b97\u88ab\u5b9a\u4e49\u4e3a\u4e00\u79cd\u8ba1\u7b97\u7c7b\u578b\uff0c\u5b83\u662f\u5728\u4e92\u8054\u7f51\u4e0a\u4ee5\u968f\u7528\u968f\u4ed8\u7684\u65b9\u5f0f\u63d0\u4f9b\u6309\u9700\u8ba1\u7b97\u670d\u52a1\u3002","title":"3.2.2 \u4e91\u8ba1\u7b97"},{"location":"computing-and-computing-power/#323","text":"\u8fb9\u7f18\u8ba1\u7b97\u88ab\u5b9a\u4e49\u4e3a\u4e13\u6ce8\u4e8e\u51cf\u5c11\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u5668\u4e4b\u95f4\u957f\u8ddd\u79bb\u901a\u4fe1\u7684\u8ba1\u7b97\u7c7b\u578b\u3002\u8fd9\u662f\u901a\u8fc7\u5728\u4e91\u4e2d\u8fd0\u884c\u8f83\u5c11\u7684\u8fdb\u7a0b\uff0c\u5e76\u5c06\u8fd9\u4e9b\u8fdb\u7a0b\u8f6c\u79fb\u5230\u7528\u6237\u7684\u8ba1\u7b97\u673a\u3001\u7269\u8054\u7f51\u8bbe\u5907\u6216\u8fb9\u7f18\u8bbe\u5907/\u670d\u52a1\u5668\u4e0a\u3002","title":"3.2.3 \u8fb9\u7f18\u8ba1\u7b97"},{"location":"computing-and-computing-power/#33","text":"\u96c6\u7fa4\u662f\u4e00\u7ec4\u72ec\u7acb\u7684\u8ba1\u7b97\u673a\uff0c\u5b83\u4eec\u4e00\u8d77\u5de5\u4f5c\u4ee5\u6267\u884c\u7ed9\u5b9a\u7684\u4efb\u52a1\u3002\u96c6\u7fa4\u8ba1\u7b97\u88ab\u5b9a\u4e49\u4e3a\u4e00\u79cd\u8ba1\u7b97\u7c7b\u578b\uff0c\u7531\u4e24\u53f0\u6216\u66f4\u591a\u7684\u72ec\u7acb\u8ba1\u7b97\u673a\u7ec4\u6210\uff0c\u88ab\u79f0\u4e3a\u8282\u70b9\uff0c\u4f5c\u4e3a\u4e00\u53f0\u673a\u5668\u4e00\u8d77\u5de5\u4f5c\u6267\u884c\u4efb\u52a1\u3002\u96c6\u7fa4\u8ba1\u7b97\u7684\u76ee\u6807\u662f\u63d0\u9ad8\u7cfb\u7edf\u7684\u6027\u80fd\u3001\u53ef\u6269\u5c55\u6027\u548c\u7b80\u5355\u6027\u3002\u6b63\u5982\u4e0b\u56fe\u4e2d\u770b\u5230\u7684\uff0c\u6240\u6709\u7684\u8282\u70b9\uff0c\uff08\u65e0\u8bba\u5b83\u4eec\u662f\u7236\u8282\u70b9\u8fd8\u662f\u5b50\u8282\u70b9\uff09\uff0c\u90fd\u4f5c\u4e3a\u4e00\u4e2a\u5355\u4e00\u7684\u5b9e\u4f53\u6765\u6267\u884c\u4efb\u52a1\u3002","title":"3.3 \u96c6\u7fa4\u8ba1\u7b97"},{"location":"computing-and-computing-power/#34","text":"\u9ad8\u6027\u80fd\u8ba1\u7b97\u7814\u7a76\u96c6\u7fa4\u67b6\u6784\u3001\u5e76\u884c\u7b97\u6cd5\u548c\u76f8\u5173\u8f6f\u4ef6\u57fa\u7840\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u8ba1\u7b97\u5b9e\u73b0\u5355\u53f0\u8ba1\u7b97\u673a\u65e0\u6cd5\u8fbe\u5230\u7684\u8fd0\u7b97\u901f\u5ea6\u3002\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e3b\u8981\u5e94\u7528\u9886\u57df\u6709\uff1a \u79d1\u5b66\u8ba1\u7b97\u7c7b\uff1a\u7269\u7406\u5316\u5b66\u3001\u6c14\u8c61\u73af\u4fdd\u3001\u751f\u547d\u79d1\u5b66\u3001\u77f3\u6cb9\u52d8\u63a2\u3001\u5929\u6587\u63a2\u6d4b\u7b49\u3002 \u5de5\u7a0b\u8ba1\u7b97\u7c7b\uff1a\u8ba1\u7b97\u673a\u8f85\u52a9\u5de5\u7a0b\u3001\u8ba1\u7b97\u673a\u8f85\u52a9\u5236\u9020\u3001\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\u3001\u7535\u78c1\u4eff\u771f\u7b49\u3002 \u667a\u80fd\u8ba1\u7b97\u7c7b\uff1a\u5373\u4eba\u5de5\u667a\u80fd\uff08AI\uff0cArtificial Intelligence\uff09\u8ba1\u7b97\uff0c\u5305\u62ec\uff1a\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u6570\u636e\u5206\u6790\u7b49\u3002","title":"3.4 \u9ad8\u6027\u80fd\u8ba1\u7b97"},{"location":"computing-and-computing-power/#4","text":"\u8ba1\u7b97\u8bbe\u65bd\uff08\u6216\u57fa\u7840\u8ba1\u7b97\u8bbe\u65bd\uff09\uff0c\u5c31\u5982\u5b66\u6821\u662f\u6559\u80b2\u8bbe\u65bd\u3001\u533b\u9662\u662f\u533b\u7597\u8bbe\u65bd\u3001\u94c1\u8def\u662f\u4ea4\u901a\u8bbe\u65bd\u4e00\u6837\uff0c\u5f3a\u8c03\u7684\u662f\u5177\u4f53\u7684\u7269\u7406\u5f62\u6001\u3002\u56e0\u6b64\u5c06\u8ba1\u7b97\u8bbe\u65bd\u5206\u4e3a\u96c6\u7fa4\u3001\u8d85\u7b97\u3001\u6570\u636e\u4e2d\u5fc3\u3002","title":"4 \u8ba1\u7b97\u8bbe\u65bd"},{"location":"computing-and-computing-power/#41","text":"\u96c6\u7fa4\u5c31\u662f\u6307\u4e00\u7ec4\uff08\u82e5\u5e72\u4e2a\uff09\u76f8\u4e92\u72ec\u7acb\u540c\u65f6\u53ef\u4ee5\u901a\u8fc7\u7f51\u7edc\u5f7c\u6b64\u4e92\u8054\u7684\u8ba1\u7b97\u673a\uff0c\u6bcf\u4e2a\u96c6\u7fa4\u8282\u70b9\uff08\u5373\u96c6\u7fa4\u4e2d\u7684\u6bcf\u53f0\u8ba1\u7b97\u673a\uff09\u90fd\u662f\u8fd0\u884c\u5404\u81ea\u670d\u52a1\u7684\u72ec\u7acb\u670d\u52a1\u5668\uff0c\u8fd9\u4e9b\u670d\u52a1\u5668\u5408\u4f5c\u505a\u540c\u4e00\u4ef6\u4e8b\u3002\u5f53\u7528\u6237\u8bf7\u6c42\u96c6\u7fa4\u7cfb\u7edf\u65f6\uff0c\u96c6\u7fa4\u7ed9\u7528\u6237\u7684\u611f\u89c9\u5c31\u662f\u4e00\u4e2a\u5355\u4e00\u72ec\u7acb\u7684\u670d\u52a1\u5668\uff0c\u800c\u5b9e\u9645\u4e0a\u7528\u6237\u8bf7\u6c42\u7684\u662f\u4e00\u7ec4\u96c6\u7fa4\u670d\u52a1\u5668\u3002 \u5206\u5e03\u5f0f\u4e0e\u96c6\u7fa4\u7684\u533a\u522b\u662f\u4ec0\u4e48\uff1f\u5f15\u7528\u4e00\u4e2a\u4f8b\u5b50\u8bf4\u660e[7]\uff1a\u5c0f\u996d\u5e97\u539f\u6765\u53ea\u6709\u4e00\u4e2a\u53a8\u5e08\uff0c\u5207\u83dc\u6d17\u83dc\u5907\u6599\u7092\u83dc\u5168\u5e72\u3002\u540e\u6765\u5ba2\u4eba\u591a\u4e86\uff0c\u53a8\u623f\u4e00\u4e2a\u53a8\u5e08\u5fd9\u4e0d\u8fc7\u6765\uff0c\u53c8\u8bf7\u4e86\u4e2a\u53a8\u5e08\uff0c\u4e24\u4e2a\u53a8\u5e08\u90fd\u80fd\u7092\u4e00\u6837\u7684\u83dc\uff0c\u8fd9\u4e24\u4e2a\u53a8\u5e08\u7684\u5173\u7cfb\u662f**\u96c6\u7fa4**\u3002\u4e3a\u4e86\u8ba9\u53a8\u5e08\u4e13\u5fc3\u7092\u83dc\uff0c\u628a\u83dc\u505a\u5230\u6781\u81f4\uff0c\u53c8\u8bf7\u4e86\u4e2a\u914d\u83dc\u5e08\u8d1f\u8d23\u5207\u83dc\uff0c\u5907\u83dc\uff0c\u5907\u6599\uff0c\u53a8\u5e08\u548c\u914d\u83dc\u5e08\u7684\u5173\u7cfb\u662f**\u5206\u5e03\u5f0f**\u3002","title":"4.1 \u96c6\u7fa4"},{"location":"computing-and-computing-power/#42","text":"\u8d85\u7b97\u4e3b\u8981\u63d0\u4f9b\u56fd\u5bb6\u9ad8\u79d1\u6280\u9886\u57df\u548c\u5c16\u7aef\u6280\u672f\u7814\u7a76\u9700\u7684\u8fd0\u7b97\u901f\u5ea6\u548c\u5b58\u50a8\u5bb9\u91cf\uff0c\u5305\u62ec\u822a\u5929\u3001\u56fd\u9632\u3001\u77f3\u6cb9\u52d8\u63a2\u3001\u6c14\u5019\u5efa\u6a21\u548c\u57fa\u56e0\u7ec4\u6d4b\u5e8f\u7b49\u3002","title":"4.2 \u8d85\u7b97"},{"location":"computing-and-computing-power/#43","text":"\u6570\u636e\u4e2d\u5fc3\u4e3b\u8981\u9762\u5411\u9700\u8981\u4fe1\u606f\u6280\u672f\u670d\u52a1\u7684\u5546\u4e1a\u573a\u666f\uff08\u4e91\uff09\u3002\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\uff1a","title":"4.3 \u6570\u636e\u4e2d\u5fc3"},{"location":"computing-and-computing-power/#_2","text":"[1] \u300a\u5173\u4e8e\u201c\u7b97\u529b\u201d\uff0c\u8fd9\u7bc7\u6587\u7ae0\u503c\u5f97\u4e00\u770b\u300b https://36kr.com/p/1848541284191362 [2] \u90a2\u6587\u5a1f, \u96f7\u6ce2, \u8d75\u5029\u9896. \u7b97\u529b\u57fa\u7840\u8bbe\u65bd\u53d1\u5c55\u73b0\u72b6\u4e0e\u8d8b\u52bf\u5c55\u671b[J]. \u7535\u4fe1\u79d1\u5b66, 2022, 38(6): 51-61. http://www.infocomm-journal.com/dxkx/CN/10.11959/j.issn.1000-0801.2022137 [3] \u300a\u79d1\u666e\uff1a\u5e76\u884c\u8ba1\u7b97\u3001\u5206\u5e03\u5f0f\u8ba1\u7b97\u3001\u96c6\u7fa4\u8ba1\u7b97\u548c\u4e91\u8ba1\u7b97\u300b https://max2d.com/archives/976 [4] \u300a\u4ec0\u4e48\u662fASIC\uff1fASIC\u4f9b\u5e94\u5546\u6709\u54ea\u4e9b\uff1f\u300b https://mp.ofweek.com/ee/a356714038087 [5] Different Computing Paradigms https://www.geeksforgeeks.org/different-computing-paradigms/ [6] \u300a2022\u4e2d\u56fd\u4eba\u5de5\u667a\u80fd\u82af\u7247\u884c\u4e1a\u7814\u7a76\u62a5\u544a\u300b https://pdf.dfcfw.com/pdf/H3_AP202204131558974968_1.pdf?1649846951000.pdf [7] \u5206\u5e03\u5f0f\u4e0e\u96c6\u7fa4\u7684\u533a\u522b\u662f\u4ec0\u4e48\uff1f - \u5f20\u9e4f\u98de\u7684\u56de\u7b54 - \u77e5\u4e4e https://www.zhihu.com/question/20004877/answer/112124929 [8] \u6570\u636e\u4e2d\u5fc3\u5b9a\u4e49\u3001\u5206\u7c7b\u53ca\u4ea7\u4e1a\u94fe https://blog.51cto.com/kymdidicom/3140009","title":"\u53c2\u8003\u8d44\u6599"},{"location":"csapp-00/","text":"CSAPP\u6982\u5ff5\u6574\u7406\uff08\u4e00\uff09 \u00b6 \u7f16\u8bd1\u7cfb\u7edf\uff1a\u9884\u5904\u7406\u5668\u3001\u7f16\u8bd1\u5668\u3001\u6c47\u7f16\u5668\u3001\u94fe\u63a5\u5668\u4e00\u8d77\u6784\u6210\u4e86\u7f16\u8bd1\u7cfb\u7edf\u3002 GNU/Linux\uff1aGNU\u73af\u5883+Linux\u5185\u6838 \u7cfb\u7edf\u7684\u786c\u4ef6\u7ec4\u6210\uff1a \u603b\u7ebf\uff1a\u643a\u5e26\u5b57\u8282\u4fe1\u606f\u5e76\u8d1f\u8d23\u5728\u5404\u4e2a\u90e8\u4ef6\u95f4\u4f20\u9012\u3002\u4f20\u9001\u5b9a\u957f\u7684\u5b57\u8282\u5757\uff0c\u4e5f\u5c31\u662f\u5b57\uff0c\u5b57\u8282\u6570\u5373\u5b57\u957f\u662f\u4e00\u4e2a\u57fa\u672c\u7684\u7cfb\u7edf\u53c2\u6570\uff0c\u5927\u591a\u6570\u673a\u5668\u662f4\u4e2a\u5b57\u8282\uff0832\u4f4d\uff09\u62168\u4e2a\u5b57\u8282\uff0864\u4f4d\uff09\u3002 I/O\u8bbe\u5907\uff1a\u952e\u76d8\u3001\u9f20\u6807\u3001\u663e\u793a\u5668\u3001\u78c1\u76d8\u3002\u6bcf\u4e2aI/O\u8bbe\u5907\u90fd\u901a\u8fc7\u4e00\u4e2a\u63a7\u5236\u5668\u6216\u9002\u914d\u5668\u4e0eI/O\u603b\u7ebf\u76f8\u8fde\u3002 \u4e3b\u5b58\uff08\u5185\u5b58\uff09\uff1a\u4e3b\u5b58\u662f\u4e00\u4e2a\u4e34\u65f6\u5b58\u50a8\u8bbe\u5907\uff0c\u7531\u4e00\u7ec4\u52a8\u6001\u968f\u673a\u5b58\u53d6\u5b58\u50a8\u5668\uff08DRAM\uff09\u82af\u7247\u7ec4\u6210\u7684\u3002\u5b58\u50a8\u5668\u662f\u4e00\u4e2a\u7ebf\u6027\u7684\u5b57\u8282\u6570\u7ec4\uff0c\u6bcf\u4e2a\u5b57\u8282\u90fd\u6709\u5176\u552f\u4e00\u7684\u5730\u5740\uff08\u6570\u7ec4\u7d22\u5f15\uff09\uff0c\u7ec4\u6210\u7a0b\u5e8f\u7684\u6bcf\u6761\u673a\u5668\u6307\u4ee4\u90fd\u7531\u4e0d\u540c\u6570\u91cf\u7684\u5b57\u8282\u6784\u6210\u3002 \u5904\u7406\u5668\uff08CPU\uff09\uff1a\u89e3\u91ca\u6216\u6267\u884c\u5b58\u50a8\u5728\u4e3b\u5b58\u4e2d\u7684\u6307\u4ee4\u3002\u5904\u7406\u5668\u4e00\u76f4\u5728\u4e0d\u65ad\u5730\u4ecePC\u6307\u5411\u7684\u5185\u5b58\u5904\u8bfb\u53d6\u6307\u4ee4\uff0c\u6267\u884c\u6307\u4ee4\u7684\u64cd\u4f5c\uff0c\u518d\u66f4\u65b0PC\uff0c\u4f7f\u5176\u6307\u5411\u4e0b\u4e00\u6761\u6307\u4ee4\u3002 \u7a0b\u5e8f\u8ba1\u6570\u5668\uff08PC\uff09\uff1aPC\u6307\u5411\u4e3b\u5b58\u4e2d\u67d0\u6761\u673a\u5668\u8bed\u8a00\u6307\u4ee4\uff08\u5373\u542b\u6709\u8be5\u6761\u6307\u4ee4\u7684\u5730\u5740\uff09\u3002 \u7b97\u672f\u903b\u8f91\u5355\u5143\uff08ALU\uff09\uff1a\u8ba1\u7b97\u65b0\u7684\u6570\u636e\u548c\u5730\u5740\u503c\u3002 \u5bc4\u5b58\u5668\u6587\u4ef6\uff08Register File\uff09\uff1a\u8d1f\u8d23\u5b58\u50a8\u3002 \u6307\u4ee4\u96c6\u67b6\u6784\uff08ISA\uff09\uff1a\u5904\u7406\u5668\u6309\u7167\u4e00\u4e2a\u6307\u4ee4\u6267\u884c\u6a21\u578b\u6765\u64cd\u4f5c\uff0c\u8fd9\u4e2a\u6a21\u578b\u7531\u6307\u4ee4\u96c6\u67b6\u6784\u51b3\u5b9a\u3002\u6307\u4ee4\u96c6\u67b6\u6784\u63cf\u8ff0\u7684\u662f\u6bcf\u6761\u673a\u5668\u4ee3\u7801\u6307\u4ee4\u7684\u6548\u679c\uff1b\u800c\u5fae\u4f53\u7cfb\u7ed3\u6784\u63cf\u8ff0\u7684\u662f\u5904\u7406\u5668\u5b9e\u9645\u4e0a\u5982\u4f55\u5b9e\u73b0\u3002 cache\uff1a\u9ad8\u901f\u7f13\u5b58\u5b58\u50a8\u5668\uff08cache memory\uff09\u3002 SRAM\uff1a\u9759\u6001\u968f\u673a\u8bbf\u95ee\u5b58\u50a8\u5668\u3002 \u5b58\u50a8\u5668\u5c42\u6b21\u7ed3\u6784\uff1a L0\uff1a\u5bc4\u5b58\u5668\u3002CPU\u5bc4\u5b58\u5668\u4fdd\u5b58\u6765\u81ea\u9ad8\u901f\u7f13\u5b58\u7684\u5b57\u3002 L1\uff1aL1\u9ad8\u901f\u7f13\u5b58\uff08SRAM\uff09\u3002L1\u9ad8\u901f\u7f13\u5b58\u4fdd\u5b58\u53d6\u81eaL2\u9ad8\u901f\u7f13\u5b58\u7684\u9ad8\u901f\u7f13\u5b58\u884c\u3002 L2\uff1aL2\u9ad8\u901f\u7f13\u5b58\uff08SRAM\uff09\u3002L2\u9ad8\u901f\u7f13\u5b58\u4fdd\u5b58\u53d6\u81eaL3\u9ad8\u901f\u7f13\u5b58\u7684\u9ad8\u901f\u7f13\u5b58\u884c\u3002 L3\uff1aL3\u9ad8\u901f\u7f13\u5b58\uff08SRAM\uff09\u3002L3\u9ad8\u901f\u7f13\u5b58\u4fdd\u5b58\u53d6\u81ea\u4e3b\u5b58\u7684\u9ad8\u901f\u7f13\u5b58\u884c\u3002 L4\uff1a\u4e3b\u5b58\uff08DRAM\uff09\u3002\u4e3b\u5b58\u4fdd\u5b58\u53d6\u81ea\u672c\u5730\u78c1\u76d8\u7684\u78c1\u76d8\u5757\u3002 L5\uff1a\u672c\u5730\u4e8c\u7ea7\u5b58\u50a8\uff08\u672c\u5730\u78c1\u76d8\uff09\u3002\u672c\u5730\u78c1\u76d8\u4fdd\u5b58\u53d6\u81ea\u8fdc\u7a0b\u7f51\u7edc\u670d\u52a1\u5668\u4e0a\u78c1\u76d8\u7684\u6587\u4ef6\u3002 L6\uff1a\u8fdc\u7a0b\u4e8c\u7ea7\u5b58\u50a8\uff08\u5206\u5e03\u5f0f\u6587\u4ef6\u7cfb\u7edf\u3001web\u670d\u52a1\u5668\uff09\u3002 \u64cd\u4f5c\u7cfb\u7edf\uff1a\u5e94\u7528\u7a0b\u5e8f\uff08\u8f6f\u4ef6\uff09\u548c\u786c\u4ef6\u4e4b\u95f4\u7684\u4e00\u5c42\u8f6f\u4ef6\uff0c\u6240\u6709\u5e94\u7528\u7a0b\u5e8f\u5bf9\u786c\u4ef6\u7684\u64cd\u4f5c\u90fd\u5fc5\u987b\u901a\u8fc7\u64cd\u4f5c\u7cfb\u7edf\u3002\u64cd\u4f5c\u7cfb\u7edf\u901a\u8fc7\u51e0\u4e2a\u62bd\u8c61\u7684\u57fa\u672c\u6982\u5ff5\uff08\u8fdb\u7a0b\u3001\u865a\u62df\u5185\u5b58\u3001\u6587\u4ef6\uff09\u6765\u5b9e\u73b0\u3002 \u6587\u4ef6\u662fI/O\u8bbe\u5907\u7684\u62bd\u8c61\u3002 \u865a\u62df\u5185\u5b58\u662f\u4e3b\u5b58\u548cI/O\u8bbe\u5907\u7684\u62bd\u8c61\u3002 \u8fdb\u7a0b\u662f\u5904\u7406\u5668\u3001\u4e3b\u5b58\u548cI/O\u8bbe\u5907\u7684\u62bd\u8c61\u3002 \u8fdb\u7a0b\uff08Process\uff09\uff1a\u8fdb\u7a0b\u662f\u64cd\u4f5c\u7cfb\u7edf\u5bf9\u4e00\u4e2a\u6b63\u5728\u8fd0\u884c\u7684\u7a0b\u5e8f\u7684\u4e00\u79cd\u62bd\u8c61\u3002 \u5e76\u53d1\uff08Concurrency\uff09\uff1a\u4e00\u4e2a\u8fdb\u7a0b\u7684\u6307\u4ee4\u548c\u53e6\u4e00\u4e2a\u8fdb\u7a0b\u7684\u6307\u4ee4\u4ea4\u9519\u6267\u884c\u3002 \u4e0a\u4e0b\u6587\u5207\u6362\uff08Context Switch\uff09\uff1a\u8fd9\u79cd\u4ea4\u9519\u6267\u884c\u7684\u673a\u5236\u6210\u4e3a\u4e0a\u4e0b\u6587\u5207\u6362\u3002\u5f53\u64cd\u4f5c\u7cfb\u7edf\u51b3\u5b9a\u628a\u63a7\u5236\u6743\u4ece\u5f53\u524d\u8fdb\u7a0b\u8f6c\u79fb\u5230\u67d0\u4e2a\u65b0\u8fdb\u7a0b\u65f6\uff0c\u5c31\u4f1a\u4e0a\u4e0b\u6587\u5207\u6362\uff0c\u5373\u4fdd\u6301\u5f53\u524d\u8fdb\u7a0b\u7684\u4e0a\u4e0b\u6587\uff0c\u6062\u590d\u65b0\u8fdb\u7a0b\u7684\u4e0a\u4e0b\u6587\uff0c\u5c06\u63a7\u5236\u6743\u4f20\u9012\u7ed9\u65b0\u8fdb\u7a0b\u3002 \u4e0a\u4e0b\u6587\uff1a\u64cd\u4f5c\u7cfb\u7edf\u4fdd\u6301\u8ddf\u8e2a\u8fdb\u7a0b\u8fd0\u884c\u6240\u9700\u7684\u6240\u6709\u72b6\u6001\u4fe1\u606f\u3002 \u5185\u6838\uff1a\u4ece\u4e00\u4e2a\u8fdb\u7a0b\u5230\u53e6\u4e00\u4e2a\u8fdb\u7a0b\u7684\u8f6c\u6362\u662f\u7531\u64cd\u4f5c\u7cfb\u7edf\u5185\u6838\u7ba1\u7406\u7684\u3002\u5185\u6838\u662f\u64cd\u4f5c\u7cfb\u7edf\u5e38\u9a7b\u4e3b\u5b58\u7684\u90e8\u5206\u3002\u5f53\u5e94\u7528\u7a0b\u5e8f\u9700\u8981\u64cd\u4f5c\u7cfb\u7edf\u7684\u67d0\u4e9b\u64cd\u4f5c\u65f6\uff0c\u5b83\u5c31\u6267\u884c\u4e00\u6761\u7cfb\u7edf\u8c03\u7528\u6307\u4ee4\uff0c\u5c06\u63a7\u5236\u6743\u4f20\u9012\u7ed9\u5185\u6838\uff0c\u7136\u540e\u5185\u6838\u6267\u884c\u8bf7\u6c42\u7684\u64cd\u4f5c\u5e76\u8fd4\u56de\u5e94\u7528\u7a0b\u5e8f\u3002\u5185\u6838\u4e0d\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u8fdb\u7a0b\uff0c\u800c\u662f\u7cfb\u7edf\u7ba1\u7406\u5168\u90e8\u8fdb\u7a0b\u6240\u7528\u4ee3\u7801\u548c\u6570\u636e\u7ed3\u6784\u7684\u96c6\u5408\u3002 \u7ebf\u7a0b\uff08Thread\uff09\uff1a\u4e00\u4e2a\u8fdb\u7a0b\u5b9e\u9645\u4e0a\u53ef\u4ee5\u7531\u591a\u4e2a\u6210\u4e3a\u7ebf\u7a0b\u7684\u6267\u884c\u5355\u5143\u7ec4\u6210\uff0c\u6bcf\u4e2a\u7ebf\u7a0b\u90fd\u8fd0\u884c\u5728\u8fdb\u7a0b\u7684\u4e0a\u4e0b\u6587\u4e2d\uff0c\u5171\u4eab\u540c\u6837\u7684\u4ee3\u7801\u548c\u5168\u5c40\u6570\u636e\u3002 \u865a\u62df\u5185\u5b58\uff1a\u4e3a\u6bcf\u4e2a\u8fdb\u7a0b\u63d0\u4f9b\u4e00\u4e2a\u5047\u8c61\uff0c\u5373\u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u5728\u72ec\u5360\u4f7f\u7528\u4e3b\u5b58\u3002\u6bcf\u4e2a\u8fdb\u7a0b\u770b\u5230\u7684\u5185\u5b58\u90fd\u662f\u4e00\u81f4\u7684\uff0c\u6210\u4e3a\u865a\u62df\u5730\u5740\u7a7a\u95f4\u3002\u6bcf\u4e2a\u8fdb\u7a0b\u770b\u5230\u7684\u865a\u62df\u5730\u5740\u7a7a\u95f4\u7531\u5927\u91cf\u51c6\u786e\u5b9a\u4e49\u7684\u533a\u6784\u6210\uff0c\u6bcf\u4e2a\u533a\u7531\u4e13\u95e8\u7684\u529f\u80fd\u3002 \u6587\u4ef6\uff1a\u6587\u4ef6\u5c31\u662f\u5b57\u8282\u5e8f\u5217\u3002 \u7f51\u7edc\uff1a\u73b0\u4ee3\u7cfb\u7edf\u901a\u8fc7\u7f51\u7edc\u4e0e\u5176\u4ed6\u7cfb\u7edf\u8fde\u63a5\u5230\u4e00\u8d77\uff0c\u662f\u8ba1\u7b97\u673a\u7cfb\u7edf\u4e4b\u95f4\u901a\u4fe1\u7684\u624b\u6bb5\u3002\u7f51\u7edc\u53ef\u4ee5\u89c6\u4e3a\u4e00\u4e2aI/O\u8bbe\u5907\uff0c\u7cfb\u7edf\u4ece\u7f51\u7edc\u9002\u914d\u5668\u53d1\u9001\u548c\u8bfb\u53d6\u5176\u4ed6\u673a\u5668\u7684\u6570\u636e\u3002 \u5e76\u53d1\u548c\u5e76\u884c\uff1a\u5e76\u53d1\u662f\u4e00\u4e2a\u901a\u7528\u6982\u5ff5\uff0c\u6307\u4e00\u4e2a\u540c\u65f6\u5177\u6709\u591a\u4e2a\u6d3b\u52a8\u7684\u7cfb\u7edf\uff0c\u5e76\u53d1\u53ef\u4ee5\u4f7f\u7cfb\u7edf\u540c\u65f6\u505a\u66f4\u66f4\u591a\u4e8b\u60c5\uff08\u4e00\u65b9\u9762\u505a\u5f97\u66f4\u591a\uff0c\u4e00\u65b9\u9762\u8fd0\u884c\u5f97\u66f4\u5feb\u3002\uff09\uff1b\u800c\u5e76\u884c\uff08parallelism\uff09\u6307\u7684\u662f\u7528\u5e76\u53d1\u6765\u4f7f\u4e00\u4e2a\u7cfb\u7edf\u8fd0\u884c\u5730\u66f4\u5feb\u3002\u5e76\u884c\u53ef\u4ee5\u5728\u8ba1\u7b97\u673a\u7cfb\u7edf\u7684\u591a\u4e2a\u62bd\u8c61\u5c42\u6b21\u4e0a\u8fd0\u7528\uff1a \u7ebf\u7a0b\u7ea7\u5e76\u53d1\uff1a \u5355\u6838\u5904\u7406\u5668\uff1a\u5e76\u53d1\u662f\u6a21\u62df\u51fa\u6765\u7684\uff0c\u53ea\u662f\u901a\u8fc7\u8fdb\u7a0b\u95f4\u7684\u5feb\u901f\u5207\u6362\u5b9e\u73b0\u3002 \u591a\u6838\u5904\u7406\u5668\uff1a\u5c06\u591a\u4e2aCPU\u96c6\u6210\u5728\u4e00\u4e2a\u96c6\u6210\u7535\u8def\u82af\u7247\u4e0a\u3002 \u8d85\u7ebf\u7a0b/\u540c\u65f6\u591a\u7ebf\u7a0b\uff1a\u5e38\u89c4\u5904\u7406\u5668\u9700\u8981\u5927\u7ea620000\u4e2a\u65f6\u949f\u5468\u671f\u505a\u7ebf\u7a0b\u95f4\u5207\u6362\uff0c\u800c\u8d85\u7ebf\u7a0b\u7684\u5904\u7406\u5668\u53ef\u4ee5\u5728\u5355\u4e2a\u5468\u671f\u7684\u57fa\u7840\u4e0a\u51b3\u5b9a\u7ebf\u7a0b\u5207\u6362\u3002\u8fd9\u5fc5\u987b\u8981\u6c42\u7a0b\u5e8f\u662f\u4ee5\u591a\u7ebf\u7a0b\u65b9\u5f0f\u6765\u4e66\u5199\u7684\u3002 \u6307\u4ee4\u7ea7\u5e76\u884c\uff1a \u73b0\u4ee3\u5904\u7406\u5668\u53ef\u4ee5\u540c\u65f6\u6267\u884c\u591a\u6761\u6307\u4ee4\u7684\u5c5e\u6027\u79f0\u4e3a\u6307\u4ee4\u7ea7\u5e76\u884c\u3002 \u6d41\u6c34\u7ebf\uff08pipelining\uff09\uff1a\u5728\u6d41\u6c34\u7ebf\u4e2d\uff0c\u5c06\u6267\u884c\u4e00\u6761\u6307\u4ee4\u6240\u9700\u8981\u7684\u6d3b\u52a8\u5212\u5206\u6210\u4e0d\u540c\u7684\u6b65\u9aa4\uff0c\u5c06\u5904\u7406\u5668\u7684\u786c\u4ef6\u7ec4\u7ec7\u6210\u4e00\u7cfb\u5217\u7684\u9636\u6bb5\uff0c\u6bcf\u4e2a\u9636\u6bb5\u6267\u884c\u4e00\u4e2a\u6b65\u9aa4\uff0c\u8fd9\u4e9b\u9636\u6bb5\u53ef\u4ee5\u5e76\u884c\u64cd\u4f5c\uff0c\u7528\u6765\u5904\u7406\u4e0d\u540c\u6307\u4ee4\u7684\u4e0d\u540c\u90e8\u5206\u3002 \u8d85\u6807\u91cf\uff08superscalar\uff09\uff1a\u5982\u679c\u5904\u7406\u5668\u53ef\u4ee5\u8fbe\u5230\u6bd4\u4e00\u4e2a\u5468\u671f\u4e00\u6761\u6307\u4ee4\u66f4\u5feb\u7684\u6267\u884c\u901f\u7387\uff0c\u5c31\u79f0\u4e4b\u4e3a\u8d85\u6807\u91cf\u5904\u7406\u5668\u3002 \u5355\u6307\u4ee4 \u3001\u591a\u6570\u636e\u5e76\u884c\uff1a \u8bb8\u591a\u73b0\u4ee3\u5904\u7406\u5668\u62e5\u6709\u7279\u6b8a\u7684\u786c\u4ef6\uff0c\u5141\u8bb8\u4e00\u6761\u6307\u4ee4\u4ea7\u751f\u591a\u4e2a\u53ef\u4ee5\u5e76\u884c\u6267\u884c\u7684\u64cd\u4f5c\uff0c\u79f0\u4e3a\u5355\u6307\u4ee4\u3001\u591a\u6570\u636e\uff0c\u5373SIMD\u5e76\u884c\u3002 \u865a\u62df\u673a\uff1a\u5bf9\u6574\u4e2a\u8ba1\u7b97\u673a\u7684\u62bd\u8c61\uff0c\u5305\u62ec\u64cd\u4f5c\u7cfb\u7edf\u3001\u5904\u7406\u5668\u548c\u7a0b\u5e8f\u3002 \u6307\u4ee4\u96c6\u4f53\u7cfb\u7ed3\u6784\uff08ISA\uff09\uff1a\u6307\u4ee4\u88ab\u7f16\u7801\u4e3a\u7531\u4e00\u4e2a\u6216\u591a\u4e2a\u5b57\u8282\u5e8f\u5217\u7ec4\u6210\u7684\u4e8c\u8fdb\u5236\u683c\u5f0f\u3002\u4e00\u4e2a\u5904\u7406\u5668\u652f\u6301\u7684\u6307\u4ee4\u548c\u6307\u4ee4\u7684\u5b57\u8282\u7ea7\u7f16\u7801\u79f0\u4e3a\u5b83\u7684\u6307\u4ee4\u96c6\u4f53\u7cfb\u7ed3\u6784\u3002ISA\u5728\u7f16\u8bd1\u5668\u7f16\u5199\u8005\u548c\u5904\u7406\u5668\u8bbe\u8ba1\u4eba\u5458\u4e4b\u95f4\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6982\u5ff5\u62bd\u8c61\u5c42\uff0c\u7f16\u8bd1\u5668\u7f16\u5199\u8005\u53ea\u9700\u8981\u77e5\u9053\u5141\u8bb8\u54ea\u4e9b\u6307\u4ee4\u4ee5\u53ca\u5b83\u4eec\u662f\u5982\u4f55\u7f16\u7801\u7684\uff1b\u800c\u5904\u7406\u5668\u8bbe\u8ba1\u8005\u5fc5\u987b\u5efa\u9020\u51fa\u6267\u884c\u8fd9\u4e9b\u6307\u4ee4\u7684\u5904\u7406\u5668\u3002 HCL\uff08\u786c\u4ef6\u63a7\u5236\u8bed\u8a00\uff09\uff1a\u7528\u6765\u63cf\u8ff0\u5904\u7406\u5668\u7684\u8bbe\u8ba1\u3002 \u6d41\u6c34\u7ebf\u5316\u7684\u5904\u7406\u5668\uff1a\u5c06\u6bcf\u6761\u6307\u4ee4\u7684\u6267\u884c\u5206\u62105\u6b65\uff0c\u6bcf\u4e2a\u6b65\u9aa4\u7531\u4e00\u4e2a\u72ec\u7acb\u7684\u786c\u4ef6\u6216\u9636\u6bb5\u6765\u5904\u7406\u3002\u5904\u7406\u5668\u53ef\u4ee5\u540c\u65f6\u6267\u884c5\u6761\u6307\u4ee4\u7684\u4e0d\u540c\u9636\u6bb5\u3002 \u5192\u9669\u6216\u51b2\u7a81\uff1a\u5192\u9669\u5c31\u662f\u4e00\u6761\u6307\u4ee4\u7684\u4f4d\u7f6e\u6216\u64cd\u4f5c\u4f4d\u6570\u4f9d\u8d56\u4e8e\u5176\u4ed6\u4ecd\u5728\u6d41\u6c34\u7ebf\u4e2d\u7684\u6307\u4ee4\u3002 \u865a\u62df\u5730\u5740\uff1a\u5f15\u7528\u5185\u5b58\u4f4d\u7f6e\uff0c\u786c\u4ef6\u548c\u64cd\u4f5c\u7cfb\u7edf\u8054\u5408\u8d77\u6765\u5c06\u865a\u62df\u5730\u5740\u7ffb\u8bd1\u6210\u5b9e\u9645\u6216\u7269\u7406\u5730\u5740\u3002 \u6307\u4ee4\u7684\u4e0d\u540c\u7684\u9636\u6bb5\uff1a \u53d6\u6307\uff08fetch\uff09\uff1a\u53d6\u5740\u9636\u6bb5\u4ece\u5185\u5b58\u8bfb\u53d6\u6307\u4ee4\u5b57\u8282\uff0c\u5730\u5740\u4e3a\u7a0b\u5e8f\u8ba1\u6570\u5668\u7684\u503c\u3002 \u8bd1\u7801\uff08decode\uff09\uff1a\u8bd1\u7801\u9636\u6bb5\u4ece\u5bc4\u5b58\u5668\u6587\u4ef6\u8bfb\u5165\u6700\u591a\u4e24\u4e2a\u64cd\u4f5c\u6570\u3002 \u6267\u884c\uff08execute\uff09\uff1a\u7b97\u672f\u903b\u8f91\u5355\u5143\u8981\u4e48\u6267\u884c\u6307\u4ee4\u6307\u660e\u7684\u64cd\u4f5c\uff0c\u8ba1\u7b97\u5185\u5b58\u5f15\u7528\u7684\u6709\u6548\u5730\u5740\uff0c\u8981\u4e48\u589e\u52a0\u6216\u8005\u51cf\u5c11\u6808\u6307\u9488\u3002 \u8bbf\u5b58\uff08memory\uff09\uff1a\u8bbf\u5b58\u9636\u6bb5\u53ef\u4ee5\u5c06\u6570\u636e\u5199\u5165\u5185\u5b58\uff0c\u6216\u8005\u4ece\u5185\u5b58\u8bfb\u51fa\u6570\u636e\u3002 \u5199\u56de\uff08write back\uff09\uff1a\u5199\u56de\u9636\u6bb5\u6700\u591a\u53ef\u4ee5\u5199\u4e24\u4e2a\u7ed3\u679c\u5230\u5bc4\u5b58\u5668\u6587\u4ef6\u3002 \u66f4\u65b0PC\uff1a\u5c06PC\u8bbe\u7f6e\u6210\u4e0b\u4e00\u6761\u6307\u4ee4\u7684\u5730\u5740\u3002 \u541e\u5410\u91cf\u548c\u5ef6\u8fdf\uff1a\u6d41\u6c34\u7ebf\u7684\u4e00\u4e2a\u91cd\u8981\u7279\u6027\u5c31\u662f\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u541e\u5410\u91cf\uff08throughput\uff09\uff0c\u4e5f\u5c31\u662f\u5355\u4f4d\u65f6\u95f4\u5185\u6267\u884c\u7684\u6307\u4ee4\u603b\u6570\uff0c\u4e0d\u8fc7\u4e5f\u4f1a\u8f7b\u5fae\u589e\u52a0\u5ef6\u8fdf\uff08latency\uff09\uff0c\u4e5f\u5c31\u662f\u6267\u884c\u4e00\u4e2a\u6307\u4ee4\u6240\u9700\u8981\u7684\u65f6\u95f4\u3002\u6211\u4eec\u4ee5\u6bcf\u79d2\u5343\u5146\u6761\u6307\u4ee4\uff08GIPS\uff09\uff0c\u4e5f\u5c31\u662f\u6bcf\u79d2\u5341\u4ebf\u6761\u6307\u4ee4\uff0c\u4e3a\u5355\u4f4d\u6765\u63cf\u8ff0\u541e\u5410\u91cf\u3002\u4ece\u5934\u5230\u5c3e\u6267\u884c\u4e00\u6761\u6307\u4ee4\u6240\u9700\u8981\u7684\u65f6\u95f4\u79f0\u4e3a\u5ef6\u8fdf\uff08latency\uff09\u3002 \u7a0b\u5e8f\u5256\u6790\uff08profiling\uff09\u8fd0\u884c\u7a0b\u5e8f\u7684\u4e00\u4e2a\u7248\u672c\uff0c\u5176\u4e2d\u63d2\u5165\u4e86\u5de5\u5177\u4ee3\u7801\uff0c\u4ee5\u786e\u5b9a\u7a0b\u5e8f\u7684\u5404\u4e2a\u90e8\u5206\u9700\u8981\u591a\u5c11\u65f6\u95f4\u3002\u5256\u6790\u7684\u4e00\u4e2a\u6709\u5229\u4e4b\u5904\u5728\u4e8e\u53ef\u4ee5\u5728\u73b0\u5b9e\u7684\u57fa\u51c6\u6570\u636e\uff08benchmark data\uff09\u4e0a\u8fd0\u884c\u5b9e\u9645\u7a0b\u5e8f\u7684\u540c\u65f6\uff0c\u8fdb\u884c\u5256\u6790\u3002 CSAPP\u6982\u5ff5\u6574\u7406\uff08\u4e8c\uff09 \u00b6 \u5b58\u50a8\u5668\u7cfb\u7edf\uff08memeory system\uff09\uff1a\u5b58\u50a8\u5668\u7cfb\u7edf\u662f\u4e00\u4e2a\u5177\u6709\u4e0d\u540c\u5bb9\u91cf\u3001\u6210\u672c\u548c\u8bbf\u95ee\u65f6\u95f4\u7684\u5b58\u50a8\u8bbe\u5907\u7684\u5c42\u6b21\u7ed3\u6784\u3002 \u5c40\u90e8\u6027\uff08locality\uff09\uff1a\u5177\u6709\u826f\u597d\u5c40\u90e8\u6027\u7684\u7a0b\u5e8f\u503e\u5411\u4e8e\u8bbf\u95ee\u76f8\u540c\u6216\u662f\u90bb\u8fd1\u7684\u6570\u636e\u9879\u96c6\u5408\u3002\u5177\u6709\u826f\u597d\u5c40\u90e8\u6027\u7684\u7a0b\u5e8f\u6bd4\u5c40\u90e8\u6027\u5dee\u7684\u7a0b\u5e8f\u66f4\u591a\u503e\u5411\u4e8e\u4ece\u5b58\u50a8\u5668\u5c42\u6b21\u7ed3\u6784\u4e2d\u8f83\u9ad8\u5c42\u6b21\u5904\u8bbf\u95ee\u6570\u636e\u9879\uff0c\u56e0\u6b64\u8fd0\u884c\u5730\u66f4\u5feb\u3002 \u65f6\u95f4\u5c40\u90e8\u6027\uff1a\u5728\u4e00\u4e2a\u5177\u6709\u826f\u597d\u65f6\u95f4\u5c40\u90e8\u6027\u7684\u7a0b\u5e8f\u4e2d\uff0c\u88ab\u5f15\u7528\u8fc7\u4e00\u6b21\u7684\u5185\u5b58\u4f4d\u7f6e\u5f88\u53ef\u80fd\u5728\u4e0d\u8fdc\u7684\u5c06\u6765\u518d\u88ab\u591a\u6b21\u5f15\u7528\u3002 \u7a7a\u95f4\u5c40\u90e8\u6027\uff1a\u5728\u4e00\u4e2a\u5177\u6709\u826f\u597d\u7a7a\u95f4\u5c40\u90e8\u6027\u7684\u7a0b\u5e8f\u4e2d\uff0c\u5982\u679c\u4e00\u4e2a\u5185\u5b58\u4f4d\u7f6e\u88ab\u5f15\u7528\u4e86\u4e00\u6b21\uff0c\u90a3\u4e48\u7a0b\u5e8f\u5f88\u53ef\u80fd\u5728\u4e0d\u8fdc\u7684\u5c06\u6765\u5f15\u7528\u9644\u8fd1\u7684\u4e00\u4e2a\u5185\u5b58\u4f4d\u7f6e\u3002 \u968f\u673a\u8bbf\u95ee\u5b58\u50a8\u5668\uff08RAM\uff09\uff1a \u9759\u6001RAM\uff08SRAM\uff09\uff1a\u7528\u6765\u4f5c\u4e3a\u9ad8\u901f\u7f13\u5b58\u5b58\u50a8\u5668 \u52a8\u6001RAM\uff08DRAM\uff09\uff1a\u7528\u6765\u4f5c\u4e3a\u4e3b\u5b58\u4ee5\u53ca\u56fe\u5f62\u7cfb\u7edf\u7684\u5e27\u7f13\u51b2\u533a \u975e\u6613\u5931\u6027\u5b58\u50a8\u5668\uff08Nonvolatile memory\uff0cNVM\uff09\uff1a\u5982\u679c\u65ad\u7535\uff0cSRAM\u548cDRAM\u4f1a\u4e22\u5931\u5b83\u4eec\u7684\u4fe1\u606f\uff0c\u5b83\u4eec\u662f\u6613\u5931\u7684\u3002\u975e\u6613\u5931\u6027\u5b58\u50a8\u5668\u5173\u7535\u540e\u4ecd\u7136\u4fdd\u6301\u4fe1\u606f\uff0c\u5305\u62ec\u53ea\u8bfb\u5b58\u50a8\u5668\uff08ROM\uff09\u3002 PROM\uff1a\u53ea\u80fd\u88ab\u7f16\u7a0b\u4e00\u6b21 EPROM\uff1a\u53ef\u7f16\u7a0b100\u6b21 EEPROM\uff1a\u53ef\u7f16\u7a0b 10^5 10^5 \u6b21 \u95ea\u5b58\uff1a\u57fa\u4e8eEEPROM \u56fa\u6001\u786c\u76d8\uff08SSD\uff09\uff1a\u57fa\u4e8e\u95ea\u5b58\u7684\u78c1\u76d8\u9a71\u52a8\u5668 \u5b58\u50a8\u5668\u5c42\u6b21\u7ed3\u6784\u7684\u4e2d\u5fc3\u601d\u60f3\u662f\uff1a\u5bf9\u4e8e\u6bcf\u4e2ak\uff0c\u4f4d\u4e8ek\u5c42\u7684\u66f4\u5feb\u66f4\u5c0f\u7684\u5b58\u50a8\u8bbe\u5907\u4f5c\u4e3a\u4f4d\u4e8ek+1\u5c42\u7684\u66f4\u5927\u66f4\u6162\u7684\u5b58\u50a8\u8bbe\u5907\u7684\u7f13\u5b58\u3002\u7b2ck+1\u5c42\u7684\u5b58\u50a8\u5668\u88ab\u5212\u5206\u6210\u8fde\u7eed\u7684\u6570\u636e\u5bf9\u8c61\u7ec4\u5757\uff08block\uff09\uff0c\u6bcf\u4e2a\u5757\u90fd\u6709\u4e00\u4e2a\u552f\u4e00\u7684\u5730\u5740\u6216\u540d\u5b57\u3002\u6570\u636e\u603b\u662f\u4ee5\u5757\u5927\u5c0f\u4e3a\u4f20\u9001\u5355\u5143\u5728\u7b2ck\u5c42\u548ck+1\u5c42\u4e4b\u95f4\u6765\u56de\u590d\u5236\u7684\u3002 \u7f13\u5b58\u547d\u4e2d\uff08cache hit\uff09\uff1a\u5f53\u7a0b\u5e8f\u9700\u8981\u7b2ck+1\u5c42\u7684\u67d0\u4e2a\u6570\u636e\u5bf9\u8c61d\u65f6\uff0c\u5b83\u9996\u5148\u5728\u5f53\u524d\u5b58\u50a8\u5728\u7b2ck\u5c42\u7684\u4e00\u4e2a\u5757\u4e2d\u67e5\u627ed\u3002\u5982\u679cd\u521a\u597d\u7f13\u5b58\u5728\u7b2ck\u5c42\u4e2d\uff0c\u90a3\u4e48\u5c31\u662f\u6211\u4eec\u6240\u8bf4\u7684\u7f13\u5b58\u547d\u4e2d\u3002 \u94fe\u63a5\uff08linking\uff09\uff1a\u94fe\u63a5\u662f\u5c06\u5404\u79cd\u4ee3\u7801\u548c\u6570\u636e\u7247\u6bb5\u6536\u96c6\u5e76\u7ec4\u5408\u6210\u4e00\u4e2a\u5355\u4e00\u6587\u4ef6\u7684\u8fc7\u7a0b\uff0c\u8fd9\u4e2a\u6587\u4ef6\u53ef\u4ee5\u88ab\u52a0\u8f7d\u5230\u5185\u5b58\u5e76\u6267\u884c\u3002\u94fe\u63a5\u53ef\u4ee5\u6267\u884c\u4e8e\u7f16\u8bd1\u65f6\u3001\u52a0\u8f7d\u65f6\u3001\u8fd0\u884c\u65f6\uff0c\u4f7f\u5f97\u5206\u79bb\u7f16\u8bd1\u6210\u4e3a\u53ef\u80fd\u3002 \u63a7\u5236\u6d41\uff08control flow\uff09\uff1a\u7a0b\u5e8f\u8ba1\u6570\u5668\u5047\u8bbe\u4e00\u4e2a\u503c\u7684\u5e8f\u5217 a_0, a_1, ..., a_{n-1} a_0, a_1, ..., a_{n-1} \uff0c\u5176\u4e2d\uff0c a_k a_k \u662f\u67d0\u4e2a\u76f8\u5e94\u7684\u6307\u4ee4 I_k I_k \u7684\u5730\u5740\u3002\u6bcf\u6b21\u4ece a_k a_k \u5230 a_{k+1} a_{k+1} \u7684\u8fc7\u6e21\u79f0\u4e3a\u63a7\u5236\u8f6c\u79fb\u3002\u8fd9\u6837\u7684\u63a7\u5236\u8f6c\u79fb\u5e8f\u5217\u53eb\u505a\u5904\u7406\u5668\u7684\u63a7\u5236\u6d41\u3002 \u5f02\u5e38\u63a7\u5236\u6d41\uff1a\u73b0\u4ee3\u7cfb\u7edf\u901a\u8fc7\u4f7f\u63a7\u5236\u6d41\u53d1\u751f\u7a81\u53d8\u6765\u5bf9\u7cfb\u7edf\u72b6\u6001\u7684\u53d8\u5316\u505a\u51fa\u53cd\u5e94\u3002\u72b6\u6001\u53d8\u5316\u79f0\u4e3a\u4e8b\u4ef6\uff08event\uff09\u3002 \u5f02\u5e38\u8868\uff1a\u5f53\u5904\u7406\u5668\u68c0\u6d4b\u5230\u6709\u4e8b\u4ef6\u53d1\u751f\u65f6\uff0c\u5b83\u5c31\u4f1a\u901a\u8fc7\u4e00\u5f20\u53eb\u505a\u5f02\u5e38\u8868\u7684\u8df3\u8f6c\u8868\uff0c\u8fdb\u884c\u4e00\u4e2a\u95f4\u63a5\u8fc7\u7a0b\u8c03\u7528\uff0c\u5230\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u6765\u5904\u7406\u8fd9\u7c7b\u4e8b\u4ef6\u7684\u64cd\u4f5c\u7cfb\u7edf\u5b50\u7a0b\u5e8f\uff08\u5f02\u5e38\u5904\u7406\u7a0b\u5e8f\uff09\u3002 \u8fdb\u7a0b\uff08Process\uff09\uff1a\u5f02\u5e38\u662f\u5141\u8bb8\u64cd\u4f5c\u7cfb\u7edf\u5185\u6838\u63d0\u4f9b\u8fdb\u7a0b\u6982\u5ff5\u7684\u57fa\u672c\u6784\u9020\u5757\u3002\u8fdb\u7a0b\u63d0\u4f9b\u7ed9\u5e94\u7528\u7a0b\u5e8f\u7684\u5173\u952e\u62bd\u8c61\uff1a \u4e00\u4e2a\u72ec\u7acb\u7684\u903b\u8f91\u63a7\u5236\u6d41\uff0c\u63d0\u4f9b\u72ec\u5360\u5904\u7406\u5668\u7684\u5047\u8c61 \u4e00\u4e2a\u79c1\u6709\u7684\u5730\u5740\u7a7a\u95f4\uff0c\u63d0\u4f9b\u72ec\u5360\u5185\u5b58\u7cfb\u7edf\u7684\u5047\u8c61 \u903b\u8f91\u63a7\u5236\u6d41\uff1a\u7a0b\u5e8f\u8ba1\u6570\u5668\u4e2d\u7684\u6307\u4ee4\u5e8f\u5217\u3002\u8fdb\u7a0b\u662f\u8f6e\u6d41\u4f7f\u7528\u5904\u7406\u5668\u7684\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\u6267\u884c\u5b83\u7684\u6d41\u7684\u4e00\u90e8\u5206\uff0c\u7136\u540e\u88ab\u62a2\u5360\uff08\u6682\u65f6\u6302\u8d77\uff09\uff0c\u7136\u540e\u8f6e\u5230\u5176\u4ed6\u8fdb\u7a0b\u3002 \u5e76\u53d1\u6d41\uff1a\u4e00\u4e2a\u903b\u8f91\u6d41\u7684\u6267\u884c\u5728\u65f6\u95f4\u4e0a\u4e0e\u53e6\u4e00\u4e2a\u91cd\u53e0\uff0c\u79f0\u4e3a\u5e76\u53d1\u6d41\uff0c\u8fd9\u4e24\u4e2a\u6d41\u88ab\u79f0\u4e3a\u5e76\u53d1\uff08concurrency\uff09\u7684\u8fd0\u884c\u3002 \u591a\u4efb\u52a1\uff1a\u4e00\u4e2a\u8fdb\u7a0b\u548c\u5176\u4ed6\u8fdb\u7a0b\u8f6e\u6d41\u8fd0\u884c\u7684\u6982\u5ff5\u79f0\u4e3a\u591a\u4efb\u52a1\uff08multitasking\uff09\u3002\u4e00\u4e2a\u8fdb\u7a0b\u6267\u884c\u5b83\u7684\u63a7\u5236\u6d41\u7684\u4e00\u90e8\u5206\u7684\u6bcf\u4e00\u65f6\u95f4\u6bb5\u53eb\u65f6\u95f4\u7247\uff08time slice\uff09\uff0c\u56e0\u6b64\u591a\u4efb\u52a1\u4e5f\u53eb\u65f6\u95f4\u5206\u7247\uff08time slicing\uff09\u3002 \u5e76\u884c\uff1a\u5982\u679c\u4e24\u4e2a\u6d41\u5e76\u53d1\u5730\u8fd0\u884c\u5728\u4e0d\u540c\u7684\u5904\u7406\u5668\u6838\u6216\u8005\u8ba1\u7b97\u673a\u4e0a\uff0c\u90a3\u4e48\u6211\u4eec\u79f0\u5b83\u4eec\u4e3a\u5e76\u884c\u6d41\uff08parallel flow\uff09\uff0c\u5b83\u4eec\u5e76\u884c\u5730\u8fd0\u884c\uff08runnning in parallel\uff09\u3002 \u4e0a\u4e0b\u6587\u5207\u6362\uff08context switch\uff09\uff1a\u64cd\u4f5c\u7cfb\u7edf\u5185\u6838\u4f7f\u7528\u4e00\u79cd\u79f0\u4e3a\u4e0a\u4e0b\u6587\u5207\u6362\u7684\u8f83\u9ad8\u5c42\u5f62\u5f0f\u7684\u5f02\u5e38\u63a7\u5236\u6d41\u6765\u5b9e\u73b0\u591a\u4efb\u52a1\u3002\u5185\u6838\u4e3a\u6bcf\u4e2a\u8fdb\u7a0b\u7ef4\u6301\u4e00\u4e2a\u4e0a\u4e0b\u6587\u3002 \u8c03\u5ea6\uff08scheduling\uff09\uff1a\u5728\u8fdb\u7a0b\u6267\u884c\u7684\u67d0\u4e9b\u65f6\u523b\uff0c\u5185\u6838\u53ef\u4ee5\u51b3\u5b9a\u62a2\u5360\u5f53\u524d\u8fdb\u7a0b\uff0c\u5e76\u91cd\u65b0\u5f00\u59cb\u4e00\u4e2a\u5148\u524d\u88ab\u62a2\u5360\u4e86\u7684\u8fdb\u7a0b\u3002\u8fd9\u79cd\u51b3\u7b56\u5c31\u53eb\u505a\u8c03\u5ea6\uff08scheduling\uff09\uff0c\u5728\u5185\u6838\u4e2d\u7531\u8c03\u5ea6\u5668\uff08scheduler\uff09\u5b9e\u73b0\u3002 \u865a\u62df\u5185\u5b58\uff08VM\uff09\uff1a\u4e3a\u4e86\u66f4\u52a0\u6709\u6548\u5730\u7ba1\u7406\u5185\u5b58\u5e76\u5c11\u51fa\u9519\uff0c\u73b0\u4ee3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5bf9\u4e3b\u5b58\u7684\u62bd\u8c61\u6982\u5ff5\uff0c\u53eb\u505a\u865a\u62df\u5185\u5b58\uff08VM\uff09\u3002 \u7269\u7406\u5bfb\u5740\uff1a\u8ba1\u7b97\u673a\u7cfb\u7edf\u7684\u4e3b\u5b58\u88ab\u7ec4\u7ec7\u6210\u4e00\u4e2a\u7531M\u4e2a\u8fde\u7eed\u7684\u5b57\u8282\u5927\u5c0f\u5355\u5143\u7ec4\u6210\u7684\u6570\u7ec4\uff0c\u6bcf\u5b57\u8282\u90fd\u6709\u4e00\u4e2a\u552f\u4e00\u7684\u7269\u7406\u5730\u5740\uff08physical address\uff09\uff0cCPU\u8bbf\u95ee\u5185\u5b58\u6700\u81ea\u7136\u7684\u65b9\u5f0f\u5c31\u662f\u7269\u7406\u5bfb\u5740\uff08phsical addressing\uff09\u3002 \u865a\u62df\u5bfb\u5740\uff1aCPU\u901a\u8fc7\u751f\u6210\u4e00\u4e2a\u865a\u62df\u5730\u5740\uff08virtual address\uff09\u6765\u8bbf\u95ee\u4e3b\u5b58\uff0c\u8fd9\u4e2a\u865a\u62df\u5730\u5740\u5728\u88ab\u9001\u5230\u5185\u5b58\u4e4b\u524d\u5148\u8f6c\u6362\u6210\u9002\u5f53\u7684\u7269\u7406\u5730\u5740\u3002\u5c06\u865a\u62df\u5730\u5740\u8f6c\u6362\u4e3a\u7269\u7406\u5730\u5740\u79f0\u4e3a\u5730\u5740\u7ffb\u8bd1\u3002 \u5730\u5740\u7a7a\u95f4\uff1a\u5730\u5740\u7a7a\u95f4\u662f\u4e00\u4e2a\u975e\u8d1f\u6570\u5730\u5740\u7684\u6709\u5e8f\u96c6\u5408\u3002 \u865a\u62df\u9875\uff08virtual page\uff0cVP\uff09\uff1aVM\u7cfb\u7edf\u901a\u8fc7\u5c06\u865a\u62df\u5185\u5b58\u5206\u5272\u4e3a\u79f0\u4e3a\u865a\u62df\u9875\u7684\u5927\u5c0f\u56fa\u5b9a\u7684\u5757\u6765\u5904\u7406\u3002 \u52a8\u6001\u5185\u5b58\u5206\u914d\uff1a\u52a8\u6001\u5185\u5b58\u5206\u914d\u7ef4\u62a4\u7740\u4e00\u4e2a\u8fdb\u7a0b\u7684\u865a\u62df\u5185\u5b58\u533a\u57df\uff0c\u79f0\u4e3a\u5806\uff08heap\uff09\u3002\u5206\u914d\u5668\u5c06\u5806\u89c6\u4e3a\u4e00\u7ec4\u4e0d\u540c\u5927\u5c0f\u7684\u5757\u6765\u7ef4\u62a4\u3002\u5206\u914d\u5668\u6709\u4e24\u79cd\u98ce\u683c\uff1a \u663e\u5f0f\u5206\u914d\u5668\u3002\u8981\u6c42\u5e94\u7528\u663e\u5f0f\u5730\u91ca\u653e\u4efb\u4f55\u5df2\u5206\u914d\u7684\u5757\u3002\u4f8b\u5982\uff0cC\u8bed\u8a00\u7684malloc\u3002 \u9690\u5f0f\u5206\u914d\u5668\u3002\u8981\u6c42\u5206\u914d\u5668\u68c0\u6d4b\u4e00\u4e2a\u5df2\u5206\u914d\u5757\u4f55\u65f6\u4e0d\u518d\u7a0b\u5e8f\u6240\u4f7f\u7528\uff0c\u90a3\u4e48\u5c31\u91ca\u653e\u8fd9\u4e2a\u5757\u3002\u9690\u5f0f\u5206\u914d\u5668\u4e5f\u53eb\u5783\u573e\u6536\u96c6\u5668\uff08garbage collector\uff09\uff0c\u800c\u81ea\u52a8\u91ca\u653e\u672a\u4f7f\u7528\u7684\u5df2\u5206\u914d\u5757\u7684\u8fc7\u7a0b\u53eb\u505a\u5783\u573e\u6536\u96c6\uff08garbage collection\uff09\u3002\u4f8b\u5982\uff0cJava\u4e4b\u7c7b\u7684\u9ad8\u7ea7\u8bed\u8a00\u5c31\u4f9d\u8d56\u5783\u573e\u6536\u96c6\u6765\u91ca\u653e\u5df2\u5206\u914d\u5757\u3002 CSAPP\u6982\u5ff5\u6574\u7406\uff08\u4e09\uff09 \u00b6 \u8f93\u5165/\u8f93\u51fa\uff08I/O\uff09\uff1aI/O\u662f\u5728\u4e3b\u5b58\u548c\u5916\u90e8\u8bbe\u5907\uff08\u4f8b\u5982\u78c1\u76d8\u9a71\u52a8\u5668\u3001\u7ec8\u7aef\u548c\u7f51\u7edc\uff09\u4e4b\u95f4\u590d\u5236\u6570\u636e\u7684\u8fc7\u7a0b\u3002\u8f93\u5165\u64cd\u4f5c\u662f\u4eceI/O\u8bbe\u5907\u590d\u5236\u6570\u636e\u5230\u4e3b\u5b58\uff0c\u800c\u8f93\u51fa\u64cd\u4f5c\u662f\u4ece\u4e3b\u5b58\u590d\u5236\u6570\u636e\u5230I/O\u8bbe\u5907\u3002 \u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u6a21\u578b\uff1a\u6bcf\u4e2a\u7f51\u7edc\u5e94\u7528\u90fd\u662f\u57fa\u4e8e\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u6a21\u578b\u7684\u3002\u4e00\u4e2a\u5e94\u7528\u662f\u7531\u4e00\u4e2a\u670d\u52a1\u5668\u8fdb\u7a0b\u548c\u4e00\u4e2a\u6216\u8005\u591a\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b\u7ec4\u6210\u3002 \u4e8b\u52a1\uff08Transaction\uff09\uff1a\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u6a21\u578b\u4e2d\u7684\u57fa\u672c\u64cd\u4f5c\u662f\u4e8b\u52a1\u3002\u4e00\u4e2a\u4e8b\u52a1\u7531\u4ee5\u4e0b4\u6b65\u7ec4\u6210\uff1a 1\uff09\u5f53\u4e00\u4e2a\u5ba2\u6237\u7aef\u9700\u8981\u670d\u52a1\u65f6\uff0c\u5b83\u5411\u670d\u52a1\u5668\u53d1\u9001\u4e00\u4e2a\u8bf7\u6c42\uff0c\u53d1\u8d77\u4e00\u4e2a\u4e8b\u52a1\u3002 2\uff09\u670d\u52a1\u5668\u6536\u5230\u8bf7\u6c42\u540e\uff0c\u89e3\u91ca\u5b83\uff0c\u5e76\u4ee5\u9002\u5f53\u7684\u65b9\u5f0f\u64cd\u4f5c\u5b83\u7684\u8d44\u6e90\u3002 3\uff09\u670d\u52a1\u5668\u7ed9\u5ba2\u6237\u7aef\u53d1\u9001\u4e00\u4e2a\u54cd\u5e94\uff0c\u5e76\u7b49\u5f85\u4e0b\u4e00\u4e2a\u8bf7\u6c42\u3002 4\uff09\u5ba2\u6237\u7aef\u6536\u5230\u54cd\u5e94\u5e76\u5904\u7406\u5b83\u3002 \u7f51\u7edc\uff1a\u5bf9\u4e3b\u673a\u800c\u8a00\uff0c\u7f51\u7edc\u53ea\u662f\u4e00\u79cdI/O\u8bbe\u5907\uff0c\u662f\u6570\u636e\u6e90\u548c\u6570\u636e\u63a5\u6536\u65b9\u3002 \u534f\u8bae\uff1a\u89e3\u51b3\u4e3b\u673a\u4e4b\u95f4\u7684\u901a\u4fe1\u95ee\u9898\uff0c\u901a\u8fc7\u63a7\u5236\u4e3b\u673a\u548c\u8def\u7531\u5668\u534f\u540c\u5de5\u4f5c\u6765\u5b9e\u73b0\u6570\u636e\u4f20\u8f93\u3002 \u547d\u540d\u673a\u5236\u3002\u5b9a\u4e49\u4e00\u79cd\u4e00\u81f4\u7684\u4e3b\u673a\u5730\u5740\u683c\u5f0f\u3002 \u4f20\u9001\u673a\u5236\u3002\u5b9a\u4e49\u4e00\u79cd\u628a\u6570\u636e\u4f4d\u6346\u624e\u6210\u5305\u7684\u7edf\u4e00\u65b9\u5f0f\u3002\u4e00\u4e2a\u5305\u7531\u5305\u5934\u548c\u6709\u6548\u8f7d\u8377\u7ec4\u6210\u3002\u5176\u4e2d\u5305\u5934\u5305\u62ec\u5305\u7684\u5927\u5c0f\u4ee5\u53ca\u6e90\u4e3b\u673a\u548c\u76ee\u7684\u4e3b\u673a\u7684\u5730\u5740\uff0c\u6709\u6548\u8f7d\u8377\u5305\u62ec\u4ece\u6e90\u4e3b\u673a\u53d1\u51fa\u7684\u6570\u636e\u4f4d\u3002 \u56e0\u7279\u7f51\uff1a \u4e3b\u673a\u88ab\u6620\u5c04\u4e3a\u4e00\u7ec432\u4f4d\u7684IP\u5730\u5740 \u8fd9\u7ec4IP\u5730\u5740\u88ab\u6620\u5c04\u4e3a\u4e00\u7ec4\u79f0\u4e3a\u56e0\u7279\u7f51\u57df\u540d\u7684\u6807\u8bc6\u7b26 \u56e0\u7279\u7f51\u4e3b\u673a\u4e0a\u7684\u8fdb\u7a0b\u80fd\u591f\u901a\u8fc7\u8fde\u63a5\uff08connection\uff09\u548c\u4efb\u4f55\u5176\u4ed6\u56e0\u7279\u7f51\u4e3b\u673a\u4e0a\u7684\u8fdb\u7a0b\u901a\u4fe1 \u7aef\u53e3\uff1a Web\u670d\u52a1\u5668\uff1a Web\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u5668\u4e4b\u95f4\u7684\u4ea4\u4e92\u7528\u7684\u662fHTTP Web\u53ef\u4ee5\u7528\u4e00\u79cd\u53eb\u505aHTML\u7684\u8bed\u8a00\u6765\u7f16\u5199 Web\u670d\u52a1\u5668\u4ee5\u4e24\u79cd\u4e0d\u540c\u7684\u65b9\u5f0f\u5411\u5ba2\u6237\u7aef\u63d0\u4f9b\u5185\u5bb9\uff1a \u53d6\u4e00\u4e2a\u78c1\u76d8\u6587\u4ef6\uff0c\u5e76\u5c06\u5b83\u7684\u5185\u5bb9\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\u3002\u78c1\u76d8\u6587\u4ef6\u79f0\u4e3a\u9759\u6001\u5185\u5bb9\uff0c\u800c\u8fd4\u56de\u6587\u4ef6\u7ed9\u5ba2\u6237\u7aef\u7684\u8fc7\u7a0b\u79f0\u4e3a\u670d\u52a1\u9759\u6001\u5185\u5bb9\u3002 \u8fd0\u884c\u4e00\u4e2a\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u5e76\u5c06\u5b83\u7684\u8f93\u51fa\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\u3002\u8fd0\u884c\u65f6\u53ef\u6267\u884c\u6587\u4ef6\u7684\u8f93\u51fa\u79f0\u4e3a\u52a8\u6001\u5185\u5bb9\uff0c\u800c\u8fd0\u884c\u7a0b\u5e8f\u8fd4\u56de\u5b83\u7684\u8f93\u51fa\u5230\u5ba2\u6237\u7aef\u7684\u8fc7\u7a0b\u79f0\u4e3a\u670d\u52a1\u52a8\u6001\u5185\u5bb9\u3002 \u5e94\u7528\u7ea7\u5e76\u53d1\uff1a \u8bbf\u95ee\u6162\u901fI/O\u8bbe\u5907 \u4e0e\u4eba\u4ea4\u4e92 \u901a\u8fc7\u63a8\u8fdf\u5de5\u4f5c\u4ee5\u964d\u4f4e\u5ef6\u8fdf \u670d\u52a1\u591a\u4e2a\u7f51\u7edc\u5ba2\u6237\u7aef \u5728\u591a\u6838\u673a\u5668\u4e0a\u8fdb\u884c\u5e76\u884c\u8ba1\u7b97 \u5e76\u53d1\u7a0b\u5e8f\uff1a\u4f7f\u7528\u5e94\u7528\u7ea7\u5e76\u53d1\u7684\u5e94\u7528\u7a0b\u5e8f\u79f0\u4e3a\u5e76\u53d1\u7a0b\u5e8f\uff0c\u73b0\u4ee3\u64cd\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e09\u79cd\u57fa\u672c\u7684\u6784\u9020\u5e76\u53d1\u7a0b\u5e8f\u7684\u65b9\u6cd5\uff1a \u8fdb\u7a0b I/O\u591a\u8def\u590d\u7528 \u7ebf\u7a0b","title":"CSAPP\u6982\u5ff5\u6574\u7406\uff08\u4e00\uff09"},{"location":"csapp-00/#csapp","text":"\u7f16\u8bd1\u7cfb\u7edf\uff1a\u9884\u5904\u7406\u5668\u3001\u7f16\u8bd1\u5668\u3001\u6c47\u7f16\u5668\u3001\u94fe\u63a5\u5668\u4e00\u8d77\u6784\u6210\u4e86\u7f16\u8bd1\u7cfb\u7edf\u3002 GNU/Linux\uff1aGNU\u73af\u5883+Linux\u5185\u6838 \u7cfb\u7edf\u7684\u786c\u4ef6\u7ec4\u6210\uff1a \u603b\u7ebf\uff1a\u643a\u5e26\u5b57\u8282\u4fe1\u606f\u5e76\u8d1f\u8d23\u5728\u5404\u4e2a\u90e8\u4ef6\u95f4\u4f20\u9012\u3002\u4f20\u9001\u5b9a\u957f\u7684\u5b57\u8282\u5757\uff0c\u4e5f\u5c31\u662f\u5b57\uff0c\u5b57\u8282\u6570\u5373\u5b57\u957f\u662f\u4e00\u4e2a\u57fa\u672c\u7684\u7cfb\u7edf\u53c2\u6570\uff0c\u5927\u591a\u6570\u673a\u5668\u662f4\u4e2a\u5b57\u8282\uff0832\u4f4d\uff09\u62168\u4e2a\u5b57\u8282\uff0864\u4f4d\uff09\u3002 I/O\u8bbe\u5907\uff1a\u952e\u76d8\u3001\u9f20\u6807\u3001\u663e\u793a\u5668\u3001\u78c1\u76d8\u3002\u6bcf\u4e2aI/O\u8bbe\u5907\u90fd\u901a\u8fc7\u4e00\u4e2a\u63a7\u5236\u5668\u6216\u9002\u914d\u5668\u4e0eI/O\u603b\u7ebf\u76f8\u8fde\u3002 \u4e3b\u5b58\uff08\u5185\u5b58\uff09\uff1a\u4e3b\u5b58\u662f\u4e00\u4e2a\u4e34\u65f6\u5b58\u50a8\u8bbe\u5907\uff0c\u7531\u4e00\u7ec4\u52a8\u6001\u968f\u673a\u5b58\u53d6\u5b58\u50a8\u5668\uff08DRAM\uff09\u82af\u7247\u7ec4\u6210\u7684\u3002\u5b58\u50a8\u5668\u662f\u4e00\u4e2a\u7ebf\u6027\u7684\u5b57\u8282\u6570\u7ec4\uff0c\u6bcf\u4e2a\u5b57\u8282\u90fd\u6709\u5176\u552f\u4e00\u7684\u5730\u5740\uff08\u6570\u7ec4\u7d22\u5f15\uff09\uff0c\u7ec4\u6210\u7a0b\u5e8f\u7684\u6bcf\u6761\u673a\u5668\u6307\u4ee4\u90fd\u7531\u4e0d\u540c\u6570\u91cf\u7684\u5b57\u8282\u6784\u6210\u3002 \u5904\u7406\u5668\uff08CPU\uff09\uff1a\u89e3\u91ca\u6216\u6267\u884c\u5b58\u50a8\u5728\u4e3b\u5b58\u4e2d\u7684\u6307\u4ee4\u3002\u5904\u7406\u5668\u4e00\u76f4\u5728\u4e0d\u65ad\u5730\u4ecePC\u6307\u5411\u7684\u5185\u5b58\u5904\u8bfb\u53d6\u6307\u4ee4\uff0c\u6267\u884c\u6307\u4ee4\u7684\u64cd\u4f5c\uff0c\u518d\u66f4\u65b0PC\uff0c\u4f7f\u5176\u6307\u5411\u4e0b\u4e00\u6761\u6307\u4ee4\u3002 \u7a0b\u5e8f\u8ba1\u6570\u5668\uff08PC\uff09\uff1aPC\u6307\u5411\u4e3b\u5b58\u4e2d\u67d0\u6761\u673a\u5668\u8bed\u8a00\u6307\u4ee4\uff08\u5373\u542b\u6709\u8be5\u6761\u6307\u4ee4\u7684\u5730\u5740\uff09\u3002 \u7b97\u672f\u903b\u8f91\u5355\u5143\uff08ALU\uff09\uff1a\u8ba1\u7b97\u65b0\u7684\u6570\u636e\u548c\u5730\u5740\u503c\u3002 \u5bc4\u5b58\u5668\u6587\u4ef6\uff08Register File\uff09\uff1a\u8d1f\u8d23\u5b58\u50a8\u3002 \u6307\u4ee4\u96c6\u67b6\u6784\uff08ISA\uff09\uff1a\u5904\u7406\u5668\u6309\u7167\u4e00\u4e2a\u6307\u4ee4\u6267\u884c\u6a21\u578b\u6765\u64cd\u4f5c\uff0c\u8fd9\u4e2a\u6a21\u578b\u7531\u6307\u4ee4\u96c6\u67b6\u6784\u51b3\u5b9a\u3002\u6307\u4ee4\u96c6\u67b6\u6784\u63cf\u8ff0\u7684\u662f\u6bcf\u6761\u673a\u5668\u4ee3\u7801\u6307\u4ee4\u7684\u6548\u679c\uff1b\u800c\u5fae\u4f53\u7cfb\u7ed3\u6784\u63cf\u8ff0\u7684\u662f\u5904\u7406\u5668\u5b9e\u9645\u4e0a\u5982\u4f55\u5b9e\u73b0\u3002 cache\uff1a\u9ad8\u901f\u7f13\u5b58\u5b58\u50a8\u5668\uff08cache memory\uff09\u3002 SRAM\uff1a\u9759\u6001\u968f\u673a\u8bbf\u95ee\u5b58\u50a8\u5668\u3002 \u5b58\u50a8\u5668\u5c42\u6b21\u7ed3\u6784\uff1a L0\uff1a\u5bc4\u5b58\u5668\u3002CPU\u5bc4\u5b58\u5668\u4fdd\u5b58\u6765\u81ea\u9ad8\u901f\u7f13\u5b58\u7684\u5b57\u3002 L1\uff1aL1\u9ad8\u901f\u7f13\u5b58\uff08SRAM\uff09\u3002L1\u9ad8\u901f\u7f13\u5b58\u4fdd\u5b58\u53d6\u81eaL2\u9ad8\u901f\u7f13\u5b58\u7684\u9ad8\u901f\u7f13\u5b58\u884c\u3002 L2\uff1aL2\u9ad8\u901f\u7f13\u5b58\uff08SRAM\uff09\u3002L2\u9ad8\u901f\u7f13\u5b58\u4fdd\u5b58\u53d6\u81eaL3\u9ad8\u901f\u7f13\u5b58\u7684\u9ad8\u901f\u7f13\u5b58\u884c\u3002 L3\uff1aL3\u9ad8\u901f\u7f13\u5b58\uff08SRAM\uff09\u3002L3\u9ad8\u901f\u7f13\u5b58\u4fdd\u5b58\u53d6\u81ea\u4e3b\u5b58\u7684\u9ad8\u901f\u7f13\u5b58\u884c\u3002 L4\uff1a\u4e3b\u5b58\uff08DRAM\uff09\u3002\u4e3b\u5b58\u4fdd\u5b58\u53d6\u81ea\u672c\u5730\u78c1\u76d8\u7684\u78c1\u76d8\u5757\u3002 L5\uff1a\u672c\u5730\u4e8c\u7ea7\u5b58\u50a8\uff08\u672c\u5730\u78c1\u76d8\uff09\u3002\u672c\u5730\u78c1\u76d8\u4fdd\u5b58\u53d6\u81ea\u8fdc\u7a0b\u7f51\u7edc\u670d\u52a1\u5668\u4e0a\u78c1\u76d8\u7684\u6587\u4ef6\u3002 L6\uff1a\u8fdc\u7a0b\u4e8c\u7ea7\u5b58\u50a8\uff08\u5206\u5e03\u5f0f\u6587\u4ef6\u7cfb\u7edf\u3001web\u670d\u52a1\u5668\uff09\u3002 \u64cd\u4f5c\u7cfb\u7edf\uff1a\u5e94\u7528\u7a0b\u5e8f\uff08\u8f6f\u4ef6\uff09\u548c\u786c\u4ef6\u4e4b\u95f4\u7684\u4e00\u5c42\u8f6f\u4ef6\uff0c\u6240\u6709\u5e94\u7528\u7a0b\u5e8f\u5bf9\u786c\u4ef6\u7684\u64cd\u4f5c\u90fd\u5fc5\u987b\u901a\u8fc7\u64cd\u4f5c\u7cfb\u7edf\u3002\u64cd\u4f5c\u7cfb\u7edf\u901a\u8fc7\u51e0\u4e2a\u62bd\u8c61\u7684\u57fa\u672c\u6982\u5ff5\uff08\u8fdb\u7a0b\u3001\u865a\u62df\u5185\u5b58\u3001\u6587\u4ef6\uff09\u6765\u5b9e\u73b0\u3002 \u6587\u4ef6\u662fI/O\u8bbe\u5907\u7684\u62bd\u8c61\u3002 \u865a\u62df\u5185\u5b58\u662f\u4e3b\u5b58\u548cI/O\u8bbe\u5907\u7684\u62bd\u8c61\u3002 \u8fdb\u7a0b\u662f\u5904\u7406\u5668\u3001\u4e3b\u5b58\u548cI/O\u8bbe\u5907\u7684\u62bd\u8c61\u3002 \u8fdb\u7a0b\uff08Process\uff09\uff1a\u8fdb\u7a0b\u662f\u64cd\u4f5c\u7cfb\u7edf\u5bf9\u4e00\u4e2a\u6b63\u5728\u8fd0\u884c\u7684\u7a0b\u5e8f\u7684\u4e00\u79cd\u62bd\u8c61\u3002 \u5e76\u53d1\uff08Concurrency\uff09\uff1a\u4e00\u4e2a\u8fdb\u7a0b\u7684\u6307\u4ee4\u548c\u53e6\u4e00\u4e2a\u8fdb\u7a0b\u7684\u6307\u4ee4\u4ea4\u9519\u6267\u884c\u3002 \u4e0a\u4e0b\u6587\u5207\u6362\uff08Context Switch\uff09\uff1a\u8fd9\u79cd\u4ea4\u9519\u6267\u884c\u7684\u673a\u5236\u6210\u4e3a\u4e0a\u4e0b\u6587\u5207\u6362\u3002\u5f53\u64cd\u4f5c\u7cfb\u7edf\u51b3\u5b9a\u628a\u63a7\u5236\u6743\u4ece\u5f53\u524d\u8fdb\u7a0b\u8f6c\u79fb\u5230\u67d0\u4e2a\u65b0\u8fdb\u7a0b\u65f6\uff0c\u5c31\u4f1a\u4e0a\u4e0b\u6587\u5207\u6362\uff0c\u5373\u4fdd\u6301\u5f53\u524d\u8fdb\u7a0b\u7684\u4e0a\u4e0b\u6587\uff0c\u6062\u590d\u65b0\u8fdb\u7a0b\u7684\u4e0a\u4e0b\u6587\uff0c\u5c06\u63a7\u5236\u6743\u4f20\u9012\u7ed9\u65b0\u8fdb\u7a0b\u3002 \u4e0a\u4e0b\u6587\uff1a\u64cd\u4f5c\u7cfb\u7edf\u4fdd\u6301\u8ddf\u8e2a\u8fdb\u7a0b\u8fd0\u884c\u6240\u9700\u7684\u6240\u6709\u72b6\u6001\u4fe1\u606f\u3002 \u5185\u6838\uff1a\u4ece\u4e00\u4e2a\u8fdb\u7a0b\u5230\u53e6\u4e00\u4e2a\u8fdb\u7a0b\u7684\u8f6c\u6362\u662f\u7531\u64cd\u4f5c\u7cfb\u7edf\u5185\u6838\u7ba1\u7406\u7684\u3002\u5185\u6838\u662f\u64cd\u4f5c\u7cfb\u7edf\u5e38\u9a7b\u4e3b\u5b58\u7684\u90e8\u5206\u3002\u5f53\u5e94\u7528\u7a0b\u5e8f\u9700\u8981\u64cd\u4f5c\u7cfb\u7edf\u7684\u67d0\u4e9b\u64cd\u4f5c\u65f6\uff0c\u5b83\u5c31\u6267\u884c\u4e00\u6761\u7cfb\u7edf\u8c03\u7528\u6307\u4ee4\uff0c\u5c06\u63a7\u5236\u6743\u4f20\u9012\u7ed9\u5185\u6838\uff0c\u7136\u540e\u5185\u6838\u6267\u884c\u8bf7\u6c42\u7684\u64cd\u4f5c\u5e76\u8fd4\u56de\u5e94\u7528\u7a0b\u5e8f\u3002\u5185\u6838\u4e0d\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u8fdb\u7a0b\uff0c\u800c\u662f\u7cfb\u7edf\u7ba1\u7406\u5168\u90e8\u8fdb\u7a0b\u6240\u7528\u4ee3\u7801\u548c\u6570\u636e\u7ed3\u6784\u7684\u96c6\u5408\u3002 \u7ebf\u7a0b\uff08Thread\uff09\uff1a\u4e00\u4e2a\u8fdb\u7a0b\u5b9e\u9645\u4e0a\u53ef\u4ee5\u7531\u591a\u4e2a\u6210\u4e3a\u7ebf\u7a0b\u7684\u6267\u884c\u5355\u5143\u7ec4\u6210\uff0c\u6bcf\u4e2a\u7ebf\u7a0b\u90fd\u8fd0\u884c\u5728\u8fdb\u7a0b\u7684\u4e0a\u4e0b\u6587\u4e2d\uff0c\u5171\u4eab\u540c\u6837\u7684\u4ee3\u7801\u548c\u5168\u5c40\u6570\u636e\u3002 \u865a\u62df\u5185\u5b58\uff1a\u4e3a\u6bcf\u4e2a\u8fdb\u7a0b\u63d0\u4f9b\u4e00\u4e2a\u5047\u8c61\uff0c\u5373\u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u5728\u72ec\u5360\u4f7f\u7528\u4e3b\u5b58\u3002\u6bcf\u4e2a\u8fdb\u7a0b\u770b\u5230\u7684\u5185\u5b58\u90fd\u662f\u4e00\u81f4\u7684\uff0c\u6210\u4e3a\u865a\u62df\u5730\u5740\u7a7a\u95f4\u3002\u6bcf\u4e2a\u8fdb\u7a0b\u770b\u5230\u7684\u865a\u62df\u5730\u5740\u7a7a\u95f4\u7531\u5927\u91cf\u51c6\u786e\u5b9a\u4e49\u7684\u533a\u6784\u6210\uff0c\u6bcf\u4e2a\u533a\u7531\u4e13\u95e8\u7684\u529f\u80fd\u3002 \u6587\u4ef6\uff1a\u6587\u4ef6\u5c31\u662f\u5b57\u8282\u5e8f\u5217\u3002 \u7f51\u7edc\uff1a\u73b0\u4ee3\u7cfb\u7edf\u901a\u8fc7\u7f51\u7edc\u4e0e\u5176\u4ed6\u7cfb\u7edf\u8fde\u63a5\u5230\u4e00\u8d77\uff0c\u662f\u8ba1\u7b97\u673a\u7cfb\u7edf\u4e4b\u95f4\u901a\u4fe1\u7684\u624b\u6bb5\u3002\u7f51\u7edc\u53ef\u4ee5\u89c6\u4e3a\u4e00\u4e2aI/O\u8bbe\u5907\uff0c\u7cfb\u7edf\u4ece\u7f51\u7edc\u9002\u914d\u5668\u53d1\u9001\u548c\u8bfb\u53d6\u5176\u4ed6\u673a\u5668\u7684\u6570\u636e\u3002 \u5e76\u53d1\u548c\u5e76\u884c\uff1a\u5e76\u53d1\u662f\u4e00\u4e2a\u901a\u7528\u6982\u5ff5\uff0c\u6307\u4e00\u4e2a\u540c\u65f6\u5177\u6709\u591a\u4e2a\u6d3b\u52a8\u7684\u7cfb\u7edf\uff0c\u5e76\u53d1\u53ef\u4ee5\u4f7f\u7cfb\u7edf\u540c\u65f6\u505a\u66f4\u66f4\u591a\u4e8b\u60c5\uff08\u4e00\u65b9\u9762\u505a\u5f97\u66f4\u591a\uff0c\u4e00\u65b9\u9762\u8fd0\u884c\u5f97\u66f4\u5feb\u3002\uff09\uff1b\u800c\u5e76\u884c\uff08parallelism\uff09\u6307\u7684\u662f\u7528\u5e76\u53d1\u6765\u4f7f\u4e00\u4e2a\u7cfb\u7edf\u8fd0\u884c\u5730\u66f4\u5feb\u3002\u5e76\u884c\u53ef\u4ee5\u5728\u8ba1\u7b97\u673a\u7cfb\u7edf\u7684\u591a\u4e2a\u62bd\u8c61\u5c42\u6b21\u4e0a\u8fd0\u7528\uff1a \u7ebf\u7a0b\u7ea7\u5e76\u53d1\uff1a \u5355\u6838\u5904\u7406\u5668\uff1a\u5e76\u53d1\u662f\u6a21\u62df\u51fa\u6765\u7684\uff0c\u53ea\u662f\u901a\u8fc7\u8fdb\u7a0b\u95f4\u7684\u5feb\u901f\u5207\u6362\u5b9e\u73b0\u3002 \u591a\u6838\u5904\u7406\u5668\uff1a\u5c06\u591a\u4e2aCPU\u96c6\u6210\u5728\u4e00\u4e2a\u96c6\u6210\u7535\u8def\u82af\u7247\u4e0a\u3002 \u8d85\u7ebf\u7a0b/\u540c\u65f6\u591a\u7ebf\u7a0b\uff1a\u5e38\u89c4\u5904\u7406\u5668\u9700\u8981\u5927\u7ea620000\u4e2a\u65f6\u949f\u5468\u671f\u505a\u7ebf\u7a0b\u95f4\u5207\u6362\uff0c\u800c\u8d85\u7ebf\u7a0b\u7684\u5904\u7406\u5668\u53ef\u4ee5\u5728\u5355\u4e2a\u5468\u671f\u7684\u57fa\u7840\u4e0a\u51b3\u5b9a\u7ebf\u7a0b\u5207\u6362\u3002\u8fd9\u5fc5\u987b\u8981\u6c42\u7a0b\u5e8f\u662f\u4ee5\u591a\u7ebf\u7a0b\u65b9\u5f0f\u6765\u4e66\u5199\u7684\u3002 \u6307\u4ee4\u7ea7\u5e76\u884c\uff1a \u73b0\u4ee3\u5904\u7406\u5668\u53ef\u4ee5\u540c\u65f6\u6267\u884c\u591a\u6761\u6307\u4ee4\u7684\u5c5e\u6027\u79f0\u4e3a\u6307\u4ee4\u7ea7\u5e76\u884c\u3002 \u6d41\u6c34\u7ebf\uff08pipelining\uff09\uff1a\u5728\u6d41\u6c34\u7ebf\u4e2d\uff0c\u5c06\u6267\u884c\u4e00\u6761\u6307\u4ee4\u6240\u9700\u8981\u7684\u6d3b\u52a8\u5212\u5206\u6210\u4e0d\u540c\u7684\u6b65\u9aa4\uff0c\u5c06\u5904\u7406\u5668\u7684\u786c\u4ef6\u7ec4\u7ec7\u6210\u4e00\u7cfb\u5217\u7684\u9636\u6bb5\uff0c\u6bcf\u4e2a\u9636\u6bb5\u6267\u884c\u4e00\u4e2a\u6b65\u9aa4\uff0c\u8fd9\u4e9b\u9636\u6bb5\u53ef\u4ee5\u5e76\u884c\u64cd\u4f5c\uff0c\u7528\u6765\u5904\u7406\u4e0d\u540c\u6307\u4ee4\u7684\u4e0d\u540c\u90e8\u5206\u3002 \u8d85\u6807\u91cf\uff08superscalar\uff09\uff1a\u5982\u679c\u5904\u7406\u5668\u53ef\u4ee5\u8fbe\u5230\u6bd4\u4e00\u4e2a\u5468\u671f\u4e00\u6761\u6307\u4ee4\u66f4\u5feb\u7684\u6267\u884c\u901f\u7387\uff0c\u5c31\u79f0\u4e4b\u4e3a\u8d85\u6807\u91cf\u5904\u7406\u5668\u3002 \u5355\u6307\u4ee4 \u3001\u591a\u6570\u636e\u5e76\u884c\uff1a \u8bb8\u591a\u73b0\u4ee3\u5904\u7406\u5668\u62e5\u6709\u7279\u6b8a\u7684\u786c\u4ef6\uff0c\u5141\u8bb8\u4e00\u6761\u6307\u4ee4\u4ea7\u751f\u591a\u4e2a\u53ef\u4ee5\u5e76\u884c\u6267\u884c\u7684\u64cd\u4f5c\uff0c\u79f0\u4e3a\u5355\u6307\u4ee4\u3001\u591a\u6570\u636e\uff0c\u5373SIMD\u5e76\u884c\u3002 \u865a\u62df\u673a\uff1a\u5bf9\u6574\u4e2a\u8ba1\u7b97\u673a\u7684\u62bd\u8c61\uff0c\u5305\u62ec\u64cd\u4f5c\u7cfb\u7edf\u3001\u5904\u7406\u5668\u548c\u7a0b\u5e8f\u3002 \u6307\u4ee4\u96c6\u4f53\u7cfb\u7ed3\u6784\uff08ISA\uff09\uff1a\u6307\u4ee4\u88ab\u7f16\u7801\u4e3a\u7531\u4e00\u4e2a\u6216\u591a\u4e2a\u5b57\u8282\u5e8f\u5217\u7ec4\u6210\u7684\u4e8c\u8fdb\u5236\u683c\u5f0f\u3002\u4e00\u4e2a\u5904\u7406\u5668\u652f\u6301\u7684\u6307\u4ee4\u548c\u6307\u4ee4\u7684\u5b57\u8282\u7ea7\u7f16\u7801\u79f0\u4e3a\u5b83\u7684\u6307\u4ee4\u96c6\u4f53\u7cfb\u7ed3\u6784\u3002ISA\u5728\u7f16\u8bd1\u5668\u7f16\u5199\u8005\u548c\u5904\u7406\u5668\u8bbe\u8ba1\u4eba\u5458\u4e4b\u95f4\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6982\u5ff5\u62bd\u8c61\u5c42\uff0c\u7f16\u8bd1\u5668\u7f16\u5199\u8005\u53ea\u9700\u8981\u77e5\u9053\u5141\u8bb8\u54ea\u4e9b\u6307\u4ee4\u4ee5\u53ca\u5b83\u4eec\u662f\u5982\u4f55\u7f16\u7801\u7684\uff1b\u800c\u5904\u7406\u5668\u8bbe\u8ba1\u8005\u5fc5\u987b\u5efa\u9020\u51fa\u6267\u884c\u8fd9\u4e9b\u6307\u4ee4\u7684\u5904\u7406\u5668\u3002 HCL\uff08\u786c\u4ef6\u63a7\u5236\u8bed\u8a00\uff09\uff1a\u7528\u6765\u63cf\u8ff0\u5904\u7406\u5668\u7684\u8bbe\u8ba1\u3002 \u6d41\u6c34\u7ebf\u5316\u7684\u5904\u7406\u5668\uff1a\u5c06\u6bcf\u6761\u6307\u4ee4\u7684\u6267\u884c\u5206\u62105\u6b65\uff0c\u6bcf\u4e2a\u6b65\u9aa4\u7531\u4e00\u4e2a\u72ec\u7acb\u7684\u786c\u4ef6\u6216\u9636\u6bb5\u6765\u5904\u7406\u3002\u5904\u7406\u5668\u53ef\u4ee5\u540c\u65f6\u6267\u884c5\u6761\u6307\u4ee4\u7684\u4e0d\u540c\u9636\u6bb5\u3002 \u5192\u9669\u6216\u51b2\u7a81\uff1a\u5192\u9669\u5c31\u662f\u4e00\u6761\u6307\u4ee4\u7684\u4f4d\u7f6e\u6216\u64cd\u4f5c\u4f4d\u6570\u4f9d\u8d56\u4e8e\u5176\u4ed6\u4ecd\u5728\u6d41\u6c34\u7ebf\u4e2d\u7684\u6307\u4ee4\u3002 \u865a\u62df\u5730\u5740\uff1a\u5f15\u7528\u5185\u5b58\u4f4d\u7f6e\uff0c\u786c\u4ef6\u548c\u64cd\u4f5c\u7cfb\u7edf\u8054\u5408\u8d77\u6765\u5c06\u865a\u62df\u5730\u5740\u7ffb\u8bd1\u6210\u5b9e\u9645\u6216\u7269\u7406\u5730\u5740\u3002 \u6307\u4ee4\u7684\u4e0d\u540c\u7684\u9636\u6bb5\uff1a \u53d6\u6307\uff08fetch\uff09\uff1a\u53d6\u5740\u9636\u6bb5\u4ece\u5185\u5b58\u8bfb\u53d6\u6307\u4ee4\u5b57\u8282\uff0c\u5730\u5740\u4e3a\u7a0b\u5e8f\u8ba1\u6570\u5668\u7684\u503c\u3002 \u8bd1\u7801\uff08decode\uff09\uff1a\u8bd1\u7801\u9636\u6bb5\u4ece\u5bc4\u5b58\u5668\u6587\u4ef6\u8bfb\u5165\u6700\u591a\u4e24\u4e2a\u64cd\u4f5c\u6570\u3002 \u6267\u884c\uff08execute\uff09\uff1a\u7b97\u672f\u903b\u8f91\u5355\u5143\u8981\u4e48\u6267\u884c\u6307\u4ee4\u6307\u660e\u7684\u64cd\u4f5c\uff0c\u8ba1\u7b97\u5185\u5b58\u5f15\u7528\u7684\u6709\u6548\u5730\u5740\uff0c\u8981\u4e48\u589e\u52a0\u6216\u8005\u51cf\u5c11\u6808\u6307\u9488\u3002 \u8bbf\u5b58\uff08memory\uff09\uff1a\u8bbf\u5b58\u9636\u6bb5\u53ef\u4ee5\u5c06\u6570\u636e\u5199\u5165\u5185\u5b58\uff0c\u6216\u8005\u4ece\u5185\u5b58\u8bfb\u51fa\u6570\u636e\u3002 \u5199\u56de\uff08write back\uff09\uff1a\u5199\u56de\u9636\u6bb5\u6700\u591a\u53ef\u4ee5\u5199\u4e24\u4e2a\u7ed3\u679c\u5230\u5bc4\u5b58\u5668\u6587\u4ef6\u3002 \u66f4\u65b0PC\uff1a\u5c06PC\u8bbe\u7f6e\u6210\u4e0b\u4e00\u6761\u6307\u4ee4\u7684\u5730\u5740\u3002 \u541e\u5410\u91cf\u548c\u5ef6\u8fdf\uff1a\u6d41\u6c34\u7ebf\u7684\u4e00\u4e2a\u91cd\u8981\u7279\u6027\u5c31\u662f\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u541e\u5410\u91cf\uff08throughput\uff09\uff0c\u4e5f\u5c31\u662f\u5355\u4f4d\u65f6\u95f4\u5185\u6267\u884c\u7684\u6307\u4ee4\u603b\u6570\uff0c\u4e0d\u8fc7\u4e5f\u4f1a\u8f7b\u5fae\u589e\u52a0\u5ef6\u8fdf\uff08latency\uff09\uff0c\u4e5f\u5c31\u662f\u6267\u884c\u4e00\u4e2a\u6307\u4ee4\u6240\u9700\u8981\u7684\u65f6\u95f4\u3002\u6211\u4eec\u4ee5\u6bcf\u79d2\u5343\u5146\u6761\u6307\u4ee4\uff08GIPS\uff09\uff0c\u4e5f\u5c31\u662f\u6bcf\u79d2\u5341\u4ebf\u6761\u6307\u4ee4\uff0c\u4e3a\u5355\u4f4d\u6765\u63cf\u8ff0\u541e\u5410\u91cf\u3002\u4ece\u5934\u5230\u5c3e\u6267\u884c\u4e00\u6761\u6307\u4ee4\u6240\u9700\u8981\u7684\u65f6\u95f4\u79f0\u4e3a\u5ef6\u8fdf\uff08latency\uff09\u3002 \u7a0b\u5e8f\u5256\u6790\uff08profiling\uff09\u8fd0\u884c\u7a0b\u5e8f\u7684\u4e00\u4e2a\u7248\u672c\uff0c\u5176\u4e2d\u63d2\u5165\u4e86\u5de5\u5177\u4ee3\u7801\uff0c\u4ee5\u786e\u5b9a\u7a0b\u5e8f\u7684\u5404\u4e2a\u90e8\u5206\u9700\u8981\u591a\u5c11\u65f6\u95f4\u3002\u5256\u6790\u7684\u4e00\u4e2a\u6709\u5229\u4e4b\u5904\u5728\u4e8e\u53ef\u4ee5\u5728\u73b0\u5b9e\u7684\u57fa\u51c6\u6570\u636e\uff08benchmark data\uff09\u4e0a\u8fd0\u884c\u5b9e\u9645\u7a0b\u5e8f\u7684\u540c\u65f6\uff0c\u8fdb\u884c\u5256\u6790\u3002","title":"CSAPP\u6982\u5ff5\u6574\u7406\uff08\u4e00\uff09"},{"location":"csapp-00/#csapp_1","text":"\u5b58\u50a8\u5668\u7cfb\u7edf\uff08memeory system\uff09\uff1a\u5b58\u50a8\u5668\u7cfb\u7edf\u662f\u4e00\u4e2a\u5177\u6709\u4e0d\u540c\u5bb9\u91cf\u3001\u6210\u672c\u548c\u8bbf\u95ee\u65f6\u95f4\u7684\u5b58\u50a8\u8bbe\u5907\u7684\u5c42\u6b21\u7ed3\u6784\u3002 \u5c40\u90e8\u6027\uff08locality\uff09\uff1a\u5177\u6709\u826f\u597d\u5c40\u90e8\u6027\u7684\u7a0b\u5e8f\u503e\u5411\u4e8e\u8bbf\u95ee\u76f8\u540c\u6216\u662f\u90bb\u8fd1\u7684\u6570\u636e\u9879\u96c6\u5408\u3002\u5177\u6709\u826f\u597d\u5c40\u90e8\u6027\u7684\u7a0b\u5e8f\u6bd4\u5c40\u90e8\u6027\u5dee\u7684\u7a0b\u5e8f\u66f4\u591a\u503e\u5411\u4e8e\u4ece\u5b58\u50a8\u5668\u5c42\u6b21\u7ed3\u6784\u4e2d\u8f83\u9ad8\u5c42\u6b21\u5904\u8bbf\u95ee\u6570\u636e\u9879\uff0c\u56e0\u6b64\u8fd0\u884c\u5730\u66f4\u5feb\u3002 \u65f6\u95f4\u5c40\u90e8\u6027\uff1a\u5728\u4e00\u4e2a\u5177\u6709\u826f\u597d\u65f6\u95f4\u5c40\u90e8\u6027\u7684\u7a0b\u5e8f\u4e2d\uff0c\u88ab\u5f15\u7528\u8fc7\u4e00\u6b21\u7684\u5185\u5b58\u4f4d\u7f6e\u5f88\u53ef\u80fd\u5728\u4e0d\u8fdc\u7684\u5c06\u6765\u518d\u88ab\u591a\u6b21\u5f15\u7528\u3002 \u7a7a\u95f4\u5c40\u90e8\u6027\uff1a\u5728\u4e00\u4e2a\u5177\u6709\u826f\u597d\u7a7a\u95f4\u5c40\u90e8\u6027\u7684\u7a0b\u5e8f\u4e2d\uff0c\u5982\u679c\u4e00\u4e2a\u5185\u5b58\u4f4d\u7f6e\u88ab\u5f15\u7528\u4e86\u4e00\u6b21\uff0c\u90a3\u4e48\u7a0b\u5e8f\u5f88\u53ef\u80fd\u5728\u4e0d\u8fdc\u7684\u5c06\u6765\u5f15\u7528\u9644\u8fd1\u7684\u4e00\u4e2a\u5185\u5b58\u4f4d\u7f6e\u3002 \u968f\u673a\u8bbf\u95ee\u5b58\u50a8\u5668\uff08RAM\uff09\uff1a \u9759\u6001RAM\uff08SRAM\uff09\uff1a\u7528\u6765\u4f5c\u4e3a\u9ad8\u901f\u7f13\u5b58\u5b58\u50a8\u5668 \u52a8\u6001RAM\uff08DRAM\uff09\uff1a\u7528\u6765\u4f5c\u4e3a\u4e3b\u5b58\u4ee5\u53ca\u56fe\u5f62\u7cfb\u7edf\u7684\u5e27\u7f13\u51b2\u533a \u975e\u6613\u5931\u6027\u5b58\u50a8\u5668\uff08Nonvolatile memory\uff0cNVM\uff09\uff1a\u5982\u679c\u65ad\u7535\uff0cSRAM\u548cDRAM\u4f1a\u4e22\u5931\u5b83\u4eec\u7684\u4fe1\u606f\uff0c\u5b83\u4eec\u662f\u6613\u5931\u7684\u3002\u975e\u6613\u5931\u6027\u5b58\u50a8\u5668\u5173\u7535\u540e\u4ecd\u7136\u4fdd\u6301\u4fe1\u606f\uff0c\u5305\u62ec\u53ea\u8bfb\u5b58\u50a8\u5668\uff08ROM\uff09\u3002 PROM\uff1a\u53ea\u80fd\u88ab\u7f16\u7a0b\u4e00\u6b21 EPROM\uff1a\u53ef\u7f16\u7a0b100\u6b21 EEPROM\uff1a\u53ef\u7f16\u7a0b 10^5 10^5 \u6b21 \u95ea\u5b58\uff1a\u57fa\u4e8eEEPROM \u56fa\u6001\u786c\u76d8\uff08SSD\uff09\uff1a\u57fa\u4e8e\u95ea\u5b58\u7684\u78c1\u76d8\u9a71\u52a8\u5668 \u5b58\u50a8\u5668\u5c42\u6b21\u7ed3\u6784\u7684\u4e2d\u5fc3\u601d\u60f3\u662f\uff1a\u5bf9\u4e8e\u6bcf\u4e2ak\uff0c\u4f4d\u4e8ek\u5c42\u7684\u66f4\u5feb\u66f4\u5c0f\u7684\u5b58\u50a8\u8bbe\u5907\u4f5c\u4e3a\u4f4d\u4e8ek+1\u5c42\u7684\u66f4\u5927\u66f4\u6162\u7684\u5b58\u50a8\u8bbe\u5907\u7684\u7f13\u5b58\u3002\u7b2ck+1\u5c42\u7684\u5b58\u50a8\u5668\u88ab\u5212\u5206\u6210\u8fde\u7eed\u7684\u6570\u636e\u5bf9\u8c61\u7ec4\u5757\uff08block\uff09\uff0c\u6bcf\u4e2a\u5757\u90fd\u6709\u4e00\u4e2a\u552f\u4e00\u7684\u5730\u5740\u6216\u540d\u5b57\u3002\u6570\u636e\u603b\u662f\u4ee5\u5757\u5927\u5c0f\u4e3a\u4f20\u9001\u5355\u5143\u5728\u7b2ck\u5c42\u548ck+1\u5c42\u4e4b\u95f4\u6765\u56de\u590d\u5236\u7684\u3002 \u7f13\u5b58\u547d\u4e2d\uff08cache hit\uff09\uff1a\u5f53\u7a0b\u5e8f\u9700\u8981\u7b2ck+1\u5c42\u7684\u67d0\u4e2a\u6570\u636e\u5bf9\u8c61d\u65f6\uff0c\u5b83\u9996\u5148\u5728\u5f53\u524d\u5b58\u50a8\u5728\u7b2ck\u5c42\u7684\u4e00\u4e2a\u5757\u4e2d\u67e5\u627ed\u3002\u5982\u679cd\u521a\u597d\u7f13\u5b58\u5728\u7b2ck\u5c42\u4e2d\uff0c\u90a3\u4e48\u5c31\u662f\u6211\u4eec\u6240\u8bf4\u7684\u7f13\u5b58\u547d\u4e2d\u3002 \u94fe\u63a5\uff08linking\uff09\uff1a\u94fe\u63a5\u662f\u5c06\u5404\u79cd\u4ee3\u7801\u548c\u6570\u636e\u7247\u6bb5\u6536\u96c6\u5e76\u7ec4\u5408\u6210\u4e00\u4e2a\u5355\u4e00\u6587\u4ef6\u7684\u8fc7\u7a0b\uff0c\u8fd9\u4e2a\u6587\u4ef6\u53ef\u4ee5\u88ab\u52a0\u8f7d\u5230\u5185\u5b58\u5e76\u6267\u884c\u3002\u94fe\u63a5\u53ef\u4ee5\u6267\u884c\u4e8e\u7f16\u8bd1\u65f6\u3001\u52a0\u8f7d\u65f6\u3001\u8fd0\u884c\u65f6\uff0c\u4f7f\u5f97\u5206\u79bb\u7f16\u8bd1\u6210\u4e3a\u53ef\u80fd\u3002 \u63a7\u5236\u6d41\uff08control flow\uff09\uff1a\u7a0b\u5e8f\u8ba1\u6570\u5668\u5047\u8bbe\u4e00\u4e2a\u503c\u7684\u5e8f\u5217 a_0, a_1, ..., a_{n-1} a_0, a_1, ..., a_{n-1} \uff0c\u5176\u4e2d\uff0c a_k a_k \u662f\u67d0\u4e2a\u76f8\u5e94\u7684\u6307\u4ee4 I_k I_k \u7684\u5730\u5740\u3002\u6bcf\u6b21\u4ece a_k a_k \u5230 a_{k+1} a_{k+1} \u7684\u8fc7\u6e21\u79f0\u4e3a\u63a7\u5236\u8f6c\u79fb\u3002\u8fd9\u6837\u7684\u63a7\u5236\u8f6c\u79fb\u5e8f\u5217\u53eb\u505a\u5904\u7406\u5668\u7684\u63a7\u5236\u6d41\u3002 \u5f02\u5e38\u63a7\u5236\u6d41\uff1a\u73b0\u4ee3\u7cfb\u7edf\u901a\u8fc7\u4f7f\u63a7\u5236\u6d41\u53d1\u751f\u7a81\u53d8\u6765\u5bf9\u7cfb\u7edf\u72b6\u6001\u7684\u53d8\u5316\u505a\u51fa\u53cd\u5e94\u3002\u72b6\u6001\u53d8\u5316\u79f0\u4e3a\u4e8b\u4ef6\uff08event\uff09\u3002 \u5f02\u5e38\u8868\uff1a\u5f53\u5904\u7406\u5668\u68c0\u6d4b\u5230\u6709\u4e8b\u4ef6\u53d1\u751f\u65f6\uff0c\u5b83\u5c31\u4f1a\u901a\u8fc7\u4e00\u5f20\u53eb\u505a\u5f02\u5e38\u8868\u7684\u8df3\u8f6c\u8868\uff0c\u8fdb\u884c\u4e00\u4e2a\u95f4\u63a5\u8fc7\u7a0b\u8c03\u7528\uff0c\u5230\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u6765\u5904\u7406\u8fd9\u7c7b\u4e8b\u4ef6\u7684\u64cd\u4f5c\u7cfb\u7edf\u5b50\u7a0b\u5e8f\uff08\u5f02\u5e38\u5904\u7406\u7a0b\u5e8f\uff09\u3002 \u8fdb\u7a0b\uff08Process\uff09\uff1a\u5f02\u5e38\u662f\u5141\u8bb8\u64cd\u4f5c\u7cfb\u7edf\u5185\u6838\u63d0\u4f9b\u8fdb\u7a0b\u6982\u5ff5\u7684\u57fa\u672c\u6784\u9020\u5757\u3002\u8fdb\u7a0b\u63d0\u4f9b\u7ed9\u5e94\u7528\u7a0b\u5e8f\u7684\u5173\u952e\u62bd\u8c61\uff1a \u4e00\u4e2a\u72ec\u7acb\u7684\u903b\u8f91\u63a7\u5236\u6d41\uff0c\u63d0\u4f9b\u72ec\u5360\u5904\u7406\u5668\u7684\u5047\u8c61 \u4e00\u4e2a\u79c1\u6709\u7684\u5730\u5740\u7a7a\u95f4\uff0c\u63d0\u4f9b\u72ec\u5360\u5185\u5b58\u7cfb\u7edf\u7684\u5047\u8c61 \u903b\u8f91\u63a7\u5236\u6d41\uff1a\u7a0b\u5e8f\u8ba1\u6570\u5668\u4e2d\u7684\u6307\u4ee4\u5e8f\u5217\u3002\u8fdb\u7a0b\u662f\u8f6e\u6d41\u4f7f\u7528\u5904\u7406\u5668\u7684\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\u6267\u884c\u5b83\u7684\u6d41\u7684\u4e00\u90e8\u5206\uff0c\u7136\u540e\u88ab\u62a2\u5360\uff08\u6682\u65f6\u6302\u8d77\uff09\uff0c\u7136\u540e\u8f6e\u5230\u5176\u4ed6\u8fdb\u7a0b\u3002 \u5e76\u53d1\u6d41\uff1a\u4e00\u4e2a\u903b\u8f91\u6d41\u7684\u6267\u884c\u5728\u65f6\u95f4\u4e0a\u4e0e\u53e6\u4e00\u4e2a\u91cd\u53e0\uff0c\u79f0\u4e3a\u5e76\u53d1\u6d41\uff0c\u8fd9\u4e24\u4e2a\u6d41\u88ab\u79f0\u4e3a\u5e76\u53d1\uff08concurrency\uff09\u7684\u8fd0\u884c\u3002 \u591a\u4efb\u52a1\uff1a\u4e00\u4e2a\u8fdb\u7a0b\u548c\u5176\u4ed6\u8fdb\u7a0b\u8f6e\u6d41\u8fd0\u884c\u7684\u6982\u5ff5\u79f0\u4e3a\u591a\u4efb\u52a1\uff08multitasking\uff09\u3002\u4e00\u4e2a\u8fdb\u7a0b\u6267\u884c\u5b83\u7684\u63a7\u5236\u6d41\u7684\u4e00\u90e8\u5206\u7684\u6bcf\u4e00\u65f6\u95f4\u6bb5\u53eb\u65f6\u95f4\u7247\uff08time slice\uff09\uff0c\u56e0\u6b64\u591a\u4efb\u52a1\u4e5f\u53eb\u65f6\u95f4\u5206\u7247\uff08time slicing\uff09\u3002 \u5e76\u884c\uff1a\u5982\u679c\u4e24\u4e2a\u6d41\u5e76\u53d1\u5730\u8fd0\u884c\u5728\u4e0d\u540c\u7684\u5904\u7406\u5668\u6838\u6216\u8005\u8ba1\u7b97\u673a\u4e0a\uff0c\u90a3\u4e48\u6211\u4eec\u79f0\u5b83\u4eec\u4e3a\u5e76\u884c\u6d41\uff08parallel flow\uff09\uff0c\u5b83\u4eec\u5e76\u884c\u5730\u8fd0\u884c\uff08runnning in parallel\uff09\u3002 \u4e0a\u4e0b\u6587\u5207\u6362\uff08context switch\uff09\uff1a\u64cd\u4f5c\u7cfb\u7edf\u5185\u6838\u4f7f\u7528\u4e00\u79cd\u79f0\u4e3a\u4e0a\u4e0b\u6587\u5207\u6362\u7684\u8f83\u9ad8\u5c42\u5f62\u5f0f\u7684\u5f02\u5e38\u63a7\u5236\u6d41\u6765\u5b9e\u73b0\u591a\u4efb\u52a1\u3002\u5185\u6838\u4e3a\u6bcf\u4e2a\u8fdb\u7a0b\u7ef4\u6301\u4e00\u4e2a\u4e0a\u4e0b\u6587\u3002 \u8c03\u5ea6\uff08scheduling\uff09\uff1a\u5728\u8fdb\u7a0b\u6267\u884c\u7684\u67d0\u4e9b\u65f6\u523b\uff0c\u5185\u6838\u53ef\u4ee5\u51b3\u5b9a\u62a2\u5360\u5f53\u524d\u8fdb\u7a0b\uff0c\u5e76\u91cd\u65b0\u5f00\u59cb\u4e00\u4e2a\u5148\u524d\u88ab\u62a2\u5360\u4e86\u7684\u8fdb\u7a0b\u3002\u8fd9\u79cd\u51b3\u7b56\u5c31\u53eb\u505a\u8c03\u5ea6\uff08scheduling\uff09\uff0c\u5728\u5185\u6838\u4e2d\u7531\u8c03\u5ea6\u5668\uff08scheduler\uff09\u5b9e\u73b0\u3002 \u865a\u62df\u5185\u5b58\uff08VM\uff09\uff1a\u4e3a\u4e86\u66f4\u52a0\u6709\u6548\u5730\u7ba1\u7406\u5185\u5b58\u5e76\u5c11\u51fa\u9519\uff0c\u73b0\u4ee3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5bf9\u4e3b\u5b58\u7684\u62bd\u8c61\u6982\u5ff5\uff0c\u53eb\u505a\u865a\u62df\u5185\u5b58\uff08VM\uff09\u3002 \u7269\u7406\u5bfb\u5740\uff1a\u8ba1\u7b97\u673a\u7cfb\u7edf\u7684\u4e3b\u5b58\u88ab\u7ec4\u7ec7\u6210\u4e00\u4e2a\u7531M\u4e2a\u8fde\u7eed\u7684\u5b57\u8282\u5927\u5c0f\u5355\u5143\u7ec4\u6210\u7684\u6570\u7ec4\uff0c\u6bcf\u5b57\u8282\u90fd\u6709\u4e00\u4e2a\u552f\u4e00\u7684\u7269\u7406\u5730\u5740\uff08physical address\uff09\uff0cCPU\u8bbf\u95ee\u5185\u5b58\u6700\u81ea\u7136\u7684\u65b9\u5f0f\u5c31\u662f\u7269\u7406\u5bfb\u5740\uff08phsical addressing\uff09\u3002 \u865a\u62df\u5bfb\u5740\uff1aCPU\u901a\u8fc7\u751f\u6210\u4e00\u4e2a\u865a\u62df\u5730\u5740\uff08virtual address\uff09\u6765\u8bbf\u95ee\u4e3b\u5b58\uff0c\u8fd9\u4e2a\u865a\u62df\u5730\u5740\u5728\u88ab\u9001\u5230\u5185\u5b58\u4e4b\u524d\u5148\u8f6c\u6362\u6210\u9002\u5f53\u7684\u7269\u7406\u5730\u5740\u3002\u5c06\u865a\u62df\u5730\u5740\u8f6c\u6362\u4e3a\u7269\u7406\u5730\u5740\u79f0\u4e3a\u5730\u5740\u7ffb\u8bd1\u3002 \u5730\u5740\u7a7a\u95f4\uff1a\u5730\u5740\u7a7a\u95f4\u662f\u4e00\u4e2a\u975e\u8d1f\u6570\u5730\u5740\u7684\u6709\u5e8f\u96c6\u5408\u3002 \u865a\u62df\u9875\uff08virtual page\uff0cVP\uff09\uff1aVM\u7cfb\u7edf\u901a\u8fc7\u5c06\u865a\u62df\u5185\u5b58\u5206\u5272\u4e3a\u79f0\u4e3a\u865a\u62df\u9875\u7684\u5927\u5c0f\u56fa\u5b9a\u7684\u5757\u6765\u5904\u7406\u3002 \u52a8\u6001\u5185\u5b58\u5206\u914d\uff1a\u52a8\u6001\u5185\u5b58\u5206\u914d\u7ef4\u62a4\u7740\u4e00\u4e2a\u8fdb\u7a0b\u7684\u865a\u62df\u5185\u5b58\u533a\u57df\uff0c\u79f0\u4e3a\u5806\uff08heap\uff09\u3002\u5206\u914d\u5668\u5c06\u5806\u89c6\u4e3a\u4e00\u7ec4\u4e0d\u540c\u5927\u5c0f\u7684\u5757\u6765\u7ef4\u62a4\u3002\u5206\u914d\u5668\u6709\u4e24\u79cd\u98ce\u683c\uff1a \u663e\u5f0f\u5206\u914d\u5668\u3002\u8981\u6c42\u5e94\u7528\u663e\u5f0f\u5730\u91ca\u653e\u4efb\u4f55\u5df2\u5206\u914d\u7684\u5757\u3002\u4f8b\u5982\uff0cC\u8bed\u8a00\u7684malloc\u3002 \u9690\u5f0f\u5206\u914d\u5668\u3002\u8981\u6c42\u5206\u914d\u5668\u68c0\u6d4b\u4e00\u4e2a\u5df2\u5206\u914d\u5757\u4f55\u65f6\u4e0d\u518d\u7a0b\u5e8f\u6240\u4f7f\u7528\uff0c\u90a3\u4e48\u5c31\u91ca\u653e\u8fd9\u4e2a\u5757\u3002\u9690\u5f0f\u5206\u914d\u5668\u4e5f\u53eb\u5783\u573e\u6536\u96c6\u5668\uff08garbage collector\uff09\uff0c\u800c\u81ea\u52a8\u91ca\u653e\u672a\u4f7f\u7528\u7684\u5df2\u5206\u914d\u5757\u7684\u8fc7\u7a0b\u53eb\u505a\u5783\u573e\u6536\u96c6\uff08garbage collection\uff09\u3002\u4f8b\u5982\uff0cJava\u4e4b\u7c7b\u7684\u9ad8\u7ea7\u8bed\u8a00\u5c31\u4f9d\u8d56\u5783\u573e\u6536\u96c6\u6765\u91ca\u653e\u5df2\u5206\u914d\u5757\u3002","title":"CSAPP\u6982\u5ff5\u6574\u7406\uff08\u4e8c\uff09"},{"location":"csapp-00/#csapp_2","text":"\u8f93\u5165/\u8f93\u51fa\uff08I/O\uff09\uff1aI/O\u662f\u5728\u4e3b\u5b58\u548c\u5916\u90e8\u8bbe\u5907\uff08\u4f8b\u5982\u78c1\u76d8\u9a71\u52a8\u5668\u3001\u7ec8\u7aef\u548c\u7f51\u7edc\uff09\u4e4b\u95f4\u590d\u5236\u6570\u636e\u7684\u8fc7\u7a0b\u3002\u8f93\u5165\u64cd\u4f5c\u662f\u4eceI/O\u8bbe\u5907\u590d\u5236\u6570\u636e\u5230\u4e3b\u5b58\uff0c\u800c\u8f93\u51fa\u64cd\u4f5c\u662f\u4ece\u4e3b\u5b58\u590d\u5236\u6570\u636e\u5230I/O\u8bbe\u5907\u3002 \u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u6a21\u578b\uff1a\u6bcf\u4e2a\u7f51\u7edc\u5e94\u7528\u90fd\u662f\u57fa\u4e8e\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u6a21\u578b\u7684\u3002\u4e00\u4e2a\u5e94\u7528\u662f\u7531\u4e00\u4e2a\u670d\u52a1\u5668\u8fdb\u7a0b\u548c\u4e00\u4e2a\u6216\u8005\u591a\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b\u7ec4\u6210\u3002 \u4e8b\u52a1\uff08Transaction\uff09\uff1a\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u6a21\u578b\u4e2d\u7684\u57fa\u672c\u64cd\u4f5c\u662f\u4e8b\u52a1\u3002\u4e00\u4e2a\u4e8b\u52a1\u7531\u4ee5\u4e0b4\u6b65\u7ec4\u6210\uff1a 1\uff09\u5f53\u4e00\u4e2a\u5ba2\u6237\u7aef\u9700\u8981\u670d\u52a1\u65f6\uff0c\u5b83\u5411\u670d\u52a1\u5668\u53d1\u9001\u4e00\u4e2a\u8bf7\u6c42\uff0c\u53d1\u8d77\u4e00\u4e2a\u4e8b\u52a1\u3002 2\uff09\u670d\u52a1\u5668\u6536\u5230\u8bf7\u6c42\u540e\uff0c\u89e3\u91ca\u5b83\uff0c\u5e76\u4ee5\u9002\u5f53\u7684\u65b9\u5f0f\u64cd\u4f5c\u5b83\u7684\u8d44\u6e90\u3002 3\uff09\u670d\u52a1\u5668\u7ed9\u5ba2\u6237\u7aef\u53d1\u9001\u4e00\u4e2a\u54cd\u5e94\uff0c\u5e76\u7b49\u5f85\u4e0b\u4e00\u4e2a\u8bf7\u6c42\u3002 4\uff09\u5ba2\u6237\u7aef\u6536\u5230\u54cd\u5e94\u5e76\u5904\u7406\u5b83\u3002 \u7f51\u7edc\uff1a\u5bf9\u4e3b\u673a\u800c\u8a00\uff0c\u7f51\u7edc\u53ea\u662f\u4e00\u79cdI/O\u8bbe\u5907\uff0c\u662f\u6570\u636e\u6e90\u548c\u6570\u636e\u63a5\u6536\u65b9\u3002 \u534f\u8bae\uff1a\u89e3\u51b3\u4e3b\u673a\u4e4b\u95f4\u7684\u901a\u4fe1\u95ee\u9898\uff0c\u901a\u8fc7\u63a7\u5236\u4e3b\u673a\u548c\u8def\u7531\u5668\u534f\u540c\u5de5\u4f5c\u6765\u5b9e\u73b0\u6570\u636e\u4f20\u8f93\u3002 \u547d\u540d\u673a\u5236\u3002\u5b9a\u4e49\u4e00\u79cd\u4e00\u81f4\u7684\u4e3b\u673a\u5730\u5740\u683c\u5f0f\u3002 \u4f20\u9001\u673a\u5236\u3002\u5b9a\u4e49\u4e00\u79cd\u628a\u6570\u636e\u4f4d\u6346\u624e\u6210\u5305\u7684\u7edf\u4e00\u65b9\u5f0f\u3002\u4e00\u4e2a\u5305\u7531\u5305\u5934\u548c\u6709\u6548\u8f7d\u8377\u7ec4\u6210\u3002\u5176\u4e2d\u5305\u5934\u5305\u62ec\u5305\u7684\u5927\u5c0f\u4ee5\u53ca\u6e90\u4e3b\u673a\u548c\u76ee\u7684\u4e3b\u673a\u7684\u5730\u5740\uff0c\u6709\u6548\u8f7d\u8377\u5305\u62ec\u4ece\u6e90\u4e3b\u673a\u53d1\u51fa\u7684\u6570\u636e\u4f4d\u3002 \u56e0\u7279\u7f51\uff1a \u4e3b\u673a\u88ab\u6620\u5c04\u4e3a\u4e00\u7ec432\u4f4d\u7684IP\u5730\u5740 \u8fd9\u7ec4IP\u5730\u5740\u88ab\u6620\u5c04\u4e3a\u4e00\u7ec4\u79f0\u4e3a\u56e0\u7279\u7f51\u57df\u540d\u7684\u6807\u8bc6\u7b26 \u56e0\u7279\u7f51\u4e3b\u673a\u4e0a\u7684\u8fdb\u7a0b\u80fd\u591f\u901a\u8fc7\u8fde\u63a5\uff08connection\uff09\u548c\u4efb\u4f55\u5176\u4ed6\u56e0\u7279\u7f51\u4e3b\u673a\u4e0a\u7684\u8fdb\u7a0b\u901a\u4fe1 \u7aef\u53e3\uff1a Web\u670d\u52a1\u5668\uff1a Web\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u5668\u4e4b\u95f4\u7684\u4ea4\u4e92\u7528\u7684\u662fHTTP Web\u53ef\u4ee5\u7528\u4e00\u79cd\u53eb\u505aHTML\u7684\u8bed\u8a00\u6765\u7f16\u5199 Web\u670d\u52a1\u5668\u4ee5\u4e24\u79cd\u4e0d\u540c\u7684\u65b9\u5f0f\u5411\u5ba2\u6237\u7aef\u63d0\u4f9b\u5185\u5bb9\uff1a \u53d6\u4e00\u4e2a\u78c1\u76d8\u6587\u4ef6\uff0c\u5e76\u5c06\u5b83\u7684\u5185\u5bb9\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\u3002\u78c1\u76d8\u6587\u4ef6\u79f0\u4e3a\u9759\u6001\u5185\u5bb9\uff0c\u800c\u8fd4\u56de\u6587\u4ef6\u7ed9\u5ba2\u6237\u7aef\u7684\u8fc7\u7a0b\u79f0\u4e3a\u670d\u52a1\u9759\u6001\u5185\u5bb9\u3002 \u8fd0\u884c\u4e00\u4e2a\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u5e76\u5c06\u5b83\u7684\u8f93\u51fa\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\u3002\u8fd0\u884c\u65f6\u53ef\u6267\u884c\u6587\u4ef6\u7684\u8f93\u51fa\u79f0\u4e3a\u52a8\u6001\u5185\u5bb9\uff0c\u800c\u8fd0\u884c\u7a0b\u5e8f\u8fd4\u56de\u5b83\u7684\u8f93\u51fa\u5230\u5ba2\u6237\u7aef\u7684\u8fc7\u7a0b\u79f0\u4e3a\u670d\u52a1\u52a8\u6001\u5185\u5bb9\u3002 \u5e94\u7528\u7ea7\u5e76\u53d1\uff1a \u8bbf\u95ee\u6162\u901fI/O\u8bbe\u5907 \u4e0e\u4eba\u4ea4\u4e92 \u901a\u8fc7\u63a8\u8fdf\u5de5\u4f5c\u4ee5\u964d\u4f4e\u5ef6\u8fdf \u670d\u52a1\u591a\u4e2a\u7f51\u7edc\u5ba2\u6237\u7aef \u5728\u591a\u6838\u673a\u5668\u4e0a\u8fdb\u884c\u5e76\u884c\u8ba1\u7b97 \u5e76\u53d1\u7a0b\u5e8f\uff1a\u4f7f\u7528\u5e94\u7528\u7ea7\u5e76\u53d1\u7684\u5e94\u7528\u7a0b\u5e8f\u79f0\u4e3a\u5e76\u53d1\u7a0b\u5e8f\uff0c\u73b0\u4ee3\u64cd\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e09\u79cd\u57fa\u672c\u7684\u6784\u9020\u5e76\u53d1\u7a0b\u5e8f\u7684\u65b9\u6cd5\uff1a \u8fdb\u7a0b I/O\u591a\u8def\u590d\u7528 \u7ebf\u7a0b","title":"CSAPP\u6982\u5ff5\u6574\u7406\uff08\u4e09\uff09"},{"location":"ml-compilation-01/","text":"01 \u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u6982\u8ff0 \u00b6 \u53c2\u8003\u8d44\u6599 \u82f1\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22/ \u82f1\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/index.html \u4e2d\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22-zh/ \u4e2d\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/zh/index.html \u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u53ca\u76ee\u6807 \u00b6 \u673a\u5668\u5b66\u4e60\u90e8\u7f72\u95ee\u9898\uff1a\u5f00\u53d1\u548c\u90e8\u7f72\u4e4b\u95f4\u7684GAP\uff08\u786c\u4ef6\u3001\u64cd\u4f5c\u7cfb\u7edf\u3001\u5bb9\u5668\u3001\u5e93\u7b49\uff09 \u673a\u5668\u5b66\u4e60\u7f16\u8bd1\uff1a\u5c06\u6a21\u578b\u5f00\u53d1\u5f62\u6001\u8f6c\u5316\u4e3a\u90e8\u7f72\u5f62\u6001\u7684\u8fc7\u7a0b \u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u76ee\u6807\uff1a \u96c6\u6210\u548c\u6700\u5c0f\u5316\u4f9d\u8d56 \u5229\u7528\u786c\u4ef6\u81ea\u8eab\u7279\u6027\u52a0\u901f \u5404\u79cd\u5f62\u5f0f\u7684\u4f18\u5316 \u673a\u5668\u5b66\u4e60\u201c\u7f16\u8bd1\u201d\u662f\u76f8\u5bf9\u4e8e\u4f20\u7edf\u7f16\u8bd1\u7684\u4e00\u4e2a\u6bd4\u55bb\uff0c\u4e0d\u4e00\u5b9a\u6d89\u53ca\u4ee3\u7801\u751f\u6210 \u4e3a\u4ec0\u4e48\u5b66\u4e60\u673a\u5668\u5b66\u4e60\u7f16\u8bd1 \u00b6 \u63d0\u4f9b\u673a\u5668\u5b66\u4e60\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848 \u6df1\u5165\u7406\u89e3\u673a\u5668\u5b66\u4e60\u6846\u67b6 \u6784\u5efa\u9488\u5bf9\u65b0\u786c\u4ef6\u7684\u8f6f\u4ef6\u6808 \u4e86\u89e3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u662f\u600e\u4e48\u8dd1\u8d77\u6765\u7684 \u5173\u952e\u5143\u7d20\uff1a\u5f20\u91cf\u51fd\u6570\u548c\u62bd\u8c61 \u00b6 \u5f20\u91cf\u548c\u5f20\u91cf\u51fd\u6570 \u62bd\u8c61\u548c\u5b9e\u73b0\uff1a\u66f4\u7ec6\u5316\u7684\u62bd\u8c61\u662f\u66f4\u9ad8\u7ea7\u7684\u62bd\u8c61\u7684\u5b9e\u73b0 \u62bd\u8c61\u548c\u5b9e\u73b0\u53ef\u80fd\u662f\u6240\u6709\u8ba1\u7b97\u673a\u7cfb\u7edf\u4e2d\u6700\u91cd\u8981\u7684\u5173\u952e\u5b57\u3002\u62bd\u8c61\u6307\u5b9a\u201c\u505a\u4ec0\u4e48\u201d\uff0c\u5b9e\u73b0\u63d0\u4f9b\u201c\u5982\u4f55\u201d\u505a\u3002\u6ca1\u6709\u5177\u4f53\u7684\u754c\u9650\u3002 \u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u662f\u5728\u505a\u5f20\u91cf\u51fd\u6570\u4e4b\u95f4\u7684\u53d8\u6362\u3002 \u8bfe\u7a0b\u4e3b\u8981\u5305\u62ec\u8ba1\u7b97\u56fe\u3001\u5f20\u91cf\u7a0b\u5e8f\u3001\u7b97\u5b50\u5e93\u3001\u786c\u4ef6\u6307\u4ee4\u56db\u4e2a\u5c42\u9762\u7684\u62bd\u8c61\u3002","title":"01 \u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u6982\u8ff0"},{"location":"ml-compilation-01/#01","text":"\u53c2\u8003\u8d44\u6599 \u82f1\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22/ \u82f1\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/index.html \u4e2d\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22-zh/ \u4e2d\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/zh/index.html","title":"01 \u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u6982\u8ff0"},{"location":"ml-compilation-01/#_1","text":"\u673a\u5668\u5b66\u4e60\u90e8\u7f72\u95ee\u9898\uff1a\u5f00\u53d1\u548c\u90e8\u7f72\u4e4b\u95f4\u7684GAP\uff08\u786c\u4ef6\u3001\u64cd\u4f5c\u7cfb\u7edf\u3001\u5bb9\u5668\u3001\u5e93\u7b49\uff09 \u673a\u5668\u5b66\u4e60\u7f16\u8bd1\uff1a\u5c06\u6a21\u578b\u5f00\u53d1\u5f62\u6001\u8f6c\u5316\u4e3a\u90e8\u7f72\u5f62\u6001\u7684\u8fc7\u7a0b \u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u76ee\u6807\uff1a \u96c6\u6210\u548c\u6700\u5c0f\u5316\u4f9d\u8d56 \u5229\u7528\u786c\u4ef6\u81ea\u8eab\u7279\u6027\u52a0\u901f \u5404\u79cd\u5f62\u5f0f\u7684\u4f18\u5316 \u673a\u5668\u5b66\u4e60\u201c\u7f16\u8bd1\u201d\u662f\u76f8\u5bf9\u4e8e\u4f20\u7edf\u7f16\u8bd1\u7684\u4e00\u4e2a\u6bd4\u55bb\uff0c\u4e0d\u4e00\u5b9a\u6d89\u53ca\u4ee3\u7801\u751f\u6210","title":"\u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u53ca\u76ee\u6807"},{"location":"ml-compilation-01/#_2","text":"\u63d0\u4f9b\u673a\u5668\u5b66\u4e60\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848 \u6df1\u5165\u7406\u89e3\u673a\u5668\u5b66\u4e60\u6846\u67b6 \u6784\u5efa\u9488\u5bf9\u65b0\u786c\u4ef6\u7684\u8f6f\u4ef6\u6808 \u4e86\u89e3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u662f\u600e\u4e48\u8dd1\u8d77\u6765\u7684","title":"\u4e3a\u4ec0\u4e48\u5b66\u4e60\u673a\u5668\u5b66\u4e60\u7f16\u8bd1"},{"location":"ml-compilation-01/#_3","text":"\u5f20\u91cf\u548c\u5f20\u91cf\u51fd\u6570 \u62bd\u8c61\u548c\u5b9e\u73b0\uff1a\u66f4\u7ec6\u5316\u7684\u62bd\u8c61\u662f\u66f4\u9ad8\u7ea7\u7684\u62bd\u8c61\u7684\u5b9e\u73b0 \u62bd\u8c61\u548c\u5b9e\u73b0\u53ef\u80fd\u662f\u6240\u6709\u8ba1\u7b97\u673a\u7cfb\u7edf\u4e2d\u6700\u91cd\u8981\u7684\u5173\u952e\u5b57\u3002\u62bd\u8c61\u6307\u5b9a\u201c\u505a\u4ec0\u4e48\u201d\uff0c\u5b9e\u73b0\u63d0\u4f9b\u201c\u5982\u4f55\u201d\u505a\u3002\u6ca1\u6709\u5177\u4f53\u7684\u754c\u9650\u3002 \u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u662f\u5728\u505a\u5f20\u91cf\u51fd\u6570\u4e4b\u95f4\u7684\u53d8\u6362\u3002 \u8bfe\u7a0b\u4e3b\u8981\u5305\u62ec\u8ba1\u7b97\u56fe\u3001\u5f20\u91cf\u7a0b\u5e8f\u3001\u7b97\u5b50\u5e93\u3001\u786c\u4ef6\u6307\u4ee4\u56db\u4e2a\u5c42\u9762\u7684\u62bd\u8c61\u3002","title":"\u5173\u952e\u5143\u7d20\uff1a\u5f20\u91cf\u51fd\u6570\u548c\u62bd\u8c61"},{"location":"ml-compilation-02/","text":"02 \u5f20\u91cf\u7a0b\u5e8f\u62bd\u8c61 \u00b6 \u53c2\u8003\u8d44\u6599 \u82f1\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22/ \u82f1\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/index.html \u4e2d\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22-zh/ \u4e2d\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/zh/index.html \u5143\u5f20\u91cf\u51fd\u6570 \u00b6 \u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u7684\u8fc7\u7a0b\u53ef\u4ee5\u88ab\u770b\u4f5c\u5f20\u91cf\u51fd\u6570\u4e4b\u95f4\u7684\u53d8\u6362\u3002\u4e00\u4e2a\u5178\u578b\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6267\u884c\u5305\u542b\u8bb8\u591a\u6b65\u5c06\u8f93\u5165\u5f20\u91cf\u4e4b\u95f4\u8f6c\u5316\u4e3a\u6700\u7ec8\u9884\u6d4b\u7684\u8ba1\u7b97\u6b65\u9aa4\uff0c\u5176\u4e2d\u7684\u6bcf\u4e00\u6b65\u90fd\u88ab\u79f0\u4e3a\u5143\u5f20\u91cf\u51fd\u6570 (primitive tensor function)\u3002 \u5f20\u91cf\u7b97\u5b50 linear, add, relu \u548c softmax \u5747\u4e3a\u5143\u5f20\u91cf\u51fd\u6570\u3002 \u8bb8\u591a\u4e0d\u540c\u7684\u62bd\u8c61\u80fd\u591f\u8868\u793a\uff08\u548c\u5b9e\u73b0\uff09\u540c\u6837\u7684\u5143\u5f20\u91cf\u51fd\u6570\uff08\u6b63\u5982\u4e0b\u56fe\u6240\u793a\uff09\u3002 \u8bb8\u591a\u673a\u5668\u5b66\u4e60\u6846\u67b6\u90fd\u63d0\u4f9b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u7f16\u8bd1\u8fc7\u7a0b\uff0c\u4ee5\u5c06\u5143\u5f20\u91cf\u51fd\u6570\u53d8\u6362\u4e3a\u66f4\u52a0\u4e13\u95e8\u7684\u3001\u9488\u5bf9\u7279\u5b9a\u5de5\u4f5c\u548c\u90e8\u7f72\u73af\u5883\u7684\u51fd\u6570\u3002 \u5f20\u91cf\u7a0b\u5e8f\u62bd\u8c61 \u00b6 \u901a\u5e38\u6765\u8bf4\uff0c\u4e00\u4e2a\u5178\u578b\u7684\u5143\u5f20\u91cf\u51fd\u6570\u5b9e\u73b0\u7684\u62bd\u8c61\u5305\u542b\u4e86\u4e00\u4e0b\u6210\u5206\uff1a\u5b58\u50a8\u6570\u636e\u7684\u591a\u7ef4\u6570\u7ec4\uff08Multi-dimensional buffers\uff09\uff0c\u9a71\u52a8\u5f20\u91cf\u8ba1\u7b97\u7684\u5faa\u73af\u5d4c\u5957\uff08Loop nests\uff09\u4ee5\u53ca\u8ba1\u7b97\u90e8\u5206\u672c\u8eab\u7684\u8bed\u53e5\uff08Computions\uff09\u3002 \u5f20\u91cf\u7a0b\u5e8f\u662f\u4e00\u4e2a\u8868\u793a\u5143\u5f20\u91cf\u51fd\u6570\u7684\u6709\u6548\u62bd\u8c61\u3002 \u5173\u952e\u6210\u5206\u5305\u62ec: \u591a\u7ef4\u6570\u7ec4\uff0c\u5faa\u73af\u5d4c\u5957\uff0c\u8ba1\u7b97\u8bed\u53e5\u3002 \u7a0b\u5e8f\u53d8\u6362\u53ef\u4ee5\u88ab\u7528\u4e8e\u52a0\u901f\u5f20\u91cf\u7a0b\u5e8f\u7684\u6267\u884c\u3002 \u5f20\u91cf\u7a0b\u5e8f\u4e2d\u989d\u5916\u7684\u7ed3\u6784\u80fd\u591f\u4e3a\u7a0b\u5e8f\u53d8\u6362\u63d0\u4f9b\u66f4\u591a\u7684\u4fe1\u606f\u3002","title":"02 \u5f20\u91cf\u7a0b\u5e8f\u62bd\u8c61"},{"location":"ml-compilation-02/#02","text":"\u53c2\u8003\u8d44\u6599 \u82f1\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22/ \u82f1\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/index.html \u4e2d\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22-zh/ \u4e2d\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/zh/index.html","title":"02 \u5f20\u91cf\u7a0b\u5e8f\u62bd\u8c61"},{"location":"ml-compilation-02/#_1","text":"\u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u7684\u8fc7\u7a0b\u53ef\u4ee5\u88ab\u770b\u4f5c\u5f20\u91cf\u51fd\u6570\u4e4b\u95f4\u7684\u53d8\u6362\u3002\u4e00\u4e2a\u5178\u578b\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6267\u884c\u5305\u542b\u8bb8\u591a\u6b65\u5c06\u8f93\u5165\u5f20\u91cf\u4e4b\u95f4\u8f6c\u5316\u4e3a\u6700\u7ec8\u9884\u6d4b\u7684\u8ba1\u7b97\u6b65\u9aa4\uff0c\u5176\u4e2d\u7684\u6bcf\u4e00\u6b65\u90fd\u88ab\u79f0\u4e3a\u5143\u5f20\u91cf\u51fd\u6570 (primitive tensor function)\u3002 \u5f20\u91cf\u7b97\u5b50 linear, add, relu \u548c softmax \u5747\u4e3a\u5143\u5f20\u91cf\u51fd\u6570\u3002 \u8bb8\u591a\u4e0d\u540c\u7684\u62bd\u8c61\u80fd\u591f\u8868\u793a\uff08\u548c\u5b9e\u73b0\uff09\u540c\u6837\u7684\u5143\u5f20\u91cf\u51fd\u6570\uff08\u6b63\u5982\u4e0b\u56fe\u6240\u793a\uff09\u3002 \u8bb8\u591a\u673a\u5668\u5b66\u4e60\u6846\u67b6\u90fd\u63d0\u4f9b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u7f16\u8bd1\u8fc7\u7a0b\uff0c\u4ee5\u5c06\u5143\u5f20\u91cf\u51fd\u6570\u53d8\u6362\u4e3a\u66f4\u52a0\u4e13\u95e8\u7684\u3001\u9488\u5bf9\u7279\u5b9a\u5de5\u4f5c\u548c\u90e8\u7f72\u73af\u5883\u7684\u51fd\u6570\u3002","title":"\u5143\u5f20\u91cf\u51fd\u6570"},{"location":"ml-compilation-02/#_2","text":"\u901a\u5e38\u6765\u8bf4\uff0c\u4e00\u4e2a\u5178\u578b\u7684\u5143\u5f20\u91cf\u51fd\u6570\u5b9e\u73b0\u7684\u62bd\u8c61\u5305\u542b\u4e86\u4e00\u4e0b\u6210\u5206\uff1a\u5b58\u50a8\u6570\u636e\u7684\u591a\u7ef4\u6570\u7ec4\uff08Multi-dimensional buffers\uff09\uff0c\u9a71\u52a8\u5f20\u91cf\u8ba1\u7b97\u7684\u5faa\u73af\u5d4c\u5957\uff08Loop nests\uff09\u4ee5\u53ca\u8ba1\u7b97\u90e8\u5206\u672c\u8eab\u7684\u8bed\u53e5\uff08Computions\uff09\u3002 \u5f20\u91cf\u7a0b\u5e8f\u662f\u4e00\u4e2a\u8868\u793a\u5143\u5f20\u91cf\u51fd\u6570\u7684\u6709\u6548\u62bd\u8c61\u3002 \u5173\u952e\u6210\u5206\u5305\u62ec: \u591a\u7ef4\u6570\u7ec4\uff0c\u5faa\u73af\u5d4c\u5957\uff0c\u8ba1\u7b97\u8bed\u53e5\u3002 \u7a0b\u5e8f\u53d8\u6362\u53ef\u4ee5\u88ab\u7528\u4e8e\u52a0\u901f\u5f20\u91cf\u7a0b\u5e8f\u7684\u6267\u884c\u3002 \u5f20\u91cf\u7a0b\u5e8f\u4e2d\u989d\u5916\u7684\u7ed3\u6784\u80fd\u591f\u4e3a\u7a0b\u5e8f\u53d8\u6362\u63d0\u4f9b\u66f4\u591a\u7684\u4fe1\u606f\u3002","title":"\u5f20\u91cf\u7a0b\u5e8f\u62bd\u8c61"},{"location":"ml-compilation-03/","text":"03 \u5f20\u91cf\u7a0b\u5e8f\u62bd\u8c61\u6848\u4f8b\u7814\u7a76\uff1aTensorIR \u00b6 \u53c2\u8003\u8d44\u6599 \u82f1\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22/ \u82f1\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/index.html \u4e2d\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22-zh/ \u4e2d\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/zh/index.html TensorIR \u662f\u6807\u51c6\u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u6846\u67b6 Apache TVM \u4e2d\u4f7f\u7528\u7684\u5f20\u91cf\u7a0b\u5e8f\u62bd\u8c61\u3002 \u4f7f\u7528\u5f20\u91cf\u7a0b\u5e8f\u62bd\u8c61\u7684\u4e3b\u8981\u76ee\u7684\u662f\u8868\u793a\u5faa\u73af\u548c\u76f8\u5173\u7684\u786c\u4ef6\u52a0\u901f\u9009\u62e9\uff0c\u5982\u591a\u7ebf\u7a0b\u3001\u7279\u6b8a\u786c\u4ef6\u6307\u4ee4\u7684\u4f7f\u7528\u548c\u5185\u5b58\u8bbf\u95ee\u3002 \u5f20\u91cf\u51fd\u6570\u62bd\u8c61 - TensorIR\u5b9e\u4f8b \u00b6 \u5bf9\u4e8e\u4e24\u4e2a\u5927\u5c0f\u4e3a 128\u00d7128 128\u00d7128 \u7684\u77e9\u9635 A \u548c B\uff0c\u8fdb\u884c\u5982\u4e0b\u4e24\u6b65\u7684\u5f20\u91cf\u8ba1\u7b97\uff1a\u4e00\u4e2a\u7ebf\u6027\u5c42\uff08\u77e9\u9635\u4e58\u6cd5\uff09\u4e0e\u4e00\u4e2a ReLU \u6fc0\u6d3b\u5c42\u3002 $$ Y_{i,j} = \\sum_k{A_{i,k}\u00d7{B_{k,j}}} $$ C_{i,j} = relu(Y_{i,j}) = max(Y_{i,j},0) C_{i,j} = relu(Y_{i,j}) = max(Y_{i,j},0) NumPy\u8868\u793a \u00b6 dtype = \"float32\" a_np = np . random . rand ( 128 , 128 ) . astype ( dtype ) b_np = np . random . rand ( 128 , 128 ) . astype ( dtype ) # a @ b is equivalent to np.matmul(a, b) c_mm_relu = np . maximum ( a_np @ b_np , 0 ) NumPy\u5b9e\u73b0mm_relu \u00b6 def lnumpy_mm_relu ( A : np . ndarray , B : np . ndarray , C : np . ndarray ): Y = np . empty (( 128 , 128 ), dtype = \"float32\" ) for i in range ( 128 ): for j in range ( 128 ): for k in range ( 128 ): if k == 0 : Y [ i , j ] = 0 Y [ i , j ] = Y [ i , j ] + A [ i , k ] * B [ k , j ] for i in range ( 128 ): for j in range ( 128 ): C [ i , j ] = max ( Y [ i , j ], 0 ) TensorIR \u5b9e\u73b0mm_relu \u00b6 \u4e0b\u9762\u7684\u4ee3\u7801\u5757\u5c55\u793a\u4e86 mm_relu \u7684 TensorIR \u5b9e\u73b0\u3002\u8fd9\u91cc\u7684\u4ee3\u7801\u662f\u7528\u4e00\u79cd\u540d\u4e3a TVMScript \u7684\u8bed\u8a00\u5b9e\u73b0\u7684\uff0c\u5b83\u662f\u4e00\u79cd\u5d4c\u5165\u5728 Python AST \u4e2d\u7684\u7279\u5b9a\u9886\u57df\u65b9\u8a00\u3002 @tvm . script . ir_module class MyModule : @T . prim_func def mm_relu ( A : T . Buffer [( 128 , 128 ), \"float32\" ], B : T . Buffer [( 128 , 128 ), \"float32\" ], C : T . Buffer [( 128 , 128 ), \"float32\" ]): T . func_attr ({ \"global_symbol\" : \"mm_relu\" , \"tir.noalias\" : True }) Y = T . alloc_buffer (( 128 , 128 ), dtype = \"float32\" ) for i , j , k in T . grid ( 128 , 128 , 128 ): with T . block ( \"Y\" ): vi = T . axis . spatial ( 128 , i ) vj = T . axis . spatial ( 128 , j ) vk = T . axis . reduce ( 128 , k ) with T . init (): Y [ vi , vj ] = T . float32 ( 0 ) Y [ vi , vj ] = Y [ vi , vj ] + A [ vi , vk ] * B [ vk , vj ] for i , j in T . grid ( 128 , 128 ): with T . block ( \"C\" ): vi = T . axis . spatial ( 128 , i ) vj = T . axis . spatial ( 128 , j ) C [ vi , vj ] = T . max ( Y [ vi , vj ], T . float32 ( 0 )) TensorIR\u548cNumPy\u4ee3\u7801\u5bf9\u5e94\u2b50 \u00b6 \u5757\u548c\u5757\u8f74\uff08Block axis\uff09 \u00b6 TensorIR \u5305\u542b\u4e00\u4e2a\u540d\u4e3a T.block \u7684\u989d\u5916\u7ed3\u6784\u3002\u5757 \u662f TensorIR \u4e2d\u7684\u57fa\u672c\u8ba1\u7b97\u5355\u4f4d\u3002\u4e00\u4e2a\u5757\u5305\u542b\u4e00\u7ec4\u5757\u8f74\uff08 vi\u3001vj\u3001vk \uff09\u548c\u56f4\u7ed5\u5b83\u4eec\u5b9a\u4e49\u7684\u8ba1\u7b97\u3002 vi = T . axis . spatial ( 128 , i ) vj = T . axis . spatial ( 128 , j ) vk = T . axis . reduce ( 128 , k ) \u4e0a\u9762\u4e09\u884c\u58f0\u660e\u4e86\u5173\u4e8e\u5757\u8f74\u7684**\u5173\u952e\u6027\u8d28**\uff0c\u8bed\u6cd5\u5982\u4e0b\u3002 [ block_axis ] = T . axis . [ axis_type ]([ axis_range ], [ mapped_value ]) \u8fd9\u4e09\u884c\u5305\u542b\u4ee5\u4e0b\u4fe1\u606f\uff1a \u5b9a\u4e49\u4e86 vi \u3001 vj \u3001 vk \u5e94\u88ab\u7ed1\u5b9a\u5230\u7684\u4f4d\u7f6e\uff08\u5728\u672c\u4f8b\u4e2d\u4e3a i \u3001 j \u548c k \uff09\uff1b \u58f0\u660e\u4e86 vi \u3001 vj \u3001 vk \u7684\u539f\u59cb\u8303\u56f4\uff08 T.axis.spatial(128, i) \u4e2d\u7684 128 \uff09\uff1b \u58f0\u660e\u4e86\u5757\u8f74\u7684\u5c5e\u6027\uff08 spatial , reduce \uff09\u3002 \u8f74\u5c5e\u6027\u6807\u8bb0\u4e86\u8f74\u4e0e\u6b63\u5728\u6267\u884c\u7684\u8ba1\u7b97\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 \u5757 Y \u901a\u8fc7\u8bfb\u53d6\u6765\u81ea A[vi, vk] \u548c B[vk, vj] \u7684\u503c\u6765\u8ba1\u7b97\u7ed3\u679c Y[vi, vj] \uff0c\u5e76\u5bf9\u6240\u6709\u53ef\u80fd\u7684 vk \u6267\u884c\u6c42\u548c\u3002 \u5728\u8fd9\u4e2a\u7279\u5b9a\u793a\u4f8b\u4e2d\uff0c\u5982\u679c\u6211\u4eec\u5c06 vi \u3001 vj \u56fa\u5b9a\u4e3a (0, 1) \uff0c\u5e76\u5bf9 vk in range(0, 128) \u6267\u884c\u5757 Y \uff0c\u6211\u4eec\u53ef\u4ee5\u72ec\u7acb\u4e8e\u5176\u4ed6\u53ef\u80fd\u7684\u4f4d\u7f6e\uff08\u5177\u6709\u4e0d\u540c vi , vj \u503c\u7684\u4f4d\u7f6e\uff09\u6709\u6548\u5730\u8ba1\u7b97 C[0, 1] \u3002\u6211\u4eec\u53ef\u4ee5\u79f0 vi \u3001 vj \u4e3a**\u7a7a\u95f4\u8f74**\uff0c\u6d89\u53ca\u5f52\u7ea6\uff08 vk \uff09\u7684\u8f74\u88ab\u547d\u540d\u4e3a**\u5f52\u7ea6\u8f74**\u3002 \u51fd\u6570\u5c5e\u6027\u548c\u88c5\u9970\u5668 \u00b6 \u51fd\u6570\u5c5e\u6027\u4fe1\u606f\u5305\u542b\u5173\u4e8e\u51fd\u6570\u7684\u989d\u5916\u4fe1\u606f\u3002 T . func_attr ({ \"global_symbol\" : \"mm_relu\" , \"tir.noalias\" : True }) @tvm.script.ir_module \u548c @T.prim_func \u8fd9\u4e24\u4e2a\u88c5\u9970\u5668\u7528\u4e8e\u8868\u793a\u5bf9\u5e94\u90e8\u5206\u7684\u7c7b\u578b\u3002 @tvm.script.ir_module \u8868\u793a MyModule \u662f\u4e00\u4e2a IRModule\u3002IRModule \u662f\u5728\u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u4e2d\u4fdd\u5b58\u5f20\u91cf\u51fd\u6570\u96c6\u5408\u7684\u5bb9\u5668\u5bf9\u8c61\u3002 \u5c0f\u7ed3 \u00b6 \u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u4e00\u540c\u770b\u8fc7\u4e86\u4e00\u4e2a TensorIR \u7a0b\u5e8f\u793a\u4f8b\uff0c\u5e76\u6db5\u76d6\u4e86\u5927\u90e8\u5206\u5143\u7d20\uff0c\u5305\u62ec\uff1a \u53c2\u6570\u548c\u4e2d\u95f4\u4e34\u65f6\u5185\u5b58\u4e2d\u7684\u7f13\u51b2\u533a\u58f0\u660e\uff1b For \u5faa\u73af\u8fed\u4ee3\uff1b \u5757\u548c\u5757\u8f74\u5c5e\u6027\u3002 \u5f20\u91cf\u51fd\u6570\u53d8\u6362 \u00b6 \u5728\u5b9e\u8df5\u4e2d\uff0c\u53ef\u4ee5\u6709\u591a\u79cd\u65b9\u6cd5\u6765\u5b9e\u73b0\u76f8\u540c\u7684\u529f\u80fd\uff0c\u5e76\u4e14\u6bcf\u79cd\u5b9e\u73b0\u90fd\u53ef\u80fd\u5bfc\u81f4\u4e0d\u540c\u7684\u6027\u80fd\u3002 # \u65b9\u6cd51 def lnumpy_mm_relu ( A : np . ndarray , B : np . ndarray , C : np . ndarray ): Y = np . empty (( 128 , 128 ), dtype = \"float32\" ) for i in range ( 128 ): for j in range ( 128 ): for k in range ( 128 ): if k == 0 : Y [ i , j ] = 0 Y [ i , j ] = Y [ i , j ] + A [ i , k ] * B [ k , j ] for i in range ( 128 ): for j in range ( 128 ): C [ i , j ] = max ( Y [ i , j ], 0 ) # \u65b9\u6cd52 def lnumpy_mm_relu_v2 ( A : np . ndarray , B : np . ndarray , C : np . ndarray ): Y = np . empty (( 128 , 128 ), dtype = \"float32\" ) for i in range ( 128 ): for j0 in range ( 32 ): for k in range ( 128 ): for j1 in range ( 4 ): j = j0 * 4 + j1 if k == 0 : Y [ i , j ] = 0 Y [ i , j ] = Y [ i , j ] + A [ i , k ] * B [ k , j ] for i in range ( 128 ): for j in range ( 128 ): C [ i , j ] = max ( Y [ i , j ], 0 ) c_np = np . empty (( 128 , 128 ), dtype = dtype ) lnumpy_mm_relu_v2 ( a_np , b_np , c_np ) np . testing . assert_allclose ( c_mm_relu , c_np , rtol = 1e-5 ) # \u65b9\u6cd53 def lnumpy_mm_relu_v3 ( A : np . ndarray , B : np . ndarray , C : np . ndarray ): Y = np . empty (( 128 , 128 ), dtype = \"float32\" ) for i in range ( 128 ): for j0 in range ( 32 ): # Y_init for j1 in range ( 4 ): j = j0 * 4 + j1 Y [ i , j ] = 0 # Y_update for k in range ( 128 ): for j1 in range ( 4 ): j = j0 * 4 + j1 Y [ i , j ] = Y [ i , j ] + A [ i , k ] * B [ k , j ] # C for j1 in range ( 4 ): j = j0 * 4 + j1 C [ i , j ] = max ( Y [ i , j ], 0 ) c_np = np . empty (( 128 , 128 ), dtype = dtype ) lnumpy_mm_relu_v3 ( a_np , b_np , c_np ) np . testing . assert_allclose ( c_mm_relu , c_np , rtol = 1e-5 ) \u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u6d41\u7a0b \u00b6 \u6807\u51c6\u5f00\u53d1\u8fc7\u7a0b\uff1a \u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u6d41\u7a0b\uff1a \u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u6d41\u7a0b\u7684\u4e3b\u8981\u533a\u522b\u5728\u4e8e IRModule\uff08\u7a0b\u5e8f\uff09\u4e4b\u95f4\u7684\u7a0b\u5e8f\u53d8\u6362\u3002\u6240\u4ee5\u6211\u4eec\u4e0d\u4ec5\u53ef\u4ee5\u901a\u8fc7\u5f00\u53d1\uff08\u901a\u8fc7\u624b\u52a8\u7f16\u5199\u4ee3\u7801\u3010TVMScript\u3011\u6216\u751f\u6210\u4ee3\u7801\u3010\u5f20\u91cf\u8868\u8fbe\u5f0f\u3011\uff09\u63d0\u51fa\u7a0b\u5e8f\u53d8\u4f53\uff0c\u8fd8\u53ef\u4ee5\u901a\u8fc7\u53d8\u6362\u5f20\u91cf\u7a0b\u5e8f\u6765\u83b7\u5f97\u53d8\u4f53\u3002","title":"03 \u5f20\u91cf\u7a0b\u5e8f\u62bd\u8c61\u6848\u4f8b\u7814\u7a76\uff1aTensorIR"},{"location":"ml-compilation-03/#03-tensorir","text":"\u53c2\u8003\u8d44\u6599 \u82f1\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22/ \u82f1\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/index.html \u4e2d\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22-zh/ \u4e2d\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/zh/index.html TensorIR \u662f\u6807\u51c6\u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u6846\u67b6 Apache TVM \u4e2d\u4f7f\u7528\u7684\u5f20\u91cf\u7a0b\u5e8f\u62bd\u8c61\u3002 \u4f7f\u7528\u5f20\u91cf\u7a0b\u5e8f\u62bd\u8c61\u7684\u4e3b\u8981\u76ee\u7684\u662f\u8868\u793a\u5faa\u73af\u548c\u76f8\u5173\u7684\u786c\u4ef6\u52a0\u901f\u9009\u62e9\uff0c\u5982\u591a\u7ebf\u7a0b\u3001\u7279\u6b8a\u786c\u4ef6\u6307\u4ee4\u7684\u4f7f\u7528\u548c\u5185\u5b58\u8bbf\u95ee\u3002","title":"03 \u5f20\u91cf\u7a0b\u5e8f\u62bd\u8c61\u6848\u4f8b\u7814\u7a76\uff1aTensorIR"},{"location":"ml-compilation-03/#-tensorir","text":"\u5bf9\u4e8e\u4e24\u4e2a\u5927\u5c0f\u4e3a 128\u00d7128 128\u00d7128 \u7684\u77e9\u9635 A \u548c B\uff0c\u8fdb\u884c\u5982\u4e0b\u4e24\u6b65\u7684\u5f20\u91cf\u8ba1\u7b97\uff1a\u4e00\u4e2a\u7ebf\u6027\u5c42\uff08\u77e9\u9635\u4e58\u6cd5\uff09\u4e0e\u4e00\u4e2a ReLU \u6fc0\u6d3b\u5c42\u3002 $$ Y_{i,j} = \\sum_k{A_{i,k}\u00d7{B_{k,j}}} $$ C_{i,j} = relu(Y_{i,j}) = max(Y_{i,j},0) C_{i,j} = relu(Y_{i,j}) = max(Y_{i,j},0)","title":"\u5f20\u91cf\u51fd\u6570\u62bd\u8c61 - TensorIR\u5b9e\u4f8b"},{"location":"ml-compilation-03/#numpy","text":"dtype = \"float32\" a_np = np . random . rand ( 128 , 128 ) . astype ( dtype ) b_np = np . random . rand ( 128 , 128 ) . astype ( dtype ) # a @ b is equivalent to np.matmul(a, b) c_mm_relu = np . maximum ( a_np @ b_np , 0 )","title":"NumPy\u8868\u793a"},{"location":"ml-compilation-03/#numpymm_relu","text":"def lnumpy_mm_relu ( A : np . ndarray , B : np . ndarray , C : np . ndarray ): Y = np . empty (( 128 , 128 ), dtype = \"float32\" ) for i in range ( 128 ): for j in range ( 128 ): for k in range ( 128 ): if k == 0 : Y [ i , j ] = 0 Y [ i , j ] = Y [ i , j ] + A [ i , k ] * B [ k , j ] for i in range ( 128 ): for j in range ( 128 ): C [ i , j ] = max ( Y [ i , j ], 0 )","title":"NumPy\u5b9e\u73b0mm_relu"},{"location":"ml-compilation-03/#tensorir-mm_relu","text":"\u4e0b\u9762\u7684\u4ee3\u7801\u5757\u5c55\u793a\u4e86 mm_relu \u7684 TensorIR \u5b9e\u73b0\u3002\u8fd9\u91cc\u7684\u4ee3\u7801\u662f\u7528\u4e00\u79cd\u540d\u4e3a TVMScript \u7684\u8bed\u8a00\u5b9e\u73b0\u7684\uff0c\u5b83\u662f\u4e00\u79cd\u5d4c\u5165\u5728 Python AST \u4e2d\u7684\u7279\u5b9a\u9886\u57df\u65b9\u8a00\u3002 @tvm . script . ir_module class MyModule : @T . prim_func def mm_relu ( A : T . Buffer [( 128 , 128 ), \"float32\" ], B : T . Buffer [( 128 , 128 ), \"float32\" ], C : T . Buffer [( 128 , 128 ), \"float32\" ]): T . func_attr ({ \"global_symbol\" : \"mm_relu\" , \"tir.noalias\" : True }) Y = T . alloc_buffer (( 128 , 128 ), dtype = \"float32\" ) for i , j , k in T . grid ( 128 , 128 , 128 ): with T . block ( \"Y\" ): vi = T . axis . spatial ( 128 , i ) vj = T . axis . spatial ( 128 , j ) vk = T . axis . reduce ( 128 , k ) with T . init (): Y [ vi , vj ] = T . float32 ( 0 ) Y [ vi , vj ] = Y [ vi , vj ] + A [ vi , vk ] * B [ vk , vj ] for i , j in T . grid ( 128 , 128 ): with T . block ( \"C\" ): vi = T . axis . spatial ( 128 , i ) vj = T . axis . spatial ( 128 , j ) C [ vi , vj ] = T . max ( Y [ vi , vj ], T . float32 ( 0 ))","title":"TensorIR \u5b9e\u73b0mm_relu"},{"location":"ml-compilation-03/#tensorirnumpy","text":"","title":"TensorIR\u548cNumPy\u4ee3\u7801\u5bf9\u5e94\u2b50"},{"location":"ml-compilation-03/#block-axis","text":"TensorIR \u5305\u542b\u4e00\u4e2a\u540d\u4e3a T.block \u7684\u989d\u5916\u7ed3\u6784\u3002\u5757 \u662f TensorIR \u4e2d\u7684\u57fa\u672c\u8ba1\u7b97\u5355\u4f4d\u3002\u4e00\u4e2a\u5757\u5305\u542b\u4e00\u7ec4\u5757\u8f74\uff08 vi\u3001vj\u3001vk \uff09\u548c\u56f4\u7ed5\u5b83\u4eec\u5b9a\u4e49\u7684\u8ba1\u7b97\u3002 vi = T . axis . spatial ( 128 , i ) vj = T . axis . spatial ( 128 , j ) vk = T . axis . reduce ( 128 , k ) \u4e0a\u9762\u4e09\u884c\u58f0\u660e\u4e86\u5173\u4e8e\u5757\u8f74\u7684**\u5173\u952e\u6027\u8d28**\uff0c\u8bed\u6cd5\u5982\u4e0b\u3002 [ block_axis ] = T . axis . [ axis_type ]([ axis_range ], [ mapped_value ]) \u8fd9\u4e09\u884c\u5305\u542b\u4ee5\u4e0b\u4fe1\u606f\uff1a \u5b9a\u4e49\u4e86 vi \u3001 vj \u3001 vk \u5e94\u88ab\u7ed1\u5b9a\u5230\u7684\u4f4d\u7f6e\uff08\u5728\u672c\u4f8b\u4e2d\u4e3a i \u3001 j \u548c k \uff09\uff1b \u58f0\u660e\u4e86 vi \u3001 vj \u3001 vk \u7684\u539f\u59cb\u8303\u56f4\uff08 T.axis.spatial(128, i) \u4e2d\u7684 128 \uff09\uff1b \u58f0\u660e\u4e86\u5757\u8f74\u7684\u5c5e\u6027\uff08 spatial , reduce \uff09\u3002 \u8f74\u5c5e\u6027\u6807\u8bb0\u4e86\u8f74\u4e0e\u6b63\u5728\u6267\u884c\u7684\u8ba1\u7b97\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 \u5757 Y \u901a\u8fc7\u8bfb\u53d6\u6765\u81ea A[vi, vk] \u548c B[vk, vj] \u7684\u503c\u6765\u8ba1\u7b97\u7ed3\u679c Y[vi, vj] \uff0c\u5e76\u5bf9\u6240\u6709\u53ef\u80fd\u7684 vk \u6267\u884c\u6c42\u548c\u3002 \u5728\u8fd9\u4e2a\u7279\u5b9a\u793a\u4f8b\u4e2d\uff0c\u5982\u679c\u6211\u4eec\u5c06 vi \u3001 vj \u56fa\u5b9a\u4e3a (0, 1) \uff0c\u5e76\u5bf9 vk in range(0, 128) \u6267\u884c\u5757 Y \uff0c\u6211\u4eec\u53ef\u4ee5\u72ec\u7acb\u4e8e\u5176\u4ed6\u53ef\u80fd\u7684\u4f4d\u7f6e\uff08\u5177\u6709\u4e0d\u540c vi , vj \u503c\u7684\u4f4d\u7f6e\uff09\u6709\u6548\u5730\u8ba1\u7b97 C[0, 1] \u3002\u6211\u4eec\u53ef\u4ee5\u79f0 vi \u3001 vj \u4e3a**\u7a7a\u95f4\u8f74**\uff0c\u6d89\u53ca\u5f52\u7ea6\uff08 vk \uff09\u7684\u8f74\u88ab\u547d\u540d\u4e3a**\u5f52\u7ea6\u8f74**\u3002","title":"\u5757\u548c\u5757\u8f74\uff08Block axis\uff09"},{"location":"ml-compilation-03/#_1","text":"\u51fd\u6570\u5c5e\u6027\u4fe1\u606f\u5305\u542b\u5173\u4e8e\u51fd\u6570\u7684\u989d\u5916\u4fe1\u606f\u3002 T . func_attr ({ \"global_symbol\" : \"mm_relu\" , \"tir.noalias\" : True }) @tvm.script.ir_module \u548c @T.prim_func \u8fd9\u4e24\u4e2a\u88c5\u9970\u5668\u7528\u4e8e\u8868\u793a\u5bf9\u5e94\u90e8\u5206\u7684\u7c7b\u578b\u3002 @tvm.script.ir_module \u8868\u793a MyModule \u662f\u4e00\u4e2a IRModule\u3002IRModule \u662f\u5728\u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u4e2d\u4fdd\u5b58\u5f20\u91cf\u51fd\u6570\u96c6\u5408\u7684\u5bb9\u5668\u5bf9\u8c61\u3002","title":"\u51fd\u6570\u5c5e\u6027\u548c\u88c5\u9970\u5668"},{"location":"ml-compilation-03/#_2","text":"\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u4e00\u540c\u770b\u8fc7\u4e86\u4e00\u4e2a TensorIR \u7a0b\u5e8f\u793a\u4f8b\uff0c\u5e76\u6db5\u76d6\u4e86\u5927\u90e8\u5206\u5143\u7d20\uff0c\u5305\u62ec\uff1a \u53c2\u6570\u548c\u4e2d\u95f4\u4e34\u65f6\u5185\u5b58\u4e2d\u7684\u7f13\u51b2\u533a\u58f0\u660e\uff1b For \u5faa\u73af\u8fed\u4ee3\uff1b \u5757\u548c\u5757\u8f74\u5c5e\u6027\u3002","title":"\u5c0f\u7ed3"},{"location":"ml-compilation-03/#_3","text":"\u5728\u5b9e\u8df5\u4e2d\uff0c\u53ef\u4ee5\u6709\u591a\u79cd\u65b9\u6cd5\u6765\u5b9e\u73b0\u76f8\u540c\u7684\u529f\u80fd\uff0c\u5e76\u4e14\u6bcf\u79cd\u5b9e\u73b0\u90fd\u53ef\u80fd\u5bfc\u81f4\u4e0d\u540c\u7684\u6027\u80fd\u3002 # \u65b9\u6cd51 def lnumpy_mm_relu ( A : np . ndarray , B : np . ndarray , C : np . ndarray ): Y = np . empty (( 128 , 128 ), dtype = \"float32\" ) for i in range ( 128 ): for j in range ( 128 ): for k in range ( 128 ): if k == 0 : Y [ i , j ] = 0 Y [ i , j ] = Y [ i , j ] + A [ i , k ] * B [ k , j ] for i in range ( 128 ): for j in range ( 128 ): C [ i , j ] = max ( Y [ i , j ], 0 ) # \u65b9\u6cd52 def lnumpy_mm_relu_v2 ( A : np . ndarray , B : np . ndarray , C : np . ndarray ): Y = np . empty (( 128 , 128 ), dtype = \"float32\" ) for i in range ( 128 ): for j0 in range ( 32 ): for k in range ( 128 ): for j1 in range ( 4 ): j = j0 * 4 + j1 if k == 0 : Y [ i , j ] = 0 Y [ i , j ] = Y [ i , j ] + A [ i , k ] * B [ k , j ] for i in range ( 128 ): for j in range ( 128 ): C [ i , j ] = max ( Y [ i , j ], 0 ) c_np = np . empty (( 128 , 128 ), dtype = dtype ) lnumpy_mm_relu_v2 ( a_np , b_np , c_np ) np . testing . assert_allclose ( c_mm_relu , c_np , rtol = 1e-5 ) # \u65b9\u6cd53 def lnumpy_mm_relu_v3 ( A : np . ndarray , B : np . ndarray , C : np . ndarray ): Y = np . empty (( 128 , 128 ), dtype = \"float32\" ) for i in range ( 128 ): for j0 in range ( 32 ): # Y_init for j1 in range ( 4 ): j = j0 * 4 + j1 Y [ i , j ] = 0 # Y_update for k in range ( 128 ): for j1 in range ( 4 ): j = j0 * 4 + j1 Y [ i , j ] = Y [ i , j ] + A [ i , k ] * B [ k , j ] # C for j1 in range ( 4 ): j = j0 * 4 + j1 C [ i , j ] = max ( Y [ i , j ], 0 ) c_np = np . empty (( 128 , 128 ), dtype = dtype ) lnumpy_mm_relu_v3 ( a_np , b_np , c_np ) np . testing . assert_allclose ( c_mm_relu , c_np , rtol = 1e-5 )","title":"\u5f20\u91cf\u51fd\u6570\u53d8\u6362"},{"location":"ml-compilation-03/#_4","text":"\u6807\u51c6\u5f00\u53d1\u8fc7\u7a0b\uff1a \u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u6d41\u7a0b\uff1a \u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u6d41\u7a0b\u7684\u4e3b\u8981\u533a\u522b\u5728\u4e8e IRModule\uff08\u7a0b\u5e8f\uff09\u4e4b\u95f4\u7684\u7a0b\u5e8f\u53d8\u6362\u3002\u6240\u4ee5\u6211\u4eec\u4e0d\u4ec5\u53ef\u4ee5\u901a\u8fc7\u5f00\u53d1\uff08\u901a\u8fc7\u624b\u52a8\u7f16\u5199\u4ee3\u7801\u3010TVMScript\u3011\u6216\u751f\u6210\u4ee3\u7801\u3010\u5f20\u91cf\u8868\u8fbe\u5f0f\u3011\uff09\u63d0\u51fa\u7a0b\u5e8f\u53d8\u4f53\uff0c\u8fd8\u53ef\u4ee5\u901a\u8fc7\u53d8\u6362\u5f20\u91cf\u7a0b\u5e8f\u6765\u83b7\u5f97\u53d8\u4f53\u3002","title":"\u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u6d41\u7a0b"},{"location":"ml-compilation-04/","text":"04 \u7aef\u5230\u7aef\u6a21\u578b\u6574\u5408 \u00b6 \u53c2\u8003\u8d44\u6599 \u82f1\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22/ \u82f1\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/index.html \u4e2d\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22-zh/ \u4e2d\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/zh/index.html \u672c\u8282\u8ba8\u8bba\u5982\u4f55\u6784\u5efa\u7aef\u5230\u7aef\u6a21\u578b\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u5bfc\u5165\u4f9d\u8d56\u9879\u3001\u52a0\u8f7d\u6570\u636e\u96c6\u3001\u4e0b\u8f7d\u6a21\u578b\u53c2\u6570 ! python3 - m pip install mlc - ai - nightly - f https : // mlc . ai / wheels import tvm from tvm.ir.module import IRModule from tvm.script import tir as T , relax as R import numpy as np from tvm import relax \\ # This is needed for deferring annotation parsing in TVMScript from __future__ import annotations import IPython def code2html ( code ): \"\"\"Helper function to use pygments to turn the code string into highlighted html.\"\"\" import pygments from pygments.lexers import Python3Lexer from pygments.formatters import HtmlFormatter formatter = HtmlFormatter () html = pygments . highlight ( code , Python3Lexer (), formatter ) return \"<style> %s </style> %s \\n \" % ( formatter . get_style_defs ( \".highlight\" ), html ) import torchvision import torch test_data = torchvision . datasets . FashionMNIST ( root = \"data\" , train = False , download = True , transform = torchvision . transforms . ToTensor () ) test_loader = torch . utils . data . DataLoader ( test_data , batch_size = 1 , shuffle = True ) class_names = [ 'T-shirt/top' , 'Trouser' , 'Pullover' , 'Dress' , 'Coat' , 'Sandal' , 'Shirt' , 'Sneaker' , 'Bag' , 'Ankle boot' ] img , label = next ( iter ( test_loader )) img = img . reshape ( 1 , 28 , 28 ) . numpy () ! wget https : // github . com / mlc - ai / web - data / raw / main / models / fasionmnist_mlp_params . pkl \u7aef\u5230\u7aef\u6a21\u578b\u6574\u5408 \u00b6 \u5728\u672c\u7ae0\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u4ee5\u4e0b\u6a21\u578b\u4f5c\u4e3a\u793a\u4f8b\u3002\u8fd9\u662f\u4e00\u4e2a\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u7531\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42\uff08linear\uff09\u548c\u4e00\u4e2a\u6709 relu \u6fc0\u6d3b\u5c42\u7ec4\u6210\u3002 \u4e3a\u4e86\u7b80\u5316\u95ee\u9898\uff0c\u6211\u4eec\u5220\u9664\u4e86\u6700\u7ec8\u7684 softmax \u5c42\u3002\u8f93\u51fa\u5206\u6570\u662f\u672a\u6807\u51c6\u5316\u7684\uff0c\u4f46\u6700\u5927\u503c\u4ecd\u7136\u5bf9\u5e94\u4e8e\u6700\u53ef\u80fd\u7684\u7c7b\u522b\u3002 \u6a21\u578b\u7684Numpy\u5b9e\u73b0 \u00b6 def numpy_mlp ( data , w0 , b0 , w1 , b1 ): lv0 = data @ w0 . T + b0 lv1 = np . maximum ( lv0 , 0 ) lv2 = lv1 @ w1 . T + b1 return lv2 import pickle as pkl mlp_params = pkl . load ( open ( \"fasionmnist_mlp_params.pkl\" , \"rb\" )) res = numpy_mlp ( img . reshape ( 1 , 784 ), mlp_params [ \"w0\" ], mlp_params [ \"b0\" ], mlp_params [ \"w1\" ], mlp_params [ \"b1\" ]) print ( res ) pred_kind = res . argmax ( axis = 1 ) print ( pred_kind ) print ( \"NumPy-MLP Prediction:\" , class_names [ pred_kind [ 0 ]]) \u6a21\u578b\u7684\u5e95\u5c42Numpy\u5b9e\u73b0 \u00b6 def lnumpy_linear0 ( X : np . ndarray , W : np . ndarray , B : np . ndarray , Z : np . ndarray ): Y = np . empty (( 1 , 128 ), dtype = \"float32\" ) for i in range ( 1 ): for j in range ( 128 ): for k in range ( 784 ): if k == 0 : Y [ i , j ] = 0 Y [ i , j ] = Y [ i , j ] + X [ i , k ] * W [ j , k ] for i in range ( 1 ): for j in range ( 128 ): Z [ i , j ] = Y [ i , j ] + B [ j ] def lnumpy_relu0 ( X : np . ndarray , Y : np . ndarray ): for i in range ( 1 ): for j in range ( 128 ): Y [ i , j ] = np . maximum ( X [ i , j ], 0 ) def lnumpy_linear1 ( X : np . ndarray , W : np . ndarray , B : np . ndarray , Z : np . ndarray ): Y = np . empty (( 1 , 10 ), dtype = \"float32\" ) for i in range ( 1 ): for j in range ( 10 ): for k in range ( 128 ): if k == 0 : Y [ i , j ] = 0 Y [ i , j ] = Y [ i , j ] + X [ i , k ] * W [ j , k ] for i in range ( 1 ): for j in range ( 10 ): Z [ i , j ] = Y [ i , j ] + B [ j ] def lnumpy_mlp ( data , w0 , b0 , w1 , b1 ): lv0 = np . empty (( 1 , 128 ), dtype = \"float32\" ) lnumpy_linear0 ( data , w0 , b0 , lv0 ) lv1 = np . empty (( 1 , 128 ), dtype = \"float32\" ) lnumpy_relu0 ( lv0 , lv1 ) out = np . empty (( 1 , 10 ), dtype = \"float32\" ) lnumpy_linear1 ( lv1 , w1 , b1 , out ) return out result = lnumpy_mlp ( img . reshape ( 1 , 784 ), mlp_params [ \"w0\" ], mlp_params [ \"b0\" ], mlp_params [ \"w1\" ], mlp_params [ \"b1\" ]) pred_kind = result . argmax ( axis = 1 ) print ( \"Low-level Numpy MLP Prediction:\" , class_names [ pred_kind [ 0 ]]) \u5728TVMScript\u4e2d\u6784\u5efa\u7aef\u5230\u7aefIRModule \u00b6 @tvm . script . ir_module class MyModule : @T . prim_func def relu0 ( X : T . Buffer [( 1 , 128 ), \"float32\" ], Y : T . Buffer [( 1 , 128 ), \"float32\" ]): # function attr dict T . func_attr ({ \"global_symbol\" : \"relu0\" , \"tir.noalias\" : True }) for i , j in T . grid ( 1 , 128 ): with T . block ( \"Y\" ): vi , vj = T . axis . remap ( \"SS\" , [ i , j ]) Y [ vi , vj ] = T . max ( X [ vi , vj ], T . float32 ( 0 )) @T . prim_func def linear0 ( X : T . Buffer [( 1 , 784 ), \"float32\" ], W : T . Buffer [( 128 , 784 ), \"float32\" ], B : T . Buffer [( 128 ,), \"float32\" ], Z : T . Buffer [( 1 , 128 ), \"float32\" ]): T . func_attr ({ \"global_symbol\" : \"linear0\" , \"tir.noalias\" : True }) Y = T . alloc_buffer (( 1 , 128 ), \"float32\" ) for i , j , k in T . grid ( 1 , 128 , 784 ): with T . block ( \"Y\" ): vi , vj , vk = T . axis . remap ( \"SSR\" , [ i , j , k ]) with T . init (): Y [ vi , vj ] = T . float32 ( 0 ) Y [ vi , vj ] = Y [ vi , vj ] + X [ vi , vk ] * W [ vj , vk ] for i , j in T . grid ( 1 , 128 ): with T . block ( \"Z\" ): vi , vj = T . axis . remap ( \"SS\" , [ i , j ]) Z [ vi , vj ] = Y [ vi , vj ] + B [ vj ] @T . prim_func def linear1 ( X : T . Buffer [( 1 , 128 ), \"float32\" ], W : T . Buffer [( 10 , 128 ), \"float32\" ], B : T . Buffer [( 10 ,), \"float32\" ], Z : T . Buffer [( 1 , 10 ), \"float32\" ]): T . func_attr ({ \"global_symbol\" : \"linear1\" , \"tir.noalias\" : True }) Y = T . alloc_buffer (( 1 , 10 ), \"float32\" ) for i , j , k in T . grid ( 1 , 10 , 128 ): with T . block ( \"Y\" ): vi , vj , vk = T . axis . remap ( \"SSR\" , [ i , j , k ]) with T . init (): Y [ vi , vj ] = T . float32 ( 0 ) Y [ vi , vj ] = Y [ vi , vj ] + X [ vi , vk ] * W [ vj , vk ] for i , j in T . grid ( 1 , 10 ): with T . block ( \"Z\" ): vi , vj = T . axis . remap ( \"SS\" , [ i , j ]) Z [ vi , vj ] = Y [ vi , vj ] + B [ vj ] @R . function def main ( x : Tensor (( 1 , 784 ), \"float32\" ), w0 : Tensor (( 128 , 784 ), \"float32\" ), b0 : Tensor (( 128 ,), \"float32\" ), w1 : Tensor (( 10 , 128 ), \"float32\" ), b1 : Tensor (( 10 ,), \"float32\" )): with R . dataflow (): lv0 = R . call_tir ( linear0 , ( x , w0 , b0 ), ( 1 , 128 ), dtype = \"float32\" ) lv1 = R . call_tir ( relu0 , ( lv0 ,), ( 1 , 128 ), dtype = \"float32\" ) out = R . call_tir ( linear1 , ( lv1 , w1 , b1 ), ( 1 , 10 ), dtype = \"float32\" ) R . output ( out ) return out \u8ba1\u7b97\u56fe \u00b6 \u8ba1\u7b97\u56fe\u662f\u7528\u6765\u8868\u793a\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u6a21\u578b\u5728\u8bad\u7ec3\u4e0e\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8ba1\u7b97\u903b\u8f91\u4e0e\u72b6\u6001\u7684\u5de5\u5177\u3002\u8ba1\u7b97\u6846\u67b6\u5728\u540e\u7aef\u4f1a\u5c06\u524d\u7aef\u8bed\u8a00\u6784\u5efa\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u524d\u5411\u8ba1\u7b97\u4e0e\u53cd\u5411\u68af\u5ea6\u8ba1\u7b97\u4ee5\u8ba1\u7b97\u56fe\u7684\u5f62\u5f0f\u6765\u8fdb\u884c\u8868\u793a\u3002\u8ba1\u7b97\u56fe\u7531\u57fa\u672c\u6570\u636e\u7ed3\u6784\uff1a\u5f20\u91cf(Tensor)\u548c\u57fa\u672c\u8fd0\u7b97\u5355\u5143\uff1a\u7b97\u5b50(Operator)\u6784\u6210\u3002\u5728\u8ba1\u7b97\u56fe\u4e2d\u901a\u5e38\u4f7f\u7528\u8282\u70b9\u6765\u8868\u793a\u7b97\u5b50\uff0c\u8282\u70b9\u95f4\u7684\u6709\u5411\u7ebf\u6bb5\u6765\u8868\u793a\u5f20\u91cf\u72b6\u6001\uff0c\u540c\u65f6\u4e5f\u63cf\u8ff0\u4e86\u8ba1\u7b97\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002 \u300a\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\uff1a\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u300b call_tir \u00b6 \u8ba1\u7b97\u56fe\u4e2d\u7684\u6bcf\u4e2a\u64cd\u4f5c\u6b65\u9aa4\u90fd\u5305\u542b\u4e00\u4e2a R.call_tir \u64cd\u4f5c\u3002 \u8fd9\u662f\u5f15\u5165\u5143\u5f20\u91cf\u51fd\u6570\u7684\u8fc7\u7a0b\u3002 \u5177\u4f53\u6765\u8bf4\uff0ccall_tir \u63a5\u53d7\u4e00\u4e2a\u5143\u51fd\u6570 (prim_func) \u7684\u8f93\u5165\u5217\u8868\uff0c\u5e76\u5206\u914d\u4e00\u4e2a\u8f93\u51fa\u5f20\u91cfres\uff0c\u7136\u540e\u5c06\u8f93\u5165\u548c\u8f93\u51fa\u4f20\u9012\u7ed9prim_func\u3002 \u6267\u884c prim_func \u540e\uff0c\u7ed3\u679c\u4f1a\u586b\u5145\u5230 res \u4e2d\uff0c\u7136\u540e\u6211\u4eec\u53ef\u4ee5\u8fd4\u56de\u7ed3\u679c\u3002 \u8fd9\u4e2a\u60f3\u6cd5\u662f\u8f93\u5165\u548c\u8f93\u51fa\u5728\u5916\u90e8\u663e\u5f0f\u5206\u914d\u5e76\u4f20\u9012\u7ed9\u5e95\u5c42\u5143\u51fd\u6570\u3002 Dataflow Block \u00b6 \u6784\u5efa\u5e76\u8fd0\u884c\u6a21\u578b \u00b6 IPython . display . Code ( MyModule . script (), language = \"python\" ) \u6211\u4eec\u8c03\u7528 relax.vm.build \u6765\u6784\u5efa\u8fd9\u4e2a\u51fd\u6570\u3002 ex = relax . vm . build ( MyModule , target = \"llvm\" ) type ( ex ) build \u51fd\u6570\u4f1a\u7ed9\u6211\u4eec\u4e00\u4e2a\u53ef\u6267\u884c\u6587\u4ef6\u3002\u6211\u4eec\u53ef\u4ee5\u521d\u59cb\u5316\u4e00\u4e2a\u865a\u62df\u673a\u6267\u884c\u5668\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u8fd0\u884c\u8be5\u51fd\u6570\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u5c06\u4f20\u5165\u7b2c\u4e8c\u4e2a\u53c2\u6570\uff0c\u6307\u793a\u6211\u4eec\u8981\u5728\u54ea\u4e2a\u8bbe\u5907\u4e0a\u8fd0\u884c\u7aef\u5230\u7aef\u6267\u884c\u3002 \u8ba8\u8bba\u548c\u603b\u7ed3 \u00b6 \u8ba1\u7b97\u56fe\u62bd\u8c61\u6709\u52a9\u4e8e\u5c06\u5143\u5f20\u91cf\u51fd\u6570\u62fc\u63a5\u5728\u4e00\u8d77\u4ee5\u8fdb\u884c\u7aef\u5230\u7aef\u6267\u884c\u3002 Relax \u62bd\u8c61\u7684\u5173\u952e\u8981\u7d20\u5305\u62ec call_tir \u6784\u9020\uff0c\u5c06\u76ee\u6807\u4f20\u9012\u89c4\u8303\u7684\u5143\u51fd\u6570\u5d4c\u5165\u5230\u8ba1\u7b97\u56fe\u4e2d Dataflow block \u8ba1\u7b97\u56fe\u5141\u8bb8\u8c03\u7528\u73af\u5883\u5e93\u51fd\u6570\u548c TensorIR \u51fd\u6570\u3002","title":"04 \u7aef\u5230\u7aef\u6a21\u578b\u6574\u5408"},{"location":"ml-compilation-04/#04","text":"\u53c2\u8003\u8d44\u6599 \u82f1\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22/ \u82f1\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/index.html \u4e2d\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22-zh/ \u4e2d\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/zh/index.html \u672c\u8282\u8ba8\u8bba\u5982\u4f55\u6784\u5efa\u7aef\u5230\u7aef\u6a21\u578b\u3002","title":"04 \u7aef\u5230\u7aef\u6a21\u578b\u6574\u5408"},{"location":"ml-compilation-04/#_1","text":"\u5bfc\u5165\u4f9d\u8d56\u9879\u3001\u52a0\u8f7d\u6570\u636e\u96c6\u3001\u4e0b\u8f7d\u6a21\u578b\u53c2\u6570 ! python3 - m pip install mlc - ai - nightly - f https : // mlc . ai / wheels import tvm from tvm.ir.module import IRModule from tvm.script import tir as T , relax as R import numpy as np from tvm import relax \\ # This is needed for deferring annotation parsing in TVMScript from __future__ import annotations import IPython def code2html ( code ): \"\"\"Helper function to use pygments to turn the code string into highlighted html.\"\"\" import pygments from pygments.lexers import Python3Lexer from pygments.formatters import HtmlFormatter formatter = HtmlFormatter () html = pygments . highlight ( code , Python3Lexer (), formatter ) return \"<style> %s </style> %s \\n \" % ( formatter . get_style_defs ( \".highlight\" ), html ) import torchvision import torch test_data = torchvision . datasets . FashionMNIST ( root = \"data\" , train = False , download = True , transform = torchvision . transforms . ToTensor () ) test_loader = torch . utils . data . DataLoader ( test_data , batch_size = 1 , shuffle = True ) class_names = [ 'T-shirt/top' , 'Trouser' , 'Pullover' , 'Dress' , 'Coat' , 'Sandal' , 'Shirt' , 'Sneaker' , 'Bag' , 'Ankle boot' ] img , label = next ( iter ( test_loader )) img = img . reshape ( 1 , 28 , 28 ) . numpy () ! wget https : // github . com / mlc - ai / web - data / raw / main / models / fasionmnist_mlp_params . pkl","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"ml-compilation-04/#_2","text":"\u5728\u672c\u7ae0\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u4ee5\u4e0b\u6a21\u578b\u4f5c\u4e3a\u793a\u4f8b\u3002\u8fd9\u662f\u4e00\u4e2a\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u7531\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42\uff08linear\uff09\u548c\u4e00\u4e2a\u6709 relu \u6fc0\u6d3b\u5c42\u7ec4\u6210\u3002 \u4e3a\u4e86\u7b80\u5316\u95ee\u9898\uff0c\u6211\u4eec\u5220\u9664\u4e86\u6700\u7ec8\u7684 softmax \u5c42\u3002\u8f93\u51fa\u5206\u6570\u662f\u672a\u6807\u51c6\u5316\u7684\uff0c\u4f46\u6700\u5927\u503c\u4ecd\u7136\u5bf9\u5e94\u4e8e\u6700\u53ef\u80fd\u7684\u7c7b\u522b\u3002","title":"\u7aef\u5230\u7aef\u6a21\u578b\u6574\u5408"},{"location":"ml-compilation-04/#numpy","text":"def numpy_mlp ( data , w0 , b0 , w1 , b1 ): lv0 = data @ w0 . T + b0 lv1 = np . maximum ( lv0 , 0 ) lv2 = lv1 @ w1 . T + b1 return lv2 import pickle as pkl mlp_params = pkl . load ( open ( \"fasionmnist_mlp_params.pkl\" , \"rb\" )) res = numpy_mlp ( img . reshape ( 1 , 784 ), mlp_params [ \"w0\" ], mlp_params [ \"b0\" ], mlp_params [ \"w1\" ], mlp_params [ \"b1\" ]) print ( res ) pred_kind = res . argmax ( axis = 1 ) print ( pred_kind ) print ( \"NumPy-MLP Prediction:\" , class_names [ pred_kind [ 0 ]])","title":"\u6a21\u578b\u7684Numpy\u5b9e\u73b0"},{"location":"ml-compilation-04/#numpy_1","text":"def lnumpy_linear0 ( X : np . ndarray , W : np . ndarray , B : np . ndarray , Z : np . ndarray ): Y = np . empty (( 1 , 128 ), dtype = \"float32\" ) for i in range ( 1 ): for j in range ( 128 ): for k in range ( 784 ): if k == 0 : Y [ i , j ] = 0 Y [ i , j ] = Y [ i , j ] + X [ i , k ] * W [ j , k ] for i in range ( 1 ): for j in range ( 128 ): Z [ i , j ] = Y [ i , j ] + B [ j ] def lnumpy_relu0 ( X : np . ndarray , Y : np . ndarray ): for i in range ( 1 ): for j in range ( 128 ): Y [ i , j ] = np . maximum ( X [ i , j ], 0 ) def lnumpy_linear1 ( X : np . ndarray , W : np . ndarray , B : np . ndarray , Z : np . ndarray ): Y = np . empty (( 1 , 10 ), dtype = \"float32\" ) for i in range ( 1 ): for j in range ( 10 ): for k in range ( 128 ): if k == 0 : Y [ i , j ] = 0 Y [ i , j ] = Y [ i , j ] + X [ i , k ] * W [ j , k ] for i in range ( 1 ): for j in range ( 10 ): Z [ i , j ] = Y [ i , j ] + B [ j ] def lnumpy_mlp ( data , w0 , b0 , w1 , b1 ): lv0 = np . empty (( 1 , 128 ), dtype = \"float32\" ) lnumpy_linear0 ( data , w0 , b0 , lv0 ) lv1 = np . empty (( 1 , 128 ), dtype = \"float32\" ) lnumpy_relu0 ( lv0 , lv1 ) out = np . empty (( 1 , 10 ), dtype = \"float32\" ) lnumpy_linear1 ( lv1 , w1 , b1 , out ) return out result = lnumpy_mlp ( img . reshape ( 1 , 784 ), mlp_params [ \"w0\" ], mlp_params [ \"b0\" ], mlp_params [ \"w1\" ], mlp_params [ \"b1\" ]) pred_kind = result . argmax ( axis = 1 ) print ( \"Low-level Numpy MLP Prediction:\" , class_names [ pred_kind [ 0 ]])","title":"\u6a21\u578b\u7684\u5e95\u5c42Numpy\u5b9e\u73b0"},{"location":"ml-compilation-04/#tvmscriptirmodule","text":"@tvm . script . ir_module class MyModule : @T . prim_func def relu0 ( X : T . Buffer [( 1 , 128 ), \"float32\" ], Y : T . Buffer [( 1 , 128 ), \"float32\" ]): # function attr dict T . func_attr ({ \"global_symbol\" : \"relu0\" , \"tir.noalias\" : True }) for i , j in T . grid ( 1 , 128 ): with T . block ( \"Y\" ): vi , vj = T . axis . remap ( \"SS\" , [ i , j ]) Y [ vi , vj ] = T . max ( X [ vi , vj ], T . float32 ( 0 )) @T . prim_func def linear0 ( X : T . Buffer [( 1 , 784 ), \"float32\" ], W : T . Buffer [( 128 , 784 ), \"float32\" ], B : T . Buffer [( 128 ,), \"float32\" ], Z : T . Buffer [( 1 , 128 ), \"float32\" ]): T . func_attr ({ \"global_symbol\" : \"linear0\" , \"tir.noalias\" : True }) Y = T . alloc_buffer (( 1 , 128 ), \"float32\" ) for i , j , k in T . grid ( 1 , 128 , 784 ): with T . block ( \"Y\" ): vi , vj , vk = T . axis . remap ( \"SSR\" , [ i , j , k ]) with T . init (): Y [ vi , vj ] = T . float32 ( 0 ) Y [ vi , vj ] = Y [ vi , vj ] + X [ vi , vk ] * W [ vj , vk ] for i , j in T . grid ( 1 , 128 ): with T . block ( \"Z\" ): vi , vj = T . axis . remap ( \"SS\" , [ i , j ]) Z [ vi , vj ] = Y [ vi , vj ] + B [ vj ] @T . prim_func def linear1 ( X : T . Buffer [( 1 , 128 ), \"float32\" ], W : T . Buffer [( 10 , 128 ), \"float32\" ], B : T . Buffer [( 10 ,), \"float32\" ], Z : T . Buffer [( 1 , 10 ), \"float32\" ]): T . func_attr ({ \"global_symbol\" : \"linear1\" , \"tir.noalias\" : True }) Y = T . alloc_buffer (( 1 , 10 ), \"float32\" ) for i , j , k in T . grid ( 1 , 10 , 128 ): with T . block ( \"Y\" ): vi , vj , vk = T . axis . remap ( \"SSR\" , [ i , j , k ]) with T . init (): Y [ vi , vj ] = T . float32 ( 0 ) Y [ vi , vj ] = Y [ vi , vj ] + X [ vi , vk ] * W [ vj , vk ] for i , j in T . grid ( 1 , 10 ): with T . block ( \"Z\" ): vi , vj = T . axis . remap ( \"SS\" , [ i , j ]) Z [ vi , vj ] = Y [ vi , vj ] + B [ vj ] @R . function def main ( x : Tensor (( 1 , 784 ), \"float32\" ), w0 : Tensor (( 128 , 784 ), \"float32\" ), b0 : Tensor (( 128 ,), \"float32\" ), w1 : Tensor (( 10 , 128 ), \"float32\" ), b1 : Tensor (( 10 ,), \"float32\" )): with R . dataflow (): lv0 = R . call_tir ( linear0 , ( x , w0 , b0 ), ( 1 , 128 ), dtype = \"float32\" ) lv1 = R . call_tir ( relu0 , ( lv0 ,), ( 1 , 128 ), dtype = \"float32\" ) out = R . call_tir ( linear1 , ( lv1 , w1 , b1 ), ( 1 , 10 ), dtype = \"float32\" ) R . output ( out ) return out","title":"\u5728TVMScript\u4e2d\u6784\u5efa\u7aef\u5230\u7aefIRModule"},{"location":"ml-compilation-04/#_3","text":"\u8ba1\u7b97\u56fe\u662f\u7528\u6765\u8868\u793a\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u6a21\u578b\u5728\u8bad\u7ec3\u4e0e\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8ba1\u7b97\u903b\u8f91\u4e0e\u72b6\u6001\u7684\u5de5\u5177\u3002\u8ba1\u7b97\u6846\u67b6\u5728\u540e\u7aef\u4f1a\u5c06\u524d\u7aef\u8bed\u8a00\u6784\u5efa\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u524d\u5411\u8ba1\u7b97\u4e0e\u53cd\u5411\u68af\u5ea6\u8ba1\u7b97\u4ee5\u8ba1\u7b97\u56fe\u7684\u5f62\u5f0f\u6765\u8fdb\u884c\u8868\u793a\u3002\u8ba1\u7b97\u56fe\u7531\u57fa\u672c\u6570\u636e\u7ed3\u6784\uff1a\u5f20\u91cf(Tensor)\u548c\u57fa\u672c\u8fd0\u7b97\u5355\u5143\uff1a\u7b97\u5b50(Operator)\u6784\u6210\u3002\u5728\u8ba1\u7b97\u56fe\u4e2d\u901a\u5e38\u4f7f\u7528\u8282\u70b9\u6765\u8868\u793a\u7b97\u5b50\uff0c\u8282\u70b9\u95f4\u7684\u6709\u5411\u7ebf\u6bb5\u6765\u8868\u793a\u5f20\u91cf\u72b6\u6001\uff0c\u540c\u65f6\u4e5f\u63cf\u8ff0\u4e86\u8ba1\u7b97\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002 \u300a\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\uff1a\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u300b","title":"\u8ba1\u7b97\u56fe"},{"location":"ml-compilation-04/#call_tir","text":"\u8ba1\u7b97\u56fe\u4e2d\u7684\u6bcf\u4e2a\u64cd\u4f5c\u6b65\u9aa4\u90fd\u5305\u542b\u4e00\u4e2a R.call_tir \u64cd\u4f5c\u3002 \u8fd9\u662f\u5f15\u5165\u5143\u5f20\u91cf\u51fd\u6570\u7684\u8fc7\u7a0b\u3002 \u5177\u4f53\u6765\u8bf4\uff0ccall_tir \u63a5\u53d7\u4e00\u4e2a\u5143\u51fd\u6570 (prim_func) \u7684\u8f93\u5165\u5217\u8868\uff0c\u5e76\u5206\u914d\u4e00\u4e2a\u8f93\u51fa\u5f20\u91cfres\uff0c\u7136\u540e\u5c06\u8f93\u5165\u548c\u8f93\u51fa\u4f20\u9012\u7ed9prim_func\u3002 \u6267\u884c prim_func \u540e\uff0c\u7ed3\u679c\u4f1a\u586b\u5145\u5230 res \u4e2d\uff0c\u7136\u540e\u6211\u4eec\u53ef\u4ee5\u8fd4\u56de\u7ed3\u679c\u3002 \u8fd9\u4e2a\u60f3\u6cd5\u662f\u8f93\u5165\u548c\u8f93\u51fa\u5728\u5916\u90e8\u663e\u5f0f\u5206\u914d\u5e76\u4f20\u9012\u7ed9\u5e95\u5c42\u5143\u51fd\u6570\u3002","title":"call_tir"},{"location":"ml-compilation-04/#dataflow-block","text":"","title":"Dataflow Block"},{"location":"ml-compilation-04/#_4","text":"IPython . display . Code ( MyModule . script (), language = \"python\" ) \u6211\u4eec\u8c03\u7528 relax.vm.build \u6765\u6784\u5efa\u8fd9\u4e2a\u51fd\u6570\u3002 ex = relax . vm . build ( MyModule , target = \"llvm\" ) type ( ex ) build \u51fd\u6570\u4f1a\u7ed9\u6211\u4eec\u4e00\u4e2a\u53ef\u6267\u884c\u6587\u4ef6\u3002\u6211\u4eec\u53ef\u4ee5\u521d\u59cb\u5316\u4e00\u4e2a\u865a\u62df\u673a\u6267\u884c\u5668\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u8fd0\u884c\u8be5\u51fd\u6570\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u5c06\u4f20\u5165\u7b2c\u4e8c\u4e2a\u53c2\u6570\uff0c\u6307\u793a\u6211\u4eec\u8981\u5728\u54ea\u4e2a\u8bbe\u5907\u4e0a\u8fd0\u884c\u7aef\u5230\u7aef\u6267\u884c\u3002","title":"\u6784\u5efa\u5e76\u8fd0\u884c\u6a21\u578b"},{"location":"ml-compilation-04/#_5","text":"\u8ba1\u7b97\u56fe\u62bd\u8c61\u6709\u52a9\u4e8e\u5c06\u5143\u5f20\u91cf\u51fd\u6570\u62fc\u63a5\u5728\u4e00\u8d77\u4ee5\u8fdb\u884c\u7aef\u5230\u7aef\u6267\u884c\u3002 Relax \u62bd\u8c61\u7684\u5173\u952e\u8981\u7d20\u5305\u62ec call_tir \u6784\u9020\uff0c\u5c06\u76ee\u6807\u4f20\u9012\u89c4\u8303\u7684\u5143\u51fd\u6570\u5d4c\u5165\u5230\u8ba1\u7b97\u56fe\u4e2d Dataflow block \u8ba1\u7b97\u56fe\u5141\u8bb8\u8c03\u7528\u73af\u5883\u5e93\u51fd\u6570\u548c TensorIR \u51fd\u6570\u3002","title":"\u8ba8\u8bba\u548c\u603b\u7ed3"},{"location":"ml-compilation-05/","text":"05 \u81ea\u52a8\u7a0b\u5e8f\u4f18\u5316 \u00b6 \u53c2\u8003\u8d44\u6599 \u82f1\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22/ \u82f1\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/index.html \u4e2d\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22-zh/ \u4e2d\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/zh/index.html \u2b50\ufe0f\u672c\u8282\u4ee3\u7801\uff1a ipynb \u5728\u8fc7\u53bb\u7684\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u5b66\u4e60\u4e86\u5982\u4f55\u6784\u5efa\u5143\u5f20\u91cf\u51fd\u6570\u5e76\u5c06\u5b83\u4eec\u8fde\u63a5\u8d77\u6765\u4ee5\u8fdb\u884c\u7aef\u5230\u7aef\u7684\u6a21\u578b\u6267\u884c\u3002\u672c\u7ae0\u5c06\u8ba8\u8bba\u81ea\u52a8\u5316\u4e00\u4e9b\u6d41\u7a0b\u7684\u65b9\u6cd5\u3002 \u5386\u53f2\u8f68\u8ff9 (trace)\uff1a\u5305\u542b\u4e86 IRModule \u5728\u53d8\u6362\u8fc7\u7a0b\u4e2d\u6240\u6d89\u53ca\u7684\u6b65\u9aa4\u3002 \u968f\u673a\u8c03\u5ea6\u53d8\u6362 (Stochastic Schedule Transformation)\uff1a\u5728\u6211\u4eec\u7684\u53d8\u6362\u4e2d\u6dfb\u52a0\u4e00\u4e9b\u968f\u673a\u5143\u7d20\u3002 \u968f\u673a\u53d8\u6362\u641c\u7d22\uff1a\u4f7f\u7528\u968f\u673a\u53d8\u6362\u6765\u6307\u5b9a\u597d\u7684\u7a0b\u5e8f\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u4f7f\u7528 tune_tir API \u5e2e\u52a9\u5728\u641c\u7d22\u7a7a\u95f4\u5185\u641c\u7d22\u5e76\u627e\u5230\u6700\u4f18\u7684\u8c03\u5ea6\u53d8\u6362\u3002 \u81ea\u52a8\u8c03\u5ea6\uff1aMeta-Schedule \u5e26\u6709\u5185\u7f6e\u901a\u7528\u968f\u673a\u53d8\u6362\u96c6\u5408\uff0c\u80fd\u591f\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684 TensorIR \u8ba1\u7b97\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e5f\u79f0\u4e3a\u81ea\u52a8\u8c03\u5ea6 (auto-scheduling)\uff0c\u56e0\u4e3a\u641c\u7d22\u7a7a\u95f4\u662f\u7531\u7cfb\u7edf\u751f\u6210\u7684\u3002 \u4ece MLC \u7684\u89d2\u5ea6\u6765\u770b\uff0c\u81ea\u52a8\u641c\u7d22\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u6b65\u9aa4\uff0c\u6211\u4eec\u53ea\u9700\u8981\u7528\u8c03\u4f18\u7ed3\u679c\u63d0\u4f9b\u7684\u65b0\u7684\u5143\u5f20\u91cf\u51fd\u6570\u5b9e\u73b0\u66ff\u6362\u539f\u59cb\u7684\u5143\u5f20\u91cf\u51fd\u6570\u5b9e\u73b0\u3002","title":"05 \u81ea\u52a8\u7a0b\u5e8f\u4f18\u5316"},{"location":"ml-compilation-05/#05","text":"\u53c2\u8003\u8d44\u6599 \u82f1\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22/ \u82f1\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/index.html \u4e2d\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22-zh/ \u4e2d\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/zh/index.html \u2b50\ufe0f\u672c\u8282\u4ee3\u7801\uff1a ipynb \u5728\u8fc7\u53bb\u7684\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u5b66\u4e60\u4e86\u5982\u4f55\u6784\u5efa\u5143\u5f20\u91cf\u51fd\u6570\u5e76\u5c06\u5b83\u4eec\u8fde\u63a5\u8d77\u6765\u4ee5\u8fdb\u884c\u7aef\u5230\u7aef\u7684\u6a21\u578b\u6267\u884c\u3002\u672c\u7ae0\u5c06\u8ba8\u8bba\u81ea\u52a8\u5316\u4e00\u4e9b\u6d41\u7a0b\u7684\u65b9\u6cd5\u3002 \u5386\u53f2\u8f68\u8ff9 (trace)\uff1a\u5305\u542b\u4e86 IRModule \u5728\u53d8\u6362\u8fc7\u7a0b\u4e2d\u6240\u6d89\u53ca\u7684\u6b65\u9aa4\u3002 \u968f\u673a\u8c03\u5ea6\u53d8\u6362 (Stochastic Schedule Transformation)\uff1a\u5728\u6211\u4eec\u7684\u53d8\u6362\u4e2d\u6dfb\u52a0\u4e00\u4e9b\u968f\u673a\u5143\u7d20\u3002 \u968f\u673a\u53d8\u6362\u641c\u7d22\uff1a\u4f7f\u7528\u968f\u673a\u53d8\u6362\u6765\u6307\u5b9a\u597d\u7684\u7a0b\u5e8f\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u4f7f\u7528 tune_tir API \u5e2e\u52a9\u5728\u641c\u7d22\u7a7a\u95f4\u5185\u641c\u7d22\u5e76\u627e\u5230\u6700\u4f18\u7684\u8c03\u5ea6\u53d8\u6362\u3002 \u81ea\u52a8\u8c03\u5ea6\uff1aMeta-Schedule \u5e26\u6709\u5185\u7f6e\u901a\u7528\u968f\u673a\u53d8\u6362\u96c6\u5408\uff0c\u80fd\u591f\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684 TensorIR \u8ba1\u7b97\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e5f\u79f0\u4e3a\u81ea\u52a8\u8c03\u5ea6 (auto-scheduling)\uff0c\u56e0\u4e3a\u641c\u7d22\u7a7a\u95f4\u662f\u7531\u7cfb\u7edf\u751f\u6210\u7684\u3002 \u4ece MLC \u7684\u89d2\u5ea6\u6765\u770b\uff0c\u81ea\u52a8\u641c\u7d22\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u6b65\u9aa4\uff0c\u6211\u4eec\u53ea\u9700\u8981\u7528\u8c03\u4f18\u7ed3\u679c\u63d0\u4f9b\u7684\u65b0\u7684\u5143\u5f20\u91cf\u51fd\u6570\u5b9e\u73b0\u66ff\u6362\u539f\u59cb\u7684\u5143\u5f20\u91cf\u51fd\u6570\u5b9e\u73b0\u3002","title":"05 \u81ea\u52a8\u7a0b\u5e8f\u4f18\u5316"},{"location":"ml-compilation-06/","text":"06 \u4e0e\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7684\u6574\u5408 \u00b6 \u53c2\u8003\u8d44\u6599 \u82f1\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22/ \u82f1\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/index.html \u4e2d\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22-zh/ \u4e2d\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/zh/index.html \u2b50\ufe0f\u672c\u8282\u4ee3\u7801\uff1a \u672c\u7ae0\u5c06\u8ba8\u8bba\u5982\u4f55\u5c06\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4ece\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u5f15\u5165 MLC \u6d41\u7a0b\u3002","title":"06 \u4e0e\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7684\u6574\u5408"},{"location":"ml-compilation-06/#06","text":"\u53c2\u8003\u8d44\u6599 \u82f1\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22/ \u82f1\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/index.html \u4e2d\u6587\u8bfe\u7a0b\u4e3b\u9875 https://mlc.ai/summer22-zh/ \u4e2d\u6587\u8bfe\u7a0b\u6750\u6599 https://mlc.ai/zh/index.html \u2b50\ufe0f\u672c\u8282\u4ee3\u7801\uff1a \u672c\u7ae0\u5c06\u8ba8\u8bba\u5982\u4f55\u5c06\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4ece\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u5f15\u5165 MLC \u6d41\u7a0b\u3002","title":"06 \u4e0e\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7684\u6574\u5408"},{"location":"ml-index/","text":"Machine Learning \u00b6 CS224n\u81ea\u7136\u8bed\u8a00\u5904\u7406 \u57fa\u4e8etransformers\u7684NLP \u6df1\u5165\u6d45\u51faPyTorch","title":"\u76ee\u5f55"},{"location":"ml-index/#machine-learning","text":"CS224n\u81ea\u7136\u8bed\u8a00\u5904\u7406 \u57fa\u4e8etransformers\u7684NLP \u6df1\u5165\u6d45\u51faPyTorch","title":"Machine Learning"},{"location":"nand2tetris_part_1/","text":"Nand2Tetris Part1 (\u786c\u4ef6) \u00b6 \u4ecb\u7ecd \u00b6 \u8bfe\u7a0b\u5b98\u7f51\uff1a https://www.nand2tetris.org/ Nand2Tetris\uff08\u5168\u79f0\u300a\u4f9d\u636e\u57fa\u672c\u539f\u7406\u6784\u5efa\u73b0\u4ee3\u8ba1\u7b97\u673a\uff1a\u4ece\u4e0e\u975e\u95e8\u5230\u4fc4\u7f57\u65af\u65b9\u5757\u300b\uff09\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u786c\u4ef6\u548c\u8f6f\u4ef6\u3002 \u786c\u4ef6\uff08PART1\uff09\u7684\u8bfe\u7a0b\u4f5c\u4e1a\u5305\u62ec6\u4e2a\u9879\u76ee\uff08\u5176\u4e2dP4\u548cP6\u5c5e\u4e8e\u7f16\u7a0b\u9879\u76ee\uff0c\u5176\u4ed6\u90fd\u662f\u786c\u4ef6\u6784\u5efa\u9879\u76ee\uff09\uff1a \u9879\u76ee1\uff1a\u6784\u5efa\u521d\u7ea7\u903b\u8f91\u95e8\uff0c\u5982And, Or, Not, Multiplexor\u7b49 \u9879\u76ee2\uff1a\u6784\u5efa\u4e00\u4e2a\u52a0\u6cd5\u5668\uff08Full-Adder\uff09\u82af\u7247\u7cfb\u5217\uff0c\u6700\u7ec8\u6784\u5efa\u4e00\u4e2a\u7b97\u672f\u903b\u8f91\u5355\u5143\uff08ALU\uff09 \u9879\u76ee3\uff1a\u6784\u5efa\u5bc4\u5b58\u5668\uff08Register\uff09\u548c\u5b58\u50a8\u5355\u5143\uff08Memory\uff09\uff0c\u6700\u7ec8\u6784\u5efa\u4e00\u4e2a\u968f\u673a\u5b58\u53d6\u5b58\u50a8\u5668\uff08RAM\uff09 \u9879\u76ee4\uff1a\u5b66\u4e60\u673a\u5668\u8bed\u8a00\uff08Machine Langage\uff09\u5e76\u4f7f\u7528\u5b83\u6765\u7f16\u5199\u4e00\u4e9b\u8bf4\u660e\u6027\u7684\u4f4e\u7ea7\u7a0b\u5e8f\uff08\u5373\u6c47\u7f16\u8bed\u8a00\uff09 \u9879\u76ee5\uff1a\u4f7f\u7528\u9879\u76ee1-3\u4e2d\u5efa\u7acb\u7684\u82af\u7247\u7ec4\uff0c\u6784\u5efa\u4e00\u4e2a\u4e2d\u592e\u5904\u7406\u5668\uff08CPU\uff09\u548c\u4e00\u4e2a\u786c\u4ef6\u5e73\u53f0\uff08Computer\uff09\uff0c\u80fd\u591f\u6267\u884c\u9879\u76ee4\u4e2d\u4ecb\u7ecd\u7684\u673a\u5668\u8bed\u8a00\u7f16\u5199\u7684\u7a0b\u5e8f \u9879\u76ee6\uff1a\u5f00\u53d1\u4e00\u4e2a\u6c47\u7f16\uff08Assembler\uff09\u7a0b\u5e8f\uff0c\u628a\u7528\u7b26\u53f7\u673a\u5668\u8bed\u8a00\u7f16\u5199\u7684\u7a0b\u5e8f\u7ffb\u8bd1\u6210\u4e8c\u8fdb\u5236\u53ef\u6267\u884c\u4ee3\u7801 \u786c\u4ef6\u9879\u76ee\u7ed3\u6784\u5982\u56fe\uff1a \u7b2c1\u30012\u30013\u7ae0 \u5e03\u5c14\u903b\u8f91\u3001\u5e03\u5c14\u8fd0\u7b97\u3001\u65f6\u5e8f\u903b\u8f91 \u00b6 \u7b2c1-3\u7ae0\u4e3b\u8981\u662f\u4ecb\u7ecdALU\u548c\u5bc4\u5b58\u5668\u7684\u539f\u7406\u548c\u6784\u5efa\u3002 \u7b2c4\u7ae0 \u673a\u5668\u8bed\u8a00 \u00b6 \u673a\u5668\u8bed\u8a00\u3001\u6c47\u7f16 \u00b6 \u3010\u673a\u5668\u8bed\u8a00\u3011\u673a\u5668\u8bed\u8a00\u662f\u8ba1\u7b97\u673a\u76f4\u63a5\u8bc6\u522b\u7684**\u4e8c\u8fdb\u5236\u4ee3\u7801**\u3002\u673a\u5668\u8bed\u8a00\u662f\u786c\u4ef6\u548c\u8f6f\u4ef6\u76f8\u63a5\u7684\u4e2d\u95f4\u7ebf\u3002 \u3010\u7b26\u53f7\u8868\u793a\u3011\u3010\u52a9\u8bb0\u7b26\u3011\u7a0b\u5e8f\u5458\u7528\u7b26\u53f7\u6307\u4ee4\u8868\u8fbe\u7684\u62bd\u8c61\u601d\u7ef4\u88ab\u8f6c\u6362\u6210\u6267\u884c\u5728\u7845\u7247\u4e0a\u7684\u7269\u7406\u64cd\u4f5c\u3002 \u3010\u6307\u4ee4\u3011\u3010\u6307\u4ee4\u96c6\u3011\u673a\u5668\u8bed\u8a00\u7a0b\u5e8f\u662f\u4e00\u7cfb\u5217\u7684**\u7f16\u7801\u6307\u4ee4**\u3002\u56e0\u4e3a\u4e0d\u540c\u8ba1\u7b97\u673a\u5728CPU\u7684\u64cd\u4f5c\u65b9\u5f0f\u3001\u5bc4\u5b58\u5668\u6570\u91cf\u548c\u7c7b\u578b\u4ee5\u53ca\u6c47\u7f16\u8bed\u6cd5\u4e0a\u5404\u4e0d\u76f8\u540c\uff0c\u6240\u4ee5\u6709\u5404\u81ea\u7684\u8bed\u6cd5\uff0c\u7136\u800c\u6240\u6709\u7684\u673a\u5668\u8bed\u8a00\u90fd\u652f\u6301\u76f8\u4f3c\u7684**\u901a\u7528\u547d\u4ee4\u96c6\u5408**\u3002 \u3010\u6c47\u7f16\u8bed\u8a00/Assembly Language\u3011\u3010\u6c47\u7f16\u7f16\u8bd1\u5668/Assembler\u3011\u7b26\u53f7\u8868\u793a\u4e5f\u6210\u4e3a\u6c47\u7f16\u8bed\u8a00\uff0c\u5c06\u6c47\u7f16\u7a0b\u5e8f\u7ffb\u8bd1\u6210\u4e8c\u8fdb\u5236\u7801\u7684\u7a0b\u5e8f\u5219\u79f0\u4e3a\u6c47\u7f16\u7f16\u8bd1\u5668\u3002 \u5185\u5b58\u8bbf\u95ee\u547d\u4ee4\u3001\u5bfb\u5740\u65b9\u5f0f\u3001\u6d41\u7a0b\u63a7\u5236/\u5206\u652f \u00b6 \u3010\u5185\u5b58\u8bbf\u95ee\u547d\u4ee4\u3011 \u7b97\u672f\u548c\u903b\u8f91\u547d\u4ee4\uff1a\u4e0d\u4ec5\u5141\u8bb8\u64cd\u7eb5\u5bc4\u5b58\u5668\uff0c\u800c\u4e14\u8fd8\u53ef\u4ee5\u64cd\u7eb5\u7279\u5b9a\u7684\u5185\u5b58\u5355\u5143\u3002 load\u548cstore\u547d\u4ee4\uff1a\u7528\u6765\u5728\u5bc4\u5b58\u5668\u548c\u5185\u5b58\u4e4b\u95f4\u4f20\u9012\u6570\u636e\u3002 \u3010\u5bfb\u5740\u65b9\u5f0f\u3011 \u76f4\u63a5\u5bfb\u5740/Direct Addressing\uff1a\u76f4\u63a5\u8868\u793a\u4e00\u4e2a\u6307\u5b9a\u5185\u5b58\u5355\u5143\u7684\u5730\u5740\u3002 LOAD R1, 67 // R1 \u2190 Memory[67] LOAD R1, bar // R1 \u2190 Memory[67] \u5047\u8bbebar\u6307\u5411\u5185\u5b58\u5730\u574067 \u7acb\u5373\u5bfb\u5740/Immediate Addressing\uff1a\u88ab\u7528\u6765\u52a0\u8f7d\u5e38\u6570\uff0c\u52a0\u8f7d\u90a3\u4e9b\u51fa\u73b0\u5728\u6307\u4ee4\u4ee3\u7801\u91cc\u7684\u6570\u503c\uff0c\u76f4\u63a5\u5c06\u6570\u636e\u88c5\u5165\u5bc4\u5b58\u5668\uff0c\u800c\u4e0d\u662f\u5c06\u5176\u5f53\u4f5c\u5185\u5b58\u5355\u5143\u7684\u5730\u5740\u3002 LOADI R1, 67 // R1 \u2190 67 \u5c0667\u52a0\u8f7d\u5230\u5bc4\u5b58\u5668R1\u4e2d \u95f4\u63a5\u5bfb\u5740/Indirect Addressing\uff1a\u5730\u5740\u6ca1\u6709\u76f4\u63a5\u51fa\u73b0\u5728\u6307\u4ee4\u4e2d\uff0c\u800c\u662f\u6307\u5b9a\u5185\u5b58\u5355\u5143\u7684\u5185\u5bb9\u4ee3\u8868\u5730\u5740\u3002\u88ab\u7528\u6765\u5904\u7406\u6307\u9488\u3002 \uff08\u9ad8\u7ea7\u8bed\u8a00\uff09x=foo[j] or x=*(foo+j) // foo\u662f\u6570\u7ec4\u53d8\u91cf\uff0cx\u548cj\u662f\u6574\u6570\u53d8\u91cf\uff1b\u5f53\u6570\u7ec4foo\u5728\u9ad8\u7ea7\u8bed\u8a00\u7a0b\u5e8f\u91cc\u88ab\u58f0\u660e\u5e76\u521d\u59cb\u5316\u65f6\uff0c\u7f16\u8bd1\u5668\u5206\u914d\u4e00\u7ec4\u8fde\u7eed\u7684\u5185\u5b58\u5355\u5143\u6765\u4fdd\u5b58\u8fd9\u4e2a\u6570\u7ec4\u6570\u636e\uff0c\u5e76\u7528\u7b26\u53f7foo\u6765\u6307\u4ee3\u8be5\u5185\u5b58\u5355\u5143\u7ec4\u7684\u57fa\u5730\u5740\uff08base address\uff09\uff1b\u5f53\u7f16\u8bd1\u5668\u9047\u5230foo[j]\u65f6\uff0c\u8be5\u5730\u5740\u76f8\u5bf9\u4e8e\u6570\u7ec4\u57fa\u5730\u5740\u7684\u504f\u79fb\u91cf\u4e3aj\uff0c\u6240\u4ee5\u5728C\u4e2dx=foo[j] or x=*(foo+j)\u7b49\u4ef7\uff0c*n\u4ee3\u8868memory[n]\u3002 \uff08\u6c47\u7f16\u8bed\u8a00\uff09 ADD R1, foo, j // R1 \u2190 foo+j \u5c06\u5185\u5b58\u5730\u5740foo+j\u6dfb\u52a0\u5230R1\u4e2d LOAD* R2, R1 // R2 \u2190 Memory[R1] \u5c06R1\u8868\u793a\u7684\u5185\u5b58\u5730\u5740\u4e2d\u7684\u503c\u52a0\u8f7d\u5230R2\u4e2d STR R2, x // x \u2190 R2 \u5c06R2\u4e2d\u7684\u503c\u7528\u7b26\u53f7x\u8868\u793a \u3010\u63a7\u5236\u6d41\u7a0b\u3011\u3010\u5206\u652f\u3011 \u53cd\u590d\uff1a\u8df3\u56de\u5230\u5faa\u73af\u7684\u521d\u59cb\u4f4d\u7f6e \u6709\u6761\u4ef6\u6267\u884c\uff1a\u8df3\u5230\u524d\u9762\u7684\u4f4d\u7f6e\uff08\u5982if-then\uff09 \u5b50\u7a0b\u5e8f\u8c03\u7528\uff1a\u8df3\u5230\u53e6\u4e00\u4ee3\u7801\u6bb5\u7684\u7b2c\u4e00\u6761\u547d\u4ee4 \u65e0\u6761\u4ef6\u8df3\u8f6c\uff1a\u76f4\u63a5\u8df3\u8f6c Hack Language\u89c4\u8303\u548c\u5b9e\u73b0 \u00b6 \u3010\u5185\u5b58\u5730\u5740\u7a7a\u95f4\u3011\u5185\u5b58\u4e2d\u6709\u4e24\u79cd\u4e0d\u540c\u7684\u5730\u5740\u7a7a\u95f4\uff1a\u6307\u4ee4\u5730\u5740\u7a7a\u95f4\u548c\u6570\u636e\u5730\u5740\u7a7a\u95f4\uff0c\u7b80\u79f0\u6307\u4ee4\u5185\u5b58\u548c\u6570\u636e\u5185\u5b58\u3002CPU\u4ec5\u80fd\u6267\u884c\u5b58\u50a8\u5728\u6307\u4ee4\u5185\u5b58\u4e2d\u7684\u7a0b\u5e8f\u3002 \u3010\u5bc4\u5b58\u5668\u3011 D\uff1a\u6570\u636e\u5bc4\u5b58\u5668\uff0c\u50a8\u5b58\u6570\u636e\u503c\u3002 A\uff1a\u65e2\u53ef\u4ee5\u4f5c\u4e3a\u6570\u636e\u5bc4\u5b58\u5668\uff0c\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u5730\u5740\u5bc4\u5b58\u5668\u3002\u50a8\u5b58\u3010\u6570\u503c\u3011\u6216\u3010\u6570\u636e\u5185\u5b58\u4e2d\u7684\u5730\u5740\u3011\u6216\u3010\u6307\u4ee4\u5185\u5b58\u4e2d\u7684\u5730\u5740\u3011\u3002 M: RAM \u3010@value\u3011value\u53ef\u4ee5\u662f\u6570\u503c\u6216\u662f\u4ee3\u8868\u6570\u503c\u7684\u7b26\u53f7\uff0c\u8868\u793a\u5c06\u7279\u5b9a\u7684\u503c\u5b58\u5230A\u5bc4\u5b58\u5668\u4e2d\u3002 A\u6307\u4ee4 - \u5730\u5740\u6307\u4ee4 \u00b6 C\u6307\u4ee4 - \u8ba1\u7b97\u6307\u4ee4 \u00b6 dest = comp; jump \u7b2c5\u7ae0 \u8ba1\u7b97\u673a\u4f53\u7cfb\u7ed3\u6784 \u00b6 \u57fa\u672c\u7ed3\u6784\uff1a\u51af\u8bfa\u4f9d\u66fc\u67b6\u6784 \u00b6 CPU\u7535\u8def\u56fe \u00b6 CPU\u62bd\u8c61 \u00b6 \u8ba1\u7b97\u673a\u62bd\u8c61 \u00b6 \u7b2c6\u7ae0 \u6c47\u7f16\u7f16\u8bd1\u5668 \u00b6 \"\"\" input: Prog.asm (assembly code) output: Prog.hack (binary code) \"\"\" def Parser ( asmFile ): with open ( asmFile ) as f : asmLines = f . readlines () parsedLines = [] for line in asmLines : line = line . strip () if line : if not line . startswith ( \"//\" ): if \"//\" in line : line = line . split ( \"//\" )[ 0 ] line = line . strip () parsedLines . append ( line ) else : parsedLines . append ( line ) return parsedLines def symbolTable ( parsedLines ): symbolTable = { \"R0\" : \"0000000000000000\" , \"R1\" : \"0000000000000001\" , \"R2\" : \"0000000000000010\" , \"R3\" : \"0000000000000011\" , \"R4\" : \"0000000000000100\" , \"R5\" : \"0000000000000101\" , \"R6\" : \"0000000000000110\" , \"R7\" : \"0000000000000111\" , \"R8\" : \"0000000000001000\" , \"R9\" : \"0000000000001001\" , \"R10\" : \"0000000000001010\" , \"R11\" : \"0000000000001011\" , \"R12\" : \"0000000000001100\" , \"R13\" : \"0000000000001101\" , \"R14\" : \"0000000000001110\" , \"R15\" : \"0000000000001111\" , \"SP\" : \"0000000000000000\" , \"ARG\" : \"0000000000000010\" , \"LCL\" : \"0000000000000001\" , \"THIS\" : \"0000000000000011\" , \"THAT\" : \"0000000000000100\" , \"KBD\" : \"0110000000000000\" , \"SCREEN\" : \"0100000000000000\" } A_Instruction = lambda x : x . startswith ( '@' ) C_Instruction = lambda x : \"=\" in x or \";\" in x L_Instruction = lambda x : x . startswith ( \"(\" ) and x . endswith ( \")\" ) L_Value = lambda x : x . replace ( \"(\" , \"\" ) . replace ( \")\" , \"\" ) . strip () lineNum = 0 for line in parsedLines : if A_Instruction ( line ) or C_Instruction ( line ): lineNum += 1 elif L_Instruction ( line ): binaryLineNum = bin ( lineNum )[ 2 :] . zfill ( 16 ) symbolTable [ L_Value ( line )] = binaryLineNum baseAddress = 16 for line in parsedLines : if A_Instruction : value = line [ 1 :] if value not in symbolTable : valueBinary = bin ( lineNum )[ 2 :] . zfill ( 16 ) symbolTable [ value ] = valueBinary baseAddress += 1 return symbolTable def Code ( line , symbolTable ): COMPUTATIONS = { \"0\" : \"0101010\" , \"1\" : \"0111111\" , \"-1\" : \"0111010\" , \"D\" : \"0001100\" , \"A\" : \"0110000\" , \"!D\" : \"0001101\" , \"!A\" : \"0110001\" , \"-D\" : \"0001111\" , \"-A\" : \"0110011\" , \"D+1\" : \"0011111\" , \"A+1\" : \"0110111\" , \"D-1\" : \"0001110\" , \"A-1\" : \"0110010\" , \"D+A\" : \"0000010\" , \"D-A\" : \"0010011\" , \"A-D\" : \"0000111\" , \"D&A\" : \"0000000\" , \"D|A\" : \"0010101\" , \"M\" : \"1110000\" , \"!M\" : \"1110001\" , \"-M\" : \"1110011\" , \"M+1\" : \"1110111\" , \"M-1\" : \"1110010\" , \"D+M\" : \"1000010\" , \"D-M\" : \"1010011\" , \"M-D\" : \"1000111\" , \"D&M\" : \"1000000\" , \"D|M\" : \"1010101\" } DESTINATIONS = { \"\" : \"000\" , \"M\" : \"001\" , \"D\" : \"010\" , \"MD\" : \"011\" , \"A\" : \"100\" , \"AM\" : \"101\" , \"AD\" : \"110\" , \"AMD\" : \"111\" } JUMPS = { \"\" : \"000\" , \"JGT\" : \"001\" , \"JEQ\" : \"010\" , \"JGE\" : \"011\" , \"JLT\" : \"100\" , \"JNE\" : \"101\" , \"JLE\" : \"110\" , \"JMP\" : \"111\" } if line . startswith ( \"(\" ) and line . endswith ( \")\" ): return if line . startswith ( \"@\" ): value = line [ 1 :] if value in symbolTable : return symbolTable [ value ] valueBinary = bin ( value )[ 2 :] . zfill ( 16 ) return valueBinary dest , jump = \"\" , \"\" comp = line . split ( \"=\" ) . pop () . split ( ';' )[ 0 ] # dest=comp;jump if \"=\" in line : dest = line . split ( \"=\" )[ 0 ] if \";\" in line : jump = line . split ( \";\" ) . pop () compBinary = COMPUTATIONS . get ( comp , '0000000' ) destBinary = DESTINATIONS . get ( dest , '000' ) jumpBinary = JUMPS . get ( jump , '000' ) combinedBinaryCode = '111' + compBinary + destBinary + jumpBinary return combinedBinaryCode if __name__ == \"__main__\" : parsedLines = Parser ( \"Max.asm\" ) buildSymbolTable = symbolTable ( parsedLines ) generatedBinaryCode = [ Code ( line , buildSymbolTable ) for line in parsedLines ] hackLines = \" \\n \" . join ([ line for line in generatedBinaryCode if line ]) with open ( 'Max.hack' , \"w\" ) as f : f . write ( hackLines )","title":"Nand2Tetris Part1 (Hardware)"},{"location":"nand2tetris_part_1/#nand2tetris-part1","text":"","title":"Nand2Tetris Part1 (\u786c\u4ef6)"},{"location":"nand2tetris_part_1/#_1","text":"\u8bfe\u7a0b\u5b98\u7f51\uff1a https://www.nand2tetris.org/ Nand2Tetris\uff08\u5168\u79f0\u300a\u4f9d\u636e\u57fa\u672c\u539f\u7406\u6784\u5efa\u73b0\u4ee3\u8ba1\u7b97\u673a\uff1a\u4ece\u4e0e\u975e\u95e8\u5230\u4fc4\u7f57\u65af\u65b9\u5757\u300b\uff09\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u786c\u4ef6\u548c\u8f6f\u4ef6\u3002 \u786c\u4ef6\uff08PART1\uff09\u7684\u8bfe\u7a0b\u4f5c\u4e1a\u5305\u62ec6\u4e2a\u9879\u76ee\uff08\u5176\u4e2dP4\u548cP6\u5c5e\u4e8e\u7f16\u7a0b\u9879\u76ee\uff0c\u5176\u4ed6\u90fd\u662f\u786c\u4ef6\u6784\u5efa\u9879\u76ee\uff09\uff1a \u9879\u76ee1\uff1a\u6784\u5efa\u521d\u7ea7\u903b\u8f91\u95e8\uff0c\u5982And, Or, Not, Multiplexor\u7b49 \u9879\u76ee2\uff1a\u6784\u5efa\u4e00\u4e2a\u52a0\u6cd5\u5668\uff08Full-Adder\uff09\u82af\u7247\u7cfb\u5217\uff0c\u6700\u7ec8\u6784\u5efa\u4e00\u4e2a\u7b97\u672f\u903b\u8f91\u5355\u5143\uff08ALU\uff09 \u9879\u76ee3\uff1a\u6784\u5efa\u5bc4\u5b58\u5668\uff08Register\uff09\u548c\u5b58\u50a8\u5355\u5143\uff08Memory\uff09\uff0c\u6700\u7ec8\u6784\u5efa\u4e00\u4e2a\u968f\u673a\u5b58\u53d6\u5b58\u50a8\u5668\uff08RAM\uff09 \u9879\u76ee4\uff1a\u5b66\u4e60\u673a\u5668\u8bed\u8a00\uff08Machine Langage\uff09\u5e76\u4f7f\u7528\u5b83\u6765\u7f16\u5199\u4e00\u4e9b\u8bf4\u660e\u6027\u7684\u4f4e\u7ea7\u7a0b\u5e8f\uff08\u5373\u6c47\u7f16\u8bed\u8a00\uff09 \u9879\u76ee5\uff1a\u4f7f\u7528\u9879\u76ee1-3\u4e2d\u5efa\u7acb\u7684\u82af\u7247\u7ec4\uff0c\u6784\u5efa\u4e00\u4e2a\u4e2d\u592e\u5904\u7406\u5668\uff08CPU\uff09\u548c\u4e00\u4e2a\u786c\u4ef6\u5e73\u53f0\uff08Computer\uff09\uff0c\u80fd\u591f\u6267\u884c\u9879\u76ee4\u4e2d\u4ecb\u7ecd\u7684\u673a\u5668\u8bed\u8a00\u7f16\u5199\u7684\u7a0b\u5e8f \u9879\u76ee6\uff1a\u5f00\u53d1\u4e00\u4e2a\u6c47\u7f16\uff08Assembler\uff09\u7a0b\u5e8f\uff0c\u628a\u7528\u7b26\u53f7\u673a\u5668\u8bed\u8a00\u7f16\u5199\u7684\u7a0b\u5e8f\u7ffb\u8bd1\u6210\u4e8c\u8fdb\u5236\u53ef\u6267\u884c\u4ee3\u7801 \u786c\u4ef6\u9879\u76ee\u7ed3\u6784\u5982\u56fe\uff1a","title":"\u4ecb\u7ecd"},{"location":"nand2tetris_part_1/#123","text":"\u7b2c1-3\u7ae0\u4e3b\u8981\u662f\u4ecb\u7ecdALU\u548c\u5bc4\u5b58\u5668\u7684\u539f\u7406\u548c\u6784\u5efa\u3002","title":"\u7b2c1\u30012\u30013\u7ae0 \u5e03\u5c14\u903b\u8f91\u3001\u5e03\u5c14\u8fd0\u7b97\u3001\u65f6\u5e8f\u903b\u8f91"},{"location":"nand2tetris_part_1/#4","text":"","title":"\u7b2c4\u7ae0 \u673a\u5668\u8bed\u8a00"},{"location":"nand2tetris_part_1/#_2","text":"\u3010\u673a\u5668\u8bed\u8a00\u3011\u673a\u5668\u8bed\u8a00\u662f\u8ba1\u7b97\u673a\u76f4\u63a5\u8bc6\u522b\u7684**\u4e8c\u8fdb\u5236\u4ee3\u7801**\u3002\u673a\u5668\u8bed\u8a00\u662f\u786c\u4ef6\u548c\u8f6f\u4ef6\u76f8\u63a5\u7684\u4e2d\u95f4\u7ebf\u3002 \u3010\u7b26\u53f7\u8868\u793a\u3011\u3010\u52a9\u8bb0\u7b26\u3011\u7a0b\u5e8f\u5458\u7528\u7b26\u53f7\u6307\u4ee4\u8868\u8fbe\u7684\u62bd\u8c61\u601d\u7ef4\u88ab\u8f6c\u6362\u6210\u6267\u884c\u5728\u7845\u7247\u4e0a\u7684\u7269\u7406\u64cd\u4f5c\u3002 \u3010\u6307\u4ee4\u3011\u3010\u6307\u4ee4\u96c6\u3011\u673a\u5668\u8bed\u8a00\u7a0b\u5e8f\u662f\u4e00\u7cfb\u5217\u7684**\u7f16\u7801\u6307\u4ee4**\u3002\u56e0\u4e3a\u4e0d\u540c\u8ba1\u7b97\u673a\u5728CPU\u7684\u64cd\u4f5c\u65b9\u5f0f\u3001\u5bc4\u5b58\u5668\u6570\u91cf\u548c\u7c7b\u578b\u4ee5\u53ca\u6c47\u7f16\u8bed\u6cd5\u4e0a\u5404\u4e0d\u76f8\u540c\uff0c\u6240\u4ee5\u6709\u5404\u81ea\u7684\u8bed\u6cd5\uff0c\u7136\u800c\u6240\u6709\u7684\u673a\u5668\u8bed\u8a00\u90fd\u652f\u6301\u76f8\u4f3c\u7684**\u901a\u7528\u547d\u4ee4\u96c6\u5408**\u3002 \u3010\u6c47\u7f16\u8bed\u8a00/Assembly Language\u3011\u3010\u6c47\u7f16\u7f16\u8bd1\u5668/Assembler\u3011\u7b26\u53f7\u8868\u793a\u4e5f\u6210\u4e3a\u6c47\u7f16\u8bed\u8a00\uff0c\u5c06\u6c47\u7f16\u7a0b\u5e8f\u7ffb\u8bd1\u6210\u4e8c\u8fdb\u5236\u7801\u7684\u7a0b\u5e8f\u5219\u79f0\u4e3a\u6c47\u7f16\u7f16\u8bd1\u5668\u3002","title":"\u673a\u5668\u8bed\u8a00\u3001\u6c47\u7f16"},{"location":"nand2tetris_part_1/#_3","text":"\u3010\u5185\u5b58\u8bbf\u95ee\u547d\u4ee4\u3011 \u7b97\u672f\u548c\u903b\u8f91\u547d\u4ee4\uff1a\u4e0d\u4ec5\u5141\u8bb8\u64cd\u7eb5\u5bc4\u5b58\u5668\uff0c\u800c\u4e14\u8fd8\u53ef\u4ee5\u64cd\u7eb5\u7279\u5b9a\u7684\u5185\u5b58\u5355\u5143\u3002 load\u548cstore\u547d\u4ee4\uff1a\u7528\u6765\u5728\u5bc4\u5b58\u5668\u548c\u5185\u5b58\u4e4b\u95f4\u4f20\u9012\u6570\u636e\u3002 \u3010\u5bfb\u5740\u65b9\u5f0f\u3011 \u76f4\u63a5\u5bfb\u5740/Direct Addressing\uff1a\u76f4\u63a5\u8868\u793a\u4e00\u4e2a\u6307\u5b9a\u5185\u5b58\u5355\u5143\u7684\u5730\u5740\u3002 LOAD R1, 67 // R1 \u2190 Memory[67] LOAD R1, bar // R1 \u2190 Memory[67] \u5047\u8bbebar\u6307\u5411\u5185\u5b58\u5730\u574067 \u7acb\u5373\u5bfb\u5740/Immediate Addressing\uff1a\u88ab\u7528\u6765\u52a0\u8f7d\u5e38\u6570\uff0c\u52a0\u8f7d\u90a3\u4e9b\u51fa\u73b0\u5728\u6307\u4ee4\u4ee3\u7801\u91cc\u7684\u6570\u503c\uff0c\u76f4\u63a5\u5c06\u6570\u636e\u88c5\u5165\u5bc4\u5b58\u5668\uff0c\u800c\u4e0d\u662f\u5c06\u5176\u5f53\u4f5c\u5185\u5b58\u5355\u5143\u7684\u5730\u5740\u3002 LOADI R1, 67 // R1 \u2190 67 \u5c0667\u52a0\u8f7d\u5230\u5bc4\u5b58\u5668R1\u4e2d \u95f4\u63a5\u5bfb\u5740/Indirect Addressing\uff1a\u5730\u5740\u6ca1\u6709\u76f4\u63a5\u51fa\u73b0\u5728\u6307\u4ee4\u4e2d\uff0c\u800c\u662f\u6307\u5b9a\u5185\u5b58\u5355\u5143\u7684\u5185\u5bb9\u4ee3\u8868\u5730\u5740\u3002\u88ab\u7528\u6765\u5904\u7406\u6307\u9488\u3002 \uff08\u9ad8\u7ea7\u8bed\u8a00\uff09x=foo[j] or x=*(foo+j) // foo\u662f\u6570\u7ec4\u53d8\u91cf\uff0cx\u548cj\u662f\u6574\u6570\u53d8\u91cf\uff1b\u5f53\u6570\u7ec4foo\u5728\u9ad8\u7ea7\u8bed\u8a00\u7a0b\u5e8f\u91cc\u88ab\u58f0\u660e\u5e76\u521d\u59cb\u5316\u65f6\uff0c\u7f16\u8bd1\u5668\u5206\u914d\u4e00\u7ec4\u8fde\u7eed\u7684\u5185\u5b58\u5355\u5143\u6765\u4fdd\u5b58\u8fd9\u4e2a\u6570\u7ec4\u6570\u636e\uff0c\u5e76\u7528\u7b26\u53f7foo\u6765\u6307\u4ee3\u8be5\u5185\u5b58\u5355\u5143\u7ec4\u7684\u57fa\u5730\u5740\uff08base address\uff09\uff1b\u5f53\u7f16\u8bd1\u5668\u9047\u5230foo[j]\u65f6\uff0c\u8be5\u5730\u5740\u76f8\u5bf9\u4e8e\u6570\u7ec4\u57fa\u5730\u5740\u7684\u504f\u79fb\u91cf\u4e3aj\uff0c\u6240\u4ee5\u5728C\u4e2dx=foo[j] or x=*(foo+j)\u7b49\u4ef7\uff0c*n\u4ee3\u8868memory[n]\u3002 \uff08\u6c47\u7f16\u8bed\u8a00\uff09 ADD R1, foo, j // R1 \u2190 foo+j \u5c06\u5185\u5b58\u5730\u5740foo+j\u6dfb\u52a0\u5230R1\u4e2d LOAD* R2, R1 // R2 \u2190 Memory[R1] \u5c06R1\u8868\u793a\u7684\u5185\u5b58\u5730\u5740\u4e2d\u7684\u503c\u52a0\u8f7d\u5230R2\u4e2d STR R2, x // x \u2190 R2 \u5c06R2\u4e2d\u7684\u503c\u7528\u7b26\u53f7x\u8868\u793a \u3010\u63a7\u5236\u6d41\u7a0b\u3011\u3010\u5206\u652f\u3011 \u53cd\u590d\uff1a\u8df3\u56de\u5230\u5faa\u73af\u7684\u521d\u59cb\u4f4d\u7f6e \u6709\u6761\u4ef6\u6267\u884c\uff1a\u8df3\u5230\u524d\u9762\u7684\u4f4d\u7f6e\uff08\u5982if-then\uff09 \u5b50\u7a0b\u5e8f\u8c03\u7528\uff1a\u8df3\u5230\u53e6\u4e00\u4ee3\u7801\u6bb5\u7684\u7b2c\u4e00\u6761\u547d\u4ee4 \u65e0\u6761\u4ef6\u8df3\u8f6c\uff1a\u76f4\u63a5\u8df3\u8f6c","title":"\u5185\u5b58\u8bbf\u95ee\u547d\u4ee4\u3001\u5bfb\u5740\u65b9\u5f0f\u3001\u6d41\u7a0b\u63a7\u5236/\u5206\u652f"},{"location":"nand2tetris_part_1/#hack-language","text":"\u3010\u5185\u5b58\u5730\u5740\u7a7a\u95f4\u3011\u5185\u5b58\u4e2d\u6709\u4e24\u79cd\u4e0d\u540c\u7684\u5730\u5740\u7a7a\u95f4\uff1a\u6307\u4ee4\u5730\u5740\u7a7a\u95f4\u548c\u6570\u636e\u5730\u5740\u7a7a\u95f4\uff0c\u7b80\u79f0\u6307\u4ee4\u5185\u5b58\u548c\u6570\u636e\u5185\u5b58\u3002CPU\u4ec5\u80fd\u6267\u884c\u5b58\u50a8\u5728\u6307\u4ee4\u5185\u5b58\u4e2d\u7684\u7a0b\u5e8f\u3002 \u3010\u5bc4\u5b58\u5668\u3011 D\uff1a\u6570\u636e\u5bc4\u5b58\u5668\uff0c\u50a8\u5b58\u6570\u636e\u503c\u3002 A\uff1a\u65e2\u53ef\u4ee5\u4f5c\u4e3a\u6570\u636e\u5bc4\u5b58\u5668\uff0c\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u5730\u5740\u5bc4\u5b58\u5668\u3002\u50a8\u5b58\u3010\u6570\u503c\u3011\u6216\u3010\u6570\u636e\u5185\u5b58\u4e2d\u7684\u5730\u5740\u3011\u6216\u3010\u6307\u4ee4\u5185\u5b58\u4e2d\u7684\u5730\u5740\u3011\u3002 M: RAM \u3010@value\u3011value\u53ef\u4ee5\u662f\u6570\u503c\u6216\u662f\u4ee3\u8868\u6570\u503c\u7684\u7b26\u53f7\uff0c\u8868\u793a\u5c06\u7279\u5b9a\u7684\u503c\u5b58\u5230A\u5bc4\u5b58\u5668\u4e2d\u3002","title":"Hack Language\u89c4\u8303\u548c\u5b9e\u73b0"},{"location":"nand2tetris_part_1/#a-","text":"","title":"A\u6307\u4ee4 - \u5730\u5740\u6307\u4ee4"},{"location":"nand2tetris_part_1/#c-","text":"dest = comp; jump","title":"C\u6307\u4ee4 - \u8ba1\u7b97\u6307\u4ee4"},{"location":"nand2tetris_part_1/#5","text":"","title":"\u7b2c5\u7ae0 \u8ba1\u7b97\u673a\u4f53\u7cfb\u7ed3\u6784"},{"location":"nand2tetris_part_1/#_4","text":"","title":"\u57fa\u672c\u7ed3\u6784\uff1a\u51af\u8bfa\u4f9d\u66fc\u67b6\u6784"},{"location":"nand2tetris_part_1/#cpu","text":"","title":"CPU\u7535\u8def\u56fe"},{"location":"nand2tetris_part_1/#cpu_1","text":"","title":"CPU\u62bd\u8c61"},{"location":"nand2tetris_part_1/#_5","text":"","title":"\u8ba1\u7b97\u673a\u62bd\u8c61"},{"location":"nand2tetris_part_1/#6","text":"\"\"\" input: Prog.asm (assembly code) output: Prog.hack (binary code) \"\"\" def Parser ( asmFile ): with open ( asmFile ) as f : asmLines = f . readlines () parsedLines = [] for line in asmLines : line = line . strip () if line : if not line . startswith ( \"//\" ): if \"//\" in line : line = line . split ( \"//\" )[ 0 ] line = line . strip () parsedLines . append ( line ) else : parsedLines . append ( line ) return parsedLines def symbolTable ( parsedLines ): symbolTable = { \"R0\" : \"0000000000000000\" , \"R1\" : \"0000000000000001\" , \"R2\" : \"0000000000000010\" , \"R3\" : \"0000000000000011\" , \"R4\" : \"0000000000000100\" , \"R5\" : \"0000000000000101\" , \"R6\" : \"0000000000000110\" , \"R7\" : \"0000000000000111\" , \"R8\" : \"0000000000001000\" , \"R9\" : \"0000000000001001\" , \"R10\" : \"0000000000001010\" , \"R11\" : \"0000000000001011\" , \"R12\" : \"0000000000001100\" , \"R13\" : \"0000000000001101\" , \"R14\" : \"0000000000001110\" , \"R15\" : \"0000000000001111\" , \"SP\" : \"0000000000000000\" , \"ARG\" : \"0000000000000010\" , \"LCL\" : \"0000000000000001\" , \"THIS\" : \"0000000000000011\" , \"THAT\" : \"0000000000000100\" , \"KBD\" : \"0110000000000000\" , \"SCREEN\" : \"0100000000000000\" } A_Instruction = lambda x : x . startswith ( '@' ) C_Instruction = lambda x : \"=\" in x or \";\" in x L_Instruction = lambda x : x . startswith ( \"(\" ) and x . endswith ( \")\" ) L_Value = lambda x : x . replace ( \"(\" , \"\" ) . replace ( \")\" , \"\" ) . strip () lineNum = 0 for line in parsedLines : if A_Instruction ( line ) or C_Instruction ( line ): lineNum += 1 elif L_Instruction ( line ): binaryLineNum = bin ( lineNum )[ 2 :] . zfill ( 16 ) symbolTable [ L_Value ( line )] = binaryLineNum baseAddress = 16 for line in parsedLines : if A_Instruction : value = line [ 1 :] if value not in symbolTable : valueBinary = bin ( lineNum )[ 2 :] . zfill ( 16 ) symbolTable [ value ] = valueBinary baseAddress += 1 return symbolTable def Code ( line , symbolTable ): COMPUTATIONS = { \"0\" : \"0101010\" , \"1\" : \"0111111\" , \"-1\" : \"0111010\" , \"D\" : \"0001100\" , \"A\" : \"0110000\" , \"!D\" : \"0001101\" , \"!A\" : \"0110001\" , \"-D\" : \"0001111\" , \"-A\" : \"0110011\" , \"D+1\" : \"0011111\" , \"A+1\" : \"0110111\" , \"D-1\" : \"0001110\" , \"A-1\" : \"0110010\" , \"D+A\" : \"0000010\" , \"D-A\" : \"0010011\" , \"A-D\" : \"0000111\" , \"D&A\" : \"0000000\" , \"D|A\" : \"0010101\" , \"M\" : \"1110000\" , \"!M\" : \"1110001\" , \"-M\" : \"1110011\" , \"M+1\" : \"1110111\" , \"M-1\" : \"1110010\" , \"D+M\" : \"1000010\" , \"D-M\" : \"1010011\" , \"M-D\" : \"1000111\" , \"D&M\" : \"1000000\" , \"D|M\" : \"1010101\" } DESTINATIONS = { \"\" : \"000\" , \"M\" : \"001\" , \"D\" : \"010\" , \"MD\" : \"011\" , \"A\" : \"100\" , \"AM\" : \"101\" , \"AD\" : \"110\" , \"AMD\" : \"111\" } JUMPS = { \"\" : \"000\" , \"JGT\" : \"001\" , \"JEQ\" : \"010\" , \"JGE\" : \"011\" , \"JLT\" : \"100\" , \"JNE\" : \"101\" , \"JLE\" : \"110\" , \"JMP\" : \"111\" } if line . startswith ( \"(\" ) and line . endswith ( \")\" ): return if line . startswith ( \"@\" ): value = line [ 1 :] if value in symbolTable : return symbolTable [ value ] valueBinary = bin ( value )[ 2 :] . zfill ( 16 ) return valueBinary dest , jump = \"\" , \"\" comp = line . split ( \"=\" ) . pop () . split ( ';' )[ 0 ] # dest=comp;jump if \"=\" in line : dest = line . split ( \"=\" )[ 0 ] if \";\" in line : jump = line . split ( \";\" ) . pop () compBinary = COMPUTATIONS . get ( comp , '0000000' ) destBinary = DESTINATIONS . get ( dest , '000' ) jumpBinary = JUMPS . get ( jump , '000' ) combinedBinaryCode = '111' + compBinary + destBinary + jumpBinary return combinedBinaryCode if __name__ == \"__main__\" : parsedLines = Parser ( \"Max.asm\" ) buildSymbolTable = symbolTable ( parsedLines ) generatedBinaryCode = [ Code ( line , buildSymbolTable ) for line in parsedLines ] hackLines = \" \\n \" . join ([ line for line in generatedBinaryCode if line ]) with open ( 'Max.hack' , \"w\" ) as f : f . write ( hackLines )","title":"\u7b2c6\u7ae0 \u6c47\u7f16\u7f16\u8bd1\u5668"},{"location":"nand2tetris_part_2/","text":"Nand2Tetris Part1 (\u8f6f\u4ef6) \u00b6 \u4ecb\u7ecd \u00b6 \u8bfe\u7a0b\u5b98\u7f51\uff1a https://www.nand2tetris.org/ Nand2Tetris\uff08\u5168\u79f0\u300a\u4f9d\u636e\u57fa\u672c\u539f\u7406\u6784\u5efa\u73b0\u4ee3\u8ba1\u7b97\u673a\uff1a\u4ece\u4e0e\u975e\u95e8\u5230\u4fc4\u7f57\u65af\u65b9\u5757\u300b\uff09\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u786c\u4ef6\u548c\u8f6f\u4ef6\u3002\u8f6f\u4ef6\u90e8\u5206\u5982\u56fe\uff1a \u8f6f\u4ef6\uff08PART2\uff09\u7684\u8bfe\u7a0b\u4f5c\u4e1a\u5305\u62ec6\u4e2a\u9879\u76ee\uff1a \u9879\u76ee7\uff1a\u6784\u5efa\u865a\u62df\u673a\u8bed\u8a00\u5230\u6c47\u7f16\u8bed\u8a00\u7684\u7ffb\u8bd1\u5668\uff08VM Translator\uff09\uff0c\u5b9e\u73b0\u5806\u6808\u8fd0\u7b97\u548c\u5185\u5b58\u8bbf\u95ee \u9879\u76ee8\uff1a\u6784\u5efa\u865a\u62df\u673a\u8bed\u8a00\u5230\u6c47\u7f16\u8bed\u8a00\u7684\u7ffb\u8bd1\u5668\uff08VM Translator\uff09\uff0c\u5b9e\u73b0\u7a0b\u5e8f\u63a7\u5236\u548c\u51fd\u6570\u8c03\u7528 \u9879\u76ee9\uff1a\u7528\u9ad8\u7ea7\u8bed\u8a00Jack\u7f16\u5199\u7a0b\u5e8f \u9879\u76ee10\uff1a\u6784\u5efa\u9ad8\u7ea7\u8bed\u8a00\u5230\u865a\u62df\u673a\u8bed\u8a00\u7684\u7f16\u8bd1\u5668\uff08Compiler\uff09\uff0c \u9879\u76ee11\uff1a\u6784\u5efa\u9ad8\u7ea7\u8bed\u8a00\u5230\u865a\u62df\u673a\u8bed\u8a00\u7684\u7f16\u8bd1\u5668\uff08Compiler\uff09\uff0c \u9879\u76ee12\uff1a\u64cd\u4f5c\u7cfb\u7edf\u62bd\u8c61 \u7b2c7\u7ae0\u3001\u7b2c8\u7ae0 \u865a\u62df\u673a \u00b6 \u4ece\u9ad8\u7ea7\u7f16\u7a0b\u8bed\u8a00\u5230\u4e8c\u8fdb\u5236\u4ee3\u7801\uff1aJack\u2192VM\u2192Hack(ASM)\u2192Hack(Binary) \u00b6 \u7b2c9\u7ae0 \u9ad8\u7ea7\u8bed\u8a00 \u00b6 \u4f5c\u8005\u8bbe\u8ba1\u7684\u9ad8\u7ea7\u8bed\u8a00Jack\uff0c\u8bed\u6cd5\u548cJava\uff0cC#\u7c7b\u4f3c\u3002 \u7b2c10\u7ae0\u3001\u7b2c11\u7ae0 \u7f16\u8bd1\u5668 \u00b6 \u7b2c12\u7ae0 \u64cd\u4f5c\u7cfb\u7edf \u00b6 \u64cd\u4f5c\u7cfb\u7edf\u7528\u6765\u8854\u63a5\u8ba1\u7b97\u673a\u786c\u4ef6\u548c\u8f6f\u4ef6\uff0c\u4e3b\u8981\u4f5c\u7528\u5305\u62ec\uff1a\uff081\uff09\u5c01\u88c5\u4e0d\u540c\u7684\u786c\u4ef6\u670d\u52a1\uff082\uff09\u7528\u4e0d\u540c\u7684\u51fd\u6570\u548c\u6570\u636e\u7c7b\u578b\u6269\u5c55\u4e86\u9ad8\u7ea7\u8bed\u8a00\u3002\u64cd\u4f5c\u7cfb\u7edf\u901a\u5e38\u7531\u9ad8\u7ea7\u8bed\u8a00\u7f16\u5199\uff0c\u5e76\u88ab\u7f16\u8bd1\u6210\u4e8c\u8fdb\u5236\u5f62\u5f0f\u3002","title":"Nand2Tetris Part2 (Software)"},{"location":"nand2tetris_part_2/#nand2tetris-part1","text":"","title":"Nand2Tetris Part1 (\u8f6f\u4ef6)"},{"location":"nand2tetris_part_2/#_1","text":"\u8bfe\u7a0b\u5b98\u7f51\uff1a https://www.nand2tetris.org/ Nand2Tetris\uff08\u5168\u79f0\u300a\u4f9d\u636e\u57fa\u672c\u539f\u7406\u6784\u5efa\u73b0\u4ee3\u8ba1\u7b97\u673a\uff1a\u4ece\u4e0e\u975e\u95e8\u5230\u4fc4\u7f57\u65af\u65b9\u5757\u300b\uff09\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u786c\u4ef6\u548c\u8f6f\u4ef6\u3002\u8f6f\u4ef6\u90e8\u5206\u5982\u56fe\uff1a \u8f6f\u4ef6\uff08PART2\uff09\u7684\u8bfe\u7a0b\u4f5c\u4e1a\u5305\u62ec6\u4e2a\u9879\u76ee\uff1a \u9879\u76ee7\uff1a\u6784\u5efa\u865a\u62df\u673a\u8bed\u8a00\u5230\u6c47\u7f16\u8bed\u8a00\u7684\u7ffb\u8bd1\u5668\uff08VM Translator\uff09\uff0c\u5b9e\u73b0\u5806\u6808\u8fd0\u7b97\u548c\u5185\u5b58\u8bbf\u95ee \u9879\u76ee8\uff1a\u6784\u5efa\u865a\u62df\u673a\u8bed\u8a00\u5230\u6c47\u7f16\u8bed\u8a00\u7684\u7ffb\u8bd1\u5668\uff08VM Translator\uff09\uff0c\u5b9e\u73b0\u7a0b\u5e8f\u63a7\u5236\u548c\u51fd\u6570\u8c03\u7528 \u9879\u76ee9\uff1a\u7528\u9ad8\u7ea7\u8bed\u8a00Jack\u7f16\u5199\u7a0b\u5e8f \u9879\u76ee10\uff1a\u6784\u5efa\u9ad8\u7ea7\u8bed\u8a00\u5230\u865a\u62df\u673a\u8bed\u8a00\u7684\u7f16\u8bd1\u5668\uff08Compiler\uff09\uff0c \u9879\u76ee11\uff1a\u6784\u5efa\u9ad8\u7ea7\u8bed\u8a00\u5230\u865a\u62df\u673a\u8bed\u8a00\u7684\u7f16\u8bd1\u5668\uff08Compiler\uff09\uff0c \u9879\u76ee12\uff1a\u64cd\u4f5c\u7cfb\u7edf\u62bd\u8c61","title":"\u4ecb\u7ecd"},{"location":"nand2tetris_part_2/#78","text":"","title":"\u7b2c7\u7ae0\u3001\u7b2c8\u7ae0 \u865a\u62df\u673a"},{"location":"nand2tetris_part_2/#jackvmhackasmhackbinary","text":"","title":"\u4ece\u9ad8\u7ea7\u7f16\u7a0b\u8bed\u8a00\u5230\u4e8c\u8fdb\u5236\u4ee3\u7801\uff1aJack\u2192VM\u2192Hack(ASM)\u2192Hack(Binary)"},{"location":"nand2tetris_part_2/#9","text":"\u4f5c\u8005\u8bbe\u8ba1\u7684\u9ad8\u7ea7\u8bed\u8a00Jack\uff0c\u8bed\u6cd5\u548cJava\uff0cC#\u7c7b\u4f3c\u3002","title":"\u7b2c9\u7ae0 \u9ad8\u7ea7\u8bed\u8a00"},{"location":"nand2tetris_part_2/#1011","text":"","title":"\u7b2c10\u7ae0\u3001\u7b2c11\u7ae0 \u7f16\u8bd1\u5668"},{"location":"nand2tetris_part_2/#12","text":"\u64cd\u4f5c\u7cfb\u7edf\u7528\u6765\u8854\u63a5\u8ba1\u7b97\u673a\u786c\u4ef6\u548c\u8f6f\u4ef6\uff0c\u4e3b\u8981\u4f5c\u7528\u5305\u62ec\uff1a\uff081\uff09\u5c01\u88c5\u4e0d\u540c\u7684\u786c\u4ef6\u670d\u52a1\uff082\uff09\u7528\u4e0d\u540c\u7684\u51fd\u6570\u548c\u6570\u636e\u7c7b\u578b\u6269\u5c55\u4e86\u9ad8\u7ea7\u8bed\u8a00\u3002\u64cd\u4f5c\u7cfb\u7edf\u901a\u5e38\u7531\u9ad8\u7ea7\u8bed\u8a00\u7f16\u5199\uff0c\u5e76\u88ab\u7f16\u8bd1\u6210\u4e8c\u8fdb\u5236\u5f62\u5f0f\u3002","title":"\u7b2c12\u7ae0 \u64cd\u4f5c\u7cfb\u7edf"},{"location":"nlp-transformer-summary/","text":"\u6211\u7684\u80cc\u666f \u00b6 \u7b2c\u4e00\u6b21\u53c2\u52a0Datawhale\u7ec4\u961f\u5b66\u4e60\u8bfe\u7a0b\uff0c\u6211\u7684\u76f8\u5173\u77e5\u8bc6\u80cc\u666f\u662f\uff1a - Transformer\uff1a0\u57fa\u7840 - PyTorch\uff1a0\u57fa\u7840 - NLP\uff1a0.1\u57fa\u7840 - Python\uff1a0\u57fa\u7840 \u4f5c\u4e3aNLP\u60c5\u611f\u5206\u6790\u7684\u9886\u822a\u5458\u548cTransformers\u7684\u5b66\u5458\uff0c\u6211\u5c06\u4ece\u8bfe\u7a0b\u5185\u5bb9\u548c\u8fd0\u8425\u4e24\u65b9\u9762\u5199\u4e00\u4e0b\u81ea\u5df1\u7684\u611f\u53d7\u548c\u60f3\u6cd5\u3002 Transformer\u8bfe\u7a0b\u5927\u7eb2 \u00b6 \u8bfe\u7a0b\u5185\u5bb9\u65b9\u9762 \u00b6 \u8bfe\u7a0b\u5185\u5bb9\u5bf9\u96f6\u57fa\u7840\u5165\u95e8\u7684\u4eba\u662f\u6bd4\u8f83\u53cb\u597d\u7684\u3002\u6bd4\u5982\u6211\u662f\u7b2c\u4e00\u6b21\u5b66\u4e60Transformer\uff0c\u4f46\u662f\u56fe\u89e3\u7cfb\u5217\u5f88\u5bb9\u6613\u7406\u89e3\u3002 \u4f46\u662f\u6bcf\u4e2atask\u7684\u5185\u5bb9\u96be\u5ea6\u5dee\u522b\u8fc7\u5927\u3002\u6bd4\u5982Task02\u7684Transformer\u539f\u8bba\u6587\u4ee3\u7801\u6807\u6ce8 \uff0c\u548cTask05 transformers\u6e90\u7801\u8bb2\u89e3\u3002 \u6bcf\u4e2atask\u7684\u5de5\u4f5c\u91cf\u4e0d\u5e73\u8861\uff0c\u6709\u7684\u7279\u522b\u591a\uff0c\u6709\u7684\u76f8\u5bf9\u5c11\u3002 \u7b2c\u56db\u7ae0\u53ef\u4ee5\u4efb\u9009\u4e00\u4e2a\u4efb\u52a1\u5e94\u7528\uff0c\u628a\u66f4\u591a\u65f6\u95f4\u7559\u7ed9\u5b66\u4e60\u5982\u4f55\u4f7f\u7528huggingface\u7684transformers\u3002\uff08\u5c31\u662f\u90a3\u4e2a\u5b98\u65b9\u8bfe\u7a0b\uff09 \u8bfe\u7a0b\u8fd0\u8425\u65b9\u9762 \u00b6 \u4f18\u79c0\u961f\u5458\u548c\u4f18\u79c0\u961f\u957f\u8bc4\u9009\u6807\u51c6\u9700\u8981\u7edf\u4e00 \u8865\u5361\u89c4\u5219\u9700\u8981\u7edf\u4e00 \u9010\u6b65\u5b8c\u5584\u8bfe\u7a0b\u4f53\u7cfb \u5c0f\u7a0b\u5e8f\uff0c\u7528\u8d77\u6765\u4e0d\u662f\u5f88\u65b9\u4fbf \uff08\u8111\u6d1e1\uff09\u6bcf\u6b21\u6253\u5361\u4e4b\u540e\u9a6c\u4e0a\u8fdb\u884c\u4f5c\u4e1a\u8bc4\u5ba1\u548c\u53cd\u9988\uff08\u8fc7\u4e8e\u8017\u8d39\u52a9\u6559\u7cbe\u529b\uff09 \uff08\u8111\u6d1e2\uff09\u7ed9\u6bcf\u4e2a\u5c0f\u7ec4\u6216\u4e2a\u4eba\u5b89\u6392\u4e00\u4e2a\u671f\u672b\u5927project\uff0c\u6216\u8005\u5e03\u7f6e\u5e73\u65f6\u4f5c\u4e1a\uff08\u8fc7\u4e8e\u8017\u8d39\u5b66\u5458\u7cbe\u529b\uff09 \uff08\u8111\u6d1e3\uff09\u76f4\u63a5\u7528\u4e00\u4e2a\u8bfe\u7a0b\u7ba1\u7406\u7cfb\u7edf\u8fdb\u884c\u7ba1\u7406\uff08\u7c7b\u4f3c\u4e8ecanvas,\u96e8\u8bfe\u5802\uff09(\u9010\u6e10\u5b66\u9662\u5316) \u53c2\u8003\u8d44\u6599\u6e05\u5355\uff08\u603b\uff09 \u00b6 Transformer\u5728\u7f51\u4e0a\u6709\u5f88\u591a\u5f88\u591a\u6559\u7a0b\uff0c\u5176\u4e2d\u516c\u8ba4\u7684\u3001\u666e\u904d\u6027\u7684\u6bd4\u8f83\u597d\u7684\u8d44\u6599\u5982\u4e0b\uff1a \u7406\u8bba\u90e8\u5206 [1] (\u5f3a\u63a8)\u674e\u5b8f\u6bc52021\u6625\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b https://www.bilibili.com/video/BV1Wv411h7kN?from=search&seid=17090062977285779802&spm_id_from=333.337.0.0 [2] \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8\uff08\u6db5\u76d6\u4e86\u56fe\u89e3\u7cfb\u5217\u3001annotated transformer\u3001huggingface\uff09 https://github.com/datawhalechina/learn-nlp-with-transformers [3] \u56fe\u89e3transformer|The Illustrated Transformer http://jalammar.github.io/illustrated-transformer/ [4] \u56fe\u89e3seq2seq, attention|Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ \u4ee3\u7801\u90e8\u5206 [5] The Annotated Transformer http://nlp.seas.harvard.edu//2018/04/03/attention.html [6] Huggingface/transformers https://github.com/huggingface/transformers/blob/master/README_zh-hans.md \u8bba\u6587\u90e8\u5206 Attention is all \"we\" need. \u5176\u4ed6\u4e0d\u9519\u7684\u535a\u5ba2\u6216\u6559\u7a0b [7] \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8--\u5728\u7ebf\u9605\u8bfb https://datawhalechina.github.io/learn-nlp-with-transformers/#/ [8] \u674e\u5b8f\u6bc52021\u6625\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b\u7b14\u8bb0\u2014\u2014\u81ea\u6ce8\u610f\u529b\u673a\u5236 https://www.cnblogs.com/sykline/p/14730088.html [9] \u674e\u5b8f\u6bc52021\u6625\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b\u7b14\u8bb0\u2014\u2014Transformer\u6a21\u578b https://www.cnblogs.com/sykline/p/14785552.html [10] \u674e\u5b8f\u6bc5\u673a\u5668\u5b66\u4e60\u5b66\u4e60\u7b14\u8bb0\u2014\u2014\u81ea\u6ce8\u610f\u529b\u673a\u5236 https://blog.csdn.net/p_memory/article/details/116271274 [11] \u8f66\u4e07\u7fd4-\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b0\u8303\u5f0f\uff1a\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u65b9\u6cd5\u3010\u8bb2\u5ea7+PPT\u3011 https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true [12] \u82cf\u5251\u6797-\u300aAttention is All You Need\u300b\u6d45\u8bfb\uff08\u7b80\u4ecb+\u4ee3\u7801\uff09 https://spaces.ac.cn/archives/4765","title":"Summary Transformer\u8bfe\u7a0b\u603b\u7ed3"},{"location":"nlp-transformer-summary/#_1","text":"\u7b2c\u4e00\u6b21\u53c2\u52a0Datawhale\u7ec4\u961f\u5b66\u4e60\u8bfe\u7a0b\uff0c\u6211\u7684\u76f8\u5173\u77e5\u8bc6\u80cc\u666f\u662f\uff1a - Transformer\uff1a0\u57fa\u7840 - PyTorch\uff1a0\u57fa\u7840 - NLP\uff1a0.1\u57fa\u7840 - Python\uff1a0\u57fa\u7840 \u4f5c\u4e3aNLP\u60c5\u611f\u5206\u6790\u7684\u9886\u822a\u5458\u548cTransformers\u7684\u5b66\u5458\uff0c\u6211\u5c06\u4ece\u8bfe\u7a0b\u5185\u5bb9\u548c\u8fd0\u8425\u4e24\u65b9\u9762\u5199\u4e00\u4e0b\u81ea\u5df1\u7684\u611f\u53d7\u548c\u60f3\u6cd5\u3002","title":"\u6211\u7684\u80cc\u666f"},{"location":"nlp-transformer-summary/#transformer","text":"","title":"Transformer\u8bfe\u7a0b\u5927\u7eb2"},{"location":"nlp-transformer-summary/#_2","text":"\u8bfe\u7a0b\u5185\u5bb9\u5bf9\u96f6\u57fa\u7840\u5165\u95e8\u7684\u4eba\u662f\u6bd4\u8f83\u53cb\u597d\u7684\u3002\u6bd4\u5982\u6211\u662f\u7b2c\u4e00\u6b21\u5b66\u4e60Transformer\uff0c\u4f46\u662f\u56fe\u89e3\u7cfb\u5217\u5f88\u5bb9\u6613\u7406\u89e3\u3002 \u4f46\u662f\u6bcf\u4e2atask\u7684\u5185\u5bb9\u96be\u5ea6\u5dee\u522b\u8fc7\u5927\u3002\u6bd4\u5982Task02\u7684Transformer\u539f\u8bba\u6587\u4ee3\u7801\u6807\u6ce8 \uff0c\u548cTask05 transformers\u6e90\u7801\u8bb2\u89e3\u3002 \u6bcf\u4e2atask\u7684\u5de5\u4f5c\u91cf\u4e0d\u5e73\u8861\uff0c\u6709\u7684\u7279\u522b\u591a\uff0c\u6709\u7684\u76f8\u5bf9\u5c11\u3002 \u7b2c\u56db\u7ae0\u53ef\u4ee5\u4efb\u9009\u4e00\u4e2a\u4efb\u52a1\u5e94\u7528\uff0c\u628a\u66f4\u591a\u65f6\u95f4\u7559\u7ed9\u5b66\u4e60\u5982\u4f55\u4f7f\u7528huggingface\u7684transformers\u3002\uff08\u5c31\u662f\u90a3\u4e2a\u5b98\u65b9\u8bfe\u7a0b\uff09","title":"\u8bfe\u7a0b\u5185\u5bb9\u65b9\u9762"},{"location":"nlp-transformer-summary/#_3","text":"\u4f18\u79c0\u961f\u5458\u548c\u4f18\u79c0\u961f\u957f\u8bc4\u9009\u6807\u51c6\u9700\u8981\u7edf\u4e00 \u8865\u5361\u89c4\u5219\u9700\u8981\u7edf\u4e00 \u9010\u6b65\u5b8c\u5584\u8bfe\u7a0b\u4f53\u7cfb \u5c0f\u7a0b\u5e8f\uff0c\u7528\u8d77\u6765\u4e0d\u662f\u5f88\u65b9\u4fbf \uff08\u8111\u6d1e1\uff09\u6bcf\u6b21\u6253\u5361\u4e4b\u540e\u9a6c\u4e0a\u8fdb\u884c\u4f5c\u4e1a\u8bc4\u5ba1\u548c\u53cd\u9988\uff08\u8fc7\u4e8e\u8017\u8d39\u52a9\u6559\u7cbe\u529b\uff09 \uff08\u8111\u6d1e2\uff09\u7ed9\u6bcf\u4e2a\u5c0f\u7ec4\u6216\u4e2a\u4eba\u5b89\u6392\u4e00\u4e2a\u671f\u672b\u5927project\uff0c\u6216\u8005\u5e03\u7f6e\u5e73\u65f6\u4f5c\u4e1a\uff08\u8fc7\u4e8e\u8017\u8d39\u5b66\u5458\u7cbe\u529b\uff09 \uff08\u8111\u6d1e3\uff09\u76f4\u63a5\u7528\u4e00\u4e2a\u8bfe\u7a0b\u7ba1\u7406\u7cfb\u7edf\u8fdb\u884c\u7ba1\u7406\uff08\u7c7b\u4f3c\u4e8ecanvas,\u96e8\u8bfe\u5802\uff09(\u9010\u6e10\u5b66\u9662\u5316)","title":"\u8bfe\u7a0b\u8fd0\u8425\u65b9\u9762"},{"location":"nlp-transformer-summary/#_4","text":"Transformer\u5728\u7f51\u4e0a\u6709\u5f88\u591a\u5f88\u591a\u6559\u7a0b\uff0c\u5176\u4e2d\u516c\u8ba4\u7684\u3001\u666e\u904d\u6027\u7684\u6bd4\u8f83\u597d\u7684\u8d44\u6599\u5982\u4e0b\uff1a \u7406\u8bba\u90e8\u5206 [1] (\u5f3a\u63a8)\u674e\u5b8f\u6bc52021\u6625\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b https://www.bilibili.com/video/BV1Wv411h7kN?from=search&seid=17090062977285779802&spm_id_from=333.337.0.0 [2] \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8\uff08\u6db5\u76d6\u4e86\u56fe\u89e3\u7cfb\u5217\u3001annotated transformer\u3001huggingface\uff09 https://github.com/datawhalechina/learn-nlp-with-transformers [3] \u56fe\u89e3transformer|The Illustrated Transformer http://jalammar.github.io/illustrated-transformer/ [4] \u56fe\u89e3seq2seq, attention|Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ \u4ee3\u7801\u90e8\u5206 [5] The Annotated Transformer http://nlp.seas.harvard.edu//2018/04/03/attention.html [6] Huggingface/transformers https://github.com/huggingface/transformers/blob/master/README_zh-hans.md \u8bba\u6587\u90e8\u5206 Attention is all \"we\" need. \u5176\u4ed6\u4e0d\u9519\u7684\u535a\u5ba2\u6216\u6559\u7a0b [7] \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8--\u5728\u7ebf\u9605\u8bfb https://datawhalechina.github.io/learn-nlp-with-transformers/#/ [8] \u674e\u5b8f\u6bc52021\u6625\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b\u7b14\u8bb0\u2014\u2014\u81ea\u6ce8\u610f\u529b\u673a\u5236 https://www.cnblogs.com/sykline/p/14730088.html [9] \u674e\u5b8f\u6bc52021\u6625\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b\u7b14\u8bb0\u2014\u2014Transformer\u6a21\u578b https://www.cnblogs.com/sykline/p/14785552.html [10] \u674e\u5b8f\u6bc5\u673a\u5668\u5b66\u4e60\u5b66\u4e60\u7b14\u8bb0\u2014\u2014\u81ea\u6ce8\u610f\u529b\u673a\u5236 https://blog.csdn.net/p_memory/article/details/116271274 [11] \u8f66\u4e07\u7fd4-\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b0\u8303\u5f0f\uff1a\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u65b9\u6cd5\u3010\u8bb2\u5ea7+PPT\u3011 https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true [12] \u82cf\u5251\u6797-\u300aAttention is All You Need\u300b\u6d45\u8bfb\uff08\u7b80\u4ecb+\u4ee3\u7801\uff09 https://spaces.ac.cn/archives/4765","title":"\u53c2\u8003\u8d44\u6599\u6e05\u5355\uff08\u603b\uff09"},{"location":"nlp-transformer-task01/","text":"Task01 NLP\u5b66\u4e60\u6982\u89c8 \u00b6 NLP\u601d\u7ef4\u5bfc\u56fe(2021-09-13) \u00b6 \u53c2\u8003\u8d44\u6599 \u00b6 Datawhale-\u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8 https://github.com/datawhalechina/learn-nlp-with-transformers \u300a\u81ea\u7136\u8bed\u8a00\u5904\u7406-\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\u300b https://item.jd.com/13344628.html \u5218\u77e5\u8fdc\u8001\u5e08-NLP\u7814\u7a76\u5165\u95e8\u4e4b\u9053 https://github.com/zibuyu/research_tao","title":"Task01 NLP\u5b66\u4e60\u6982\u89c8"},{"location":"nlp-transformer-task01/#task01-nlp","text":"","title":"Task01 NLP\u5b66\u4e60\u6982\u89c8"},{"location":"nlp-transformer-task01/#nlp2021-09-13","text":"","title":"NLP\u601d\u7ef4\u5bfc\u56fe(2021-09-13)"},{"location":"nlp-transformer-task01/#_1","text":"Datawhale-\u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8 https://github.com/datawhalechina/learn-nlp-with-transformers \u300a\u81ea\u7136\u8bed\u8a00\u5904\u7406-\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\u300b https://item.jd.com/13344628.html \u5218\u77e5\u8fdc\u8001\u5e08-NLP\u7814\u7a76\u5165\u95e8\u4e4b\u9053 https://github.com/zibuyu/research_tao","title":"\u53c2\u8003\u8d44\u6599"},{"location":"nlp-transformer-task02/","text":"Task02 \u5b66\u4e60Attentioin\u548cTransformer \u00b6 Attention \u00b6 seq2seq \u00b6 seq2seq\u662f\u4e00\u79cd\u5e38\u89c1\u7684NLP\u6a21\u578b\u7ed3\u6784\uff0c\u5168\u79f0\u662f\uff1asequence to sequence\uff0c\u7ffb\u8bd1\u4e3a\u201c\u5e8f\u5217\u5230\u5e8f\u5217\u201d\u3002\u987e\u540d\u601d\u4e49\uff1a\u4ece\u4e00\u4e2a\u6587\u672c\u5e8f\u5217\u5f97\u5230\u4e00\u4e2a\u65b0\u7684\u6587\u672c\u5e8f\u5217\u3002\u5178\u578b\u7684\u4efb\u52a1\u6709\uff1a\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\uff0c\u6587\u672c\u6458\u8981\u4efb\u52a1\u3002 seq2seq\u6a21\u578b\u7531\u7f16\u7801\u5668\uff08encoder\uff09\u548c\u89e3\u7801\u5668\uff08decoder\uff09\u7ec4\u6210\uff0c\u7f16\u7801\u5668\u7528\u6765\u5206\u6790\u8f93\u5165\u5e8f\u5217\uff0c\u89e3\u7801\u5668\u7528\u6765\u751f\u6210\u8f93\u51fa\u5e8f\u5217\u3002\u7f16\u7801\u5668\u4f1a\u5904\u7406\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\uff0c\u628a\u8fd9\u4e9b\u4fe1\u606f\u8f6c\u6362\u6210\u4e3a\u4e00\u4e2a\u80cc\u666f\u5411\u91cf\uff08context vector\uff09\u3002\u5f53\u6211\u4eec\u5904\u7406\u5b8c\u6574\u4e2a\u8f93\u5165\u5e8f\u5217\u540e\uff0c\u7f16\u7801\u5668\u628a\u80cc\u666f\u5411\u91cf\u53d1\u9001\u7ed9\u89e3\u7801\u5668\uff0c\u89e3\u7801\u5668\u901a\u8fc7\u80cc\u666f\u5411\u91cf\u4e2d\u7684\u4fe1\u606f\uff0c\u9010\u4e2a\u5143\u7d20\u8f93\u51fa\u65b0\u7684\u5e8f\u5217\u3002 \u5728transformer\u6a21\u578b\u4e4b\u524d\uff0cseq2seq\u4e2d\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e00\u822c\u91c7\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09 \uff0c\u867d\u7136\u975e\u5e38\u7ecf\u5178\uff0c\u4f46\u662f\u5c40\u9650\u6027\u4e5f\u975e\u5e38\u5927\u3002\u6700\u5927\u7684\u5c40\u9650\u6027\u5c31\u5728\u4e8e\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e4b\u95f4\u7684\u552f\u4e00\u8054\u7cfb\u5c31\u662f\u4e00\u4e2a\u56fa\u5b9a\u957f\u5ea6\u7684context\u5411\u91cf\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u7f16\u7801\u5668\u8981\u5c06\u6574\u4e2a\u5e8f\u5217\u7684\u4fe1\u606f\u538b\u7f29\u8fdb\u4e00\u4e2a\u56fa\u5b9a\u957f\u5ea6\u7684\u5411\u91cf\u4e2d\u3002\u8fd9\u6837\u505a\u5b58\u5728\u4e24\u4e2a\u5f0a\u7aef\uff1a - \u8bed\u4e49\u5411\u91cf\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u8868\u793a\u6574\u4e2a\u5e8f\u5217\u7684\u4fe1\u606f - \u5148\u8f93\u5165\u5230\u7f51\u7edc\u7684\u5185\u5bb9\u643a\u5e26\u7684\u4fe1\u606f\u4f1a\u88ab\u540e\u8f93\u5165\u7684\u4fe1\u606f\u8986\u76d6\u6389\uff0c\u8f93\u5165\u5e8f\u5217\u8d8a\u957f\uff0c\u8fd9\u4e2a\u73b0\u8c61\u5c31\u8d8a\u4e25\u91cd Attention \u00b6 \u4e3a\u4e86\u89e3\u51b3seq2seq\u6a21\u578b\u4e2d\u7684\u4e24\u4e2a\u5f0a\u7aef\uff0cBahdanau\u7b49\u4eba\u5728\u8bba\u6587\u300aNeural Machine Translation by Jointly Learning to Align and Translate\u300b\u4e2d\u63d0\u51fa\u4f7f\u7528Attention\u673a\u5236\uff0c\u4f7f\u5f97seq2seq\u6a21\u578b\u53ef\u4ee5\u6709\u533a\u5206\u5ea6\u3001\u6709\u91cd\u70b9\u5730\u5173\u6ce8\u8f93\u5165\u5e8f\u5217\uff0c\u4ece\u800c\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u673a\u5668\u7ffb\u8bd1\u7684\u8d28\u91cf\u3002 \u4e00\u4e2a\u6709\u6ce8\u610f\u529b\u673a\u5236\u7684seq2seq\u4e0e\u7ecf\u5178\u7684seq2seq\u4e3b\u8981\u67092\u70b9\u4e0d\u540c\uff1a 1. \u9996\u5148\uff0c\u7f16\u7801\u5668\u4f1a\u628a\u66f4\u591a\u7684\u6570\u636e\u4f20\u9012\u7ed9\u89e3\u7801\u5668\u3002\u7f16\u7801\u5668\u628a\u6240\u6709\u65f6\u95f4\u6b65\u7684 hidden state\uff08\u9690\u85cf\u5c42\u72b6\u6001\uff09\u4f20\u9012\u7ed9\u89e3\u7801\u5668\uff0c\u800c\u4e0d\u662f\u53ea\u4f20\u9012\u6700\u540e\u4e00\u4e2a hidden state\uff08\u9690\u85cf\u5c42\u72b6\u6001\uff09 2. \u6ce8\u610f\u529b\u6a21\u578b\u7684\u89e3\u7801\u5668\u5728\u4ea7\u751f\u8f93\u51fa\u4e4b\u524d\uff0c\u505a\u4e86\u4e00\u4e2a\u989d\u5916\u7684attention\u5904\u7406 Transformer \u00b6 \u6a21\u578b\u67b6\u6784 \u00b6 transformer\u539f\u8bba\u6587\u7684\u67b6\u6784\u56fe\uff1a \u4e00\u4e2a\u66f4\u6e05\u6670\u7684\u67b6\u6784\u56fe\uff1a \u4ece\u8f93\u5165\u5230\u8f93\u51fa\u62c6\u5f00\u770b\u5c31\u662f\uff1a - INPUT\uff1ainput vector + position encoding - ENCODERs\uff08\u00d76\uff09\uff0cand each encoder includes\uff1a - input - multi-head self-attention - residual connection&norm - full-connected network - residual connection&norm - output - DECODERs\uff08\u00d76\uff09\uff0cand each decoder includes\uff1a - input - Masked multihead self-attention - residual connection&norm - multi-head self-attention - residual connection&norm - full-connected network - residual connection&norm - output - OUTPUT\uff1a - output (decoder's) - linear layer - softmax layer - output \u6a21\u578b\u8f93\u5165 \u00b6 \u8bcd\u5411\u91cf \u00b6 \u548c\u5e38\u89c1\u7684NLP\u4efb\u52a1\u4e00\u6837\uff0c\u6211\u4eec\u9996\u5148\u4f1a\u4f7f\u7528\u8bcd\u5d4c\u5165\u7b97\u6cd5\uff08embedding\uff09\uff0c\u5c06\u8f93\u5165\u6587\u672c\u5e8f\u5217\u7684\u6bcf\u4e2a\u8bcd\u8f6c\u6362\u4e3a\u4e00\u4e2a\u8bcd\u5411\u91cf\u3002 \u4f4d\u7f6e\u5411\u91cf \u00b6 Transformer\u6a21\u578b\u5bf9\u6bcf\u4e2a\u8f93\u5165\u7684\u8bcd\u5411\u91cf\u90fd\u52a0\u4e0a\u4e86\u4e00\u4e2a\u4f4d\u7f6e\u5411\u91cf\u3002\u8fd9\u4e9b\u5411\u91cf\u6709\u52a9\u4e8e\u786e\u5b9a\u6bcf\u4e2a\u5355\u8bcd\u7684\u4f4d\u7f6e\u7279\u5f81\uff0c\u6216\u8005\u53e5\u5b50\u4e2d\u4e0d\u540c\u5355\u8bcd\u4e4b\u95f4\u7684\u8ddd\u79bb\u7279\u5f81\u3002\u8bcd\u5411\u91cf\u52a0\u4e0a\u4f4d\u7f6e\u5411\u91cf\u80cc\u540e\u7684\u76f4\u89c9\u662f\uff1a\u5c06\u8fd9\u4e9b\u8868\u793a\u4f4d\u7f6e\u7684\u5411\u91cf\u6dfb\u52a0\u5230\u8bcd\u5411\u91cf\u4e2d\uff0c\u5f97\u5230\u7684\u65b0\u5411\u91cf\uff0c\u53ef\u4ee5\u4e3a\u6a21\u578b\u63d0\u4f9b\u66f4\u591a\u6709\u610f\u4e49\u7684\u4fe1\u606f\uff0c\u6bd4\u5982\u8bcd\u7684\u4f4d\u7f6e\uff0c\u8bcd\u4e4b\u95f4\u7684\u8ddd\u79bb\u7b49\u3002 \uff08\u751f\u6210\u4f4d\u7f6e\u7f16\u7801\u5411\u91cf\u7684\u65b9\u6cd5\u6709\u5f88\u591a\u79cd\uff09 \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668 \u00b6 \u6ce8\uff1a1. \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e2d\u6709\u76f8\u4f3c\u7684\u6a21\u5757\u548c\u7ed3\u6784\uff0c\u6240\u4ee5\u5408\u5e76\u5230\u4e00\u8d77\u4ecb\u7ecd\u3002 2. \u672c\u90e8\u5206\u6309\u7167\u674e\u5b8f\u6bc5\u8001\u5e08\u7684Attention\uff0cTransformer\u90e8\u5206\u7684\u8bfe\u7a0bPPT\u6765\uff0c\u56e0\u4e3alee\u7684\u8bfe\u7a0b\u5bf9\u65b0\u624b\u66f4\u53cb\u597d\u3002 Self-Attention \u00b6 self-attention\u5bf9\u4e8e\u6bcf\u4e2a\u5411\u91cf\u90fd\u4f1a\u8003\u8651\u6574\u4e2asequence\u7684\u4fe1\u606f\u540e\u8f93\u51fa\u4e00\u4e2a\u5411\u91cf\uff0cself-attention\u7ed3\u6784\u5982\u4e0b\uff1a FC\uff1aFully-connected network \u5168\u8fde\u63a5\u7f51\u7edc ai: \u8f93\u5165\u53d8\u91cf\u3002\u53ef\u80fd\u662f\u6574\u4e2a\u7f51\u7edc\u7684\u8f93\u5165\uff0c\u4e5f\u53ef\u80fd\u662f\u67d0\u4e2a\u9690\u85cf\u5c42\u7684\u8f93\u51fa bi: \u8003\u8651\u6574\u4e2asequence\u4fe1\u606f\u540e\u7684\u8f93\u51fa\u53d8\u91cf \u77e9\u9635\u8ba1\u7b97\uff1a \u76ee\u6807\uff1a\u6839\u636e\u8f93\u5165\u5411\u91cf\u77e9\u9635I\uff0c\u8ba1\u7b97\u8f93\u51fa\u5411\u91cf\u77e9\u9635O\u3002\u77e9\u9635\u8fd0\u7b97\u8fc7\u7a0b\uff1a 1. \u77e9\u9635I\u5206\u522b\u4e58\u4ee5Wq, Wk, Wv\uff08\u53c2\u6570\u77e9\u9635\uff0c\u9700\u8981\u6a21\u578b\u8fdb\u884c\u5b66\u4e60\uff09\uff0c\u5f97\u5230\u77e9\u9635Q, K, V\u3002 2. \u77e9\u9635K\u7684\u8f6c\u7f6e\u4e58\u4ee5Q\uff0c\u5f97\u5230\u6ce8\u610f\u529b\u6743\u91cd\u77e9\u9635A\uff0c\u5f52\u4e00\u5316\u5f97\u5230\u77e9\u9635A\u2019\u3002 3. \u77e9\u9635V\u4e58\u77e9\u9635A\u2018\uff0c\u5f97\u5230\u8f93\u51fa\u5411\u91cf\u77e9\u9635O\u3002 Multi Head Self-Attention \u00b6 \u7b80\u5355\u5730\u8bf4\uff0c\u591a\u4e86\u51e0\u7ec4Q\uff0cK\uff0cV\u3002\u5728Self-Attention\u4e2d\uff0c\u6211\u4eec\u662f\u4f7f\u7528\ud835\udc5e\u53bb\u5bfb\u627e\u4e0e\u4e4b\u76f8\u5173\u7684\ud835\udc58\uff0c\u4f46\u662f\u8fd9\u4e2a\u76f8\u5173\u6027\u5e76\u4e0d\u4e00\u5b9a\u6709\u4e00\u79cd\u3002\u90a3\u591a\u79cd\u76f8\u5173\u6027\u4f53\u73b0\u5230\u8ba1\u7b97\u65b9\u5f0f\u4e0a\u5c31\u662f\u6709\u591a\u4e2a\u77e9\u9635\ud835\udc5e\uff0c\u4e0d\u540c\u7684\ud835\udc5e\u8d1f\u8d23\u4ee3\u8868\u4e0d\u540c\u7684\u76f8\u5173\u6027\u3002 Transformer \u7684\u8bba\u6587\u901a\u8fc7\u589e\u52a0\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff08\u4e00\u7ec4\u6ce8\u610f\u529b\u79f0\u4e3a\u4e00\u4e2a attention head\uff09\uff0c\u8fdb\u4e00\u6b65\u5b8c\u5584\u4e86Self-Attention\u3002\u8fd9\u79cd\u673a\u5236\u4ece\u5982\u4e0b\u4e24\u4e2a\u65b9\u9762\u589e\u5f3a\u4e86attention\u5c42\u7684\u80fd\u529b\uff1a - \u5b83\u6269\u5c55\u4e86\u6a21\u578b\u5173\u6ce8\u4e0d\u540c\u4f4d\u7f6e\u7684\u80fd\u529b\u3002 - \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u8d4b\u4e88attention\u5c42\u591a\u4e2a\u201c\u5b50\u8868\u793a\u7a7a\u95f4\u201d\u3002 \u6b8b\u5dee\u94fe\u63a5\u548c\u5f52\u4e00\u5316 \u00b6 \u6b8b\u5dee\u94fe\u63a5\uff1a\u4e00\u79cd\u628ainput\u5411\u91cf\u548coutput\u5411\u91cf\u76f4\u63a5\u52a0\u8d77\u6765\u7684\u67b6\u6784\u3002 \u5f52\u4e00\u5316\uff1a\u628a\u6570\u636e\u6620\u5c04\u52300\uff5e1\u8303\u56f4\u4e4b\u5185\u5904\u7406\u3002 \u6a21\u578b\u8f93\u51fa \u00b6 \u7ebf\u6027\u5c42\u548csoftmax \u00b6 Decoder \u6700\u7ec8\u7684\u8f93\u51fa\u662f\u4e00\u4e2a\u5411\u91cf\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5143\u7d20\u662f\u6d6e\u70b9\u6570\u3002\u6211\u4eec\u600e\u4e48\u628a\u8fd9\u4e2a\u5411\u91cf\u8f6c\u6362\u4e3a\u5355\u8bcd\u5462\uff1f\u8fd9\u662f\u7ebf\u6027\u5c42\u548csoftmax\u5b8c\u6210\u7684\u3002 \u7ebf\u6027\u5c42\u5c31\u662f\u4e00\u4e2a\u666e\u901a\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\uff0c\u53ef\u4ee5\u628a\u89e3\u7801\u5668\u8f93\u51fa\u7684\u5411\u91cf\uff0c\u6620\u5c04\u5230\u4e00\u4e2a\u66f4\u5927\u7684\u5411\u91cf\uff0c\u8fd9\u4e2a\u5411\u91cf\u79f0\u4e3a logits \u5411\u91cf\uff1a\u5047\u8bbe\u6211\u4eec\u7684\u6a21\u578b\u6709 10000 \u4e2a\u82f1\u8bed\u5355\u8bcd\uff08\u6a21\u578b\u7684\u8f93\u51fa\u8bcd\u6c47\u8868\uff09\uff0c\u6b64 logits \u5411\u91cf\u4fbf\u4f1a\u6709 10000 \u4e2a\u6570\u5b57\uff0c\u6bcf\u4e2a\u6570\u8868\u793a\u4e00\u4e2a\u5355\u8bcd\u7684\u5206\u6570\u3002 \u7136\u540e\uff0cSoftmax \u5c42\u4f1a\u628a\u8fd9\u4e9b\u5206\u6570\u8f6c\u6362\u4e3a\u6982\u7387\uff08\u628a\u6240\u6709\u7684\u5206\u6570\u8f6c\u6362\u4e3a\u6b63\u6570\uff0c\u5e76\u4e14\u52a0\u8d77\u6765\u7b49\u4e8e 1\uff09\u3002\u7136\u540e\u9009\u62e9\u6700\u9ad8\u6982\u7387\u7684\u90a3\u4e2a\u6570\u5b57\u5bf9\u5e94\u7684\u8bcd\uff0c\u5c31\u662f\u8fd9\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u5355\u8bcd\u3002 \u635f\u5931\u51fd\u6570 \u00b6 Transformer\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u9700\u8981\u5c06\u89e3\u7801\u5668\u7684\u8f93\u51fa\u548clabel\u4e00\u540c\u9001\u5165\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u83b7\u5f97loss\uff0c\u6700\u7ec8\u6a21\u578b\u6839\u636eloss\u8fdb\u884c\u65b9\u5411\u4f20\u64ad\u3002 \u53ea\u8981Transformer\u89e3\u7801\u5668\u9884\u6d4b\u4e86\u7ec4\u6982\u7387\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u628a\u8fd9\u7ec4\u6982\u7387\u548c\u6b63\u786e\u7684\u8f93\u51fa\u6982\u7387\u505a\u5bf9\u6bd4\uff0c\u7136\u540e\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u6765\u8c03\u6574\u6a21\u578b\u7684\u6743\u91cd\uff0c\u4f7f\u5f97\u8f93\u51fa\u7684\u6982\u7387\u5206\u5e03\u66f4\u52a0\u63a5\u8fd1\u6574\u6570\u8f93\u51fa\u3002 \u90a3\u6211\u4eec\u8981\u600e\u4e48\u6bd4\u8f83\u4e24\u4e2a\u6982\u7387\u5206\u5e03\u5462\uff1f\uff1a\u6211\u4eec\u53ef\u4ee5\u7b80\u5355\u7684\u7528\u4e24\u7ec4\u6982\u7387\u5411\u91cf\u7684\u7684\u7a7a\u95f4\u8ddd\u79bb\u4f5c\u4e3aloss\uff08\u5411\u91cf\u76f8\u51cf\uff0c\u7136\u540e\u6c42\u5e73\u65b9\u548c\uff0c\u518d\u5f00\u65b9\uff09\uff0c\u5f53\u7136\u4e5f\u53ef\u4ee5\u4f7f\u7528\u4ea4\u53c9\u71b5(cross-entropy)]\u548cKL \u6563\u5ea6(Kullback\u2013Leibler divergence)\u3002 \u53c2\u8003\u8d44\u6599 \u00b6 \u7406\u8bba\u90e8\u5206 [1] (\u5f3a\u63a8)\u674e\u5b8f\u6bc52021\u6625\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b https://www.bilibili.com/video/BV1Wv411h7kN?from=search&seid=17090062977285779802&spm_id_from=333.337.0.0 [2] \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8\uff08\u6db5\u76d6\u4e86\u56fe\u89e3\u7cfb\u5217\u3001annotated transformer\u3001huggingface\uff09 https://github.com/datawhalechina/learn-nlp-with-transformers [3] \u56fe\u89e3transformer|The Illustrated Transformer http://jalammar.github.io/illustrated-transformer/ [4] \u56fe\u89e3seq2seq, attention|Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ \u4ee3\u7801\u90e8\u5206 [5] The Annotated Transformer http://nlp.seas.harvard.edu//2018/04/03/attention.html [6] Huggingface/transformers https://github.com/huggingface/transformers/blob/master/README_zh-hans.md \u8bba\u6587\u90e8\u5206 Attention is all \"we\" need. \u5176\u4ed6\u4e0d\u9519\u7684\u535a\u5ba2\u6216\u6559\u7a0b [7] \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8--\u5728\u7ebf\u9605\u8bfb https://datawhalechina.github.io/learn-nlp-with-transformers/#/ [8] \u674e\u5b8f\u6bc52021\u6625\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b\u7b14\u8bb0\u2014\u2014\u81ea\u6ce8\u610f\u529b\u673a\u5236 https://www.cnblogs.com/sykline/p/14730088.html [9] \u674e\u5b8f\u6bc52021\u6625\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b\u7b14\u8bb0\u2014\u2014Transformer\u6a21\u578b https://www.cnblogs.com/sykline/p/14785552.html [10] \u674e\u5b8f\u6bc5\u673a\u5668\u5b66\u4e60\u5b66\u4e60\u7b14\u8bb0\u2014\u2014\u81ea\u6ce8\u610f\u529b\u673a\u5236 https://blog.csdn.net/p_memory/article/details/116271274 [11] \u8f66\u4e07\u7fd4-\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b0\u8303\u5f0f\uff1a\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u65b9\u6cd5\u3010\u8bb2\u5ea7+PPT\u3011 https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true [12] \u82cf\u5251\u6797-\u300aAttention is All You Need\u300b\u6d45\u8bfb\uff08\u7b80\u4ecb+\u4ee3\u7801\uff09 https://spaces.ac.cn/archives/4765","title":"Task02 \u5b66\u4e60Attentioin\u548cTransformer"},{"location":"nlp-transformer-task02/#task02-attentiointransformer","text":"","title":"Task02 \u5b66\u4e60Attentioin\u548cTransformer"},{"location":"nlp-transformer-task02/#attention","text":"","title":"Attention"},{"location":"nlp-transformer-task02/#seq2seq","text":"seq2seq\u662f\u4e00\u79cd\u5e38\u89c1\u7684NLP\u6a21\u578b\u7ed3\u6784\uff0c\u5168\u79f0\u662f\uff1asequence to sequence\uff0c\u7ffb\u8bd1\u4e3a\u201c\u5e8f\u5217\u5230\u5e8f\u5217\u201d\u3002\u987e\u540d\u601d\u4e49\uff1a\u4ece\u4e00\u4e2a\u6587\u672c\u5e8f\u5217\u5f97\u5230\u4e00\u4e2a\u65b0\u7684\u6587\u672c\u5e8f\u5217\u3002\u5178\u578b\u7684\u4efb\u52a1\u6709\uff1a\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\uff0c\u6587\u672c\u6458\u8981\u4efb\u52a1\u3002 seq2seq\u6a21\u578b\u7531\u7f16\u7801\u5668\uff08encoder\uff09\u548c\u89e3\u7801\u5668\uff08decoder\uff09\u7ec4\u6210\uff0c\u7f16\u7801\u5668\u7528\u6765\u5206\u6790\u8f93\u5165\u5e8f\u5217\uff0c\u89e3\u7801\u5668\u7528\u6765\u751f\u6210\u8f93\u51fa\u5e8f\u5217\u3002\u7f16\u7801\u5668\u4f1a\u5904\u7406\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\uff0c\u628a\u8fd9\u4e9b\u4fe1\u606f\u8f6c\u6362\u6210\u4e3a\u4e00\u4e2a\u80cc\u666f\u5411\u91cf\uff08context vector\uff09\u3002\u5f53\u6211\u4eec\u5904\u7406\u5b8c\u6574\u4e2a\u8f93\u5165\u5e8f\u5217\u540e\uff0c\u7f16\u7801\u5668\u628a\u80cc\u666f\u5411\u91cf\u53d1\u9001\u7ed9\u89e3\u7801\u5668\uff0c\u89e3\u7801\u5668\u901a\u8fc7\u80cc\u666f\u5411\u91cf\u4e2d\u7684\u4fe1\u606f\uff0c\u9010\u4e2a\u5143\u7d20\u8f93\u51fa\u65b0\u7684\u5e8f\u5217\u3002 \u5728transformer\u6a21\u578b\u4e4b\u524d\uff0cseq2seq\u4e2d\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e00\u822c\u91c7\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09 \uff0c\u867d\u7136\u975e\u5e38\u7ecf\u5178\uff0c\u4f46\u662f\u5c40\u9650\u6027\u4e5f\u975e\u5e38\u5927\u3002\u6700\u5927\u7684\u5c40\u9650\u6027\u5c31\u5728\u4e8e\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e4b\u95f4\u7684\u552f\u4e00\u8054\u7cfb\u5c31\u662f\u4e00\u4e2a\u56fa\u5b9a\u957f\u5ea6\u7684context\u5411\u91cf\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u7f16\u7801\u5668\u8981\u5c06\u6574\u4e2a\u5e8f\u5217\u7684\u4fe1\u606f\u538b\u7f29\u8fdb\u4e00\u4e2a\u56fa\u5b9a\u957f\u5ea6\u7684\u5411\u91cf\u4e2d\u3002\u8fd9\u6837\u505a\u5b58\u5728\u4e24\u4e2a\u5f0a\u7aef\uff1a - \u8bed\u4e49\u5411\u91cf\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u8868\u793a\u6574\u4e2a\u5e8f\u5217\u7684\u4fe1\u606f - \u5148\u8f93\u5165\u5230\u7f51\u7edc\u7684\u5185\u5bb9\u643a\u5e26\u7684\u4fe1\u606f\u4f1a\u88ab\u540e\u8f93\u5165\u7684\u4fe1\u606f\u8986\u76d6\u6389\uff0c\u8f93\u5165\u5e8f\u5217\u8d8a\u957f\uff0c\u8fd9\u4e2a\u73b0\u8c61\u5c31\u8d8a\u4e25\u91cd","title":"seq2seq"},{"location":"nlp-transformer-task02/#attention_1","text":"\u4e3a\u4e86\u89e3\u51b3seq2seq\u6a21\u578b\u4e2d\u7684\u4e24\u4e2a\u5f0a\u7aef\uff0cBahdanau\u7b49\u4eba\u5728\u8bba\u6587\u300aNeural Machine Translation by Jointly Learning to Align and Translate\u300b\u4e2d\u63d0\u51fa\u4f7f\u7528Attention\u673a\u5236\uff0c\u4f7f\u5f97seq2seq\u6a21\u578b\u53ef\u4ee5\u6709\u533a\u5206\u5ea6\u3001\u6709\u91cd\u70b9\u5730\u5173\u6ce8\u8f93\u5165\u5e8f\u5217\uff0c\u4ece\u800c\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u673a\u5668\u7ffb\u8bd1\u7684\u8d28\u91cf\u3002 \u4e00\u4e2a\u6709\u6ce8\u610f\u529b\u673a\u5236\u7684seq2seq\u4e0e\u7ecf\u5178\u7684seq2seq\u4e3b\u8981\u67092\u70b9\u4e0d\u540c\uff1a 1. \u9996\u5148\uff0c\u7f16\u7801\u5668\u4f1a\u628a\u66f4\u591a\u7684\u6570\u636e\u4f20\u9012\u7ed9\u89e3\u7801\u5668\u3002\u7f16\u7801\u5668\u628a\u6240\u6709\u65f6\u95f4\u6b65\u7684 hidden state\uff08\u9690\u85cf\u5c42\u72b6\u6001\uff09\u4f20\u9012\u7ed9\u89e3\u7801\u5668\uff0c\u800c\u4e0d\u662f\u53ea\u4f20\u9012\u6700\u540e\u4e00\u4e2a hidden state\uff08\u9690\u85cf\u5c42\u72b6\u6001\uff09 2. \u6ce8\u610f\u529b\u6a21\u578b\u7684\u89e3\u7801\u5668\u5728\u4ea7\u751f\u8f93\u51fa\u4e4b\u524d\uff0c\u505a\u4e86\u4e00\u4e2a\u989d\u5916\u7684attention\u5904\u7406","title":"Attention"},{"location":"nlp-transformer-task02/#transformer","text":"","title":"Transformer"},{"location":"nlp-transformer-task02/#_1","text":"transformer\u539f\u8bba\u6587\u7684\u67b6\u6784\u56fe\uff1a \u4e00\u4e2a\u66f4\u6e05\u6670\u7684\u67b6\u6784\u56fe\uff1a \u4ece\u8f93\u5165\u5230\u8f93\u51fa\u62c6\u5f00\u770b\u5c31\u662f\uff1a - INPUT\uff1ainput vector + position encoding - ENCODERs\uff08\u00d76\uff09\uff0cand each encoder includes\uff1a - input - multi-head self-attention - residual connection&norm - full-connected network - residual connection&norm - output - DECODERs\uff08\u00d76\uff09\uff0cand each decoder includes\uff1a - input - Masked multihead self-attention - residual connection&norm - multi-head self-attention - residual connection&norm - full-connected network - residual connection&norm - output - OUTPUT\uff1a - output (decoder's) - linear layer - softmax layer - output","title":"\u6a21\u578b\u67b6\u6784"},{"location":"nlp-transformer-task02/#_2","text":"","title":"\u6a21\u578b\u8f93\u5165"},{"location":"nlp-transformer-task02/#_3","text":"\u548c\u5e38\u89c1\u7684NLP\u4efb\u52a1\u4e00\u6837\uff0c\u6211\u4eec\u9996\u5148\u4f1a\u4f7f\u7528\u8bcd\u5d4c\u5165\u7b97\u6cd5\uff08embedding\uff09\uff0c\u5c06\u8f93\u5165\u6587\u672c\u5e8f\u5217\u7684\u6bcf\u4e2a\u8bcd\u8f6c\u6362\u4e3a\u4e00\u4e2a\u8bcd\u5411\u91cf\u3002","title":"\u8bcd\u5411\u91cf"},{"location":"nlp-transformer-task02/#_4","text":"Transformer\u6a21\u578b\u5bf9\u6bcf\u4e2a\u8f93\u5165\u7684\u8bcd\u5411\u91cf\u90fd\u52a0\u4e0a\u4e86\u4e00\u4e2a\u4f4d\u7f6e\u5411\u91cf\u3002\u8fd9\u4e9b\u5411\u91cf\u6709\u52a9\u4e8e\u786e\u5b9a\u6bcf\u4e2a\u5355\u8bcd\u7684\u4f4d\u7f6e\u7279\u5f81\uff0c\u6216\u8005\u53e5\u5b50\u4e2d\u4e0d\u540c\u5355\u8bcd\u4e4b\u95f4\u7684\u8ddd\u79bb\u7279\u5f81\u3002\u8bcd\u5411\u91cf\u52a0\u4e0a\u4f4d\u7f6e\u5411\u91cf\u80cc\u540e\u7684\u76f4\u89c9\u662f\uff1a\u5c06\u8fd9\u4e9b\u8868\u793a\u4f4d\u7f6e\u7684\u5411\u91cf\u6dfb\u52a0\u5230\u8bcd\u5411\u91cf\u4e2d\uff0c\u5f97\u5230\u7684\u65b0\u5411\u91cf\uff0c\u53ef\u4ee5\u4e3a\u6a21\u578b\u63d0\u4f9b\u66f4\u591a\u6709\u610f\u4e49\u7684\u4fe1\u606f\uff0c\u6bd4\u5982\u8bcd\u7684\u4f4d\u7f6e\uff0c\u8bcd\u4e4b\u95f4\u7684\u8ddd\u79bb\u7b49\u3002 \uff08\u751f\u6210\u4f4d\u7f6e\u7f16\u7801\u5411\u91cf\u7684\u65b9\u6cd5\u6709\u5f88\u591a\u79cd\uff09","title":"\u4f4d\u7f6e\u5411\u91cf"},{"location":"nlp-transformer-task02/#_5","text":"\u6ce8\uff1a1. \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e2d\u6709\u76f8\u4f3c\u7684\u6a21\u5757\u548c\u7ed3\u6784\uff0c\u6240\u4ee5\u5408\u5e76\u5230\u4e00\u8d77\u4ecb\u7ecd\u3002 2. \u672c\u90e8\u5206\u6309\u7167\u674e\u5b8f\u6bc5\u8001\u5e08\u7684Attention\uff0cTransformer\u90e8\u5206\u7684\u8bfe\u7a0bPPT\u6765\uff0c\u56e0\u4e3alee\u7684\u8bfe\u7a0b\u5bf9\u65b0\u624b\u66f4\u53cb\u597d\u3002","title":"\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668"},{"location":"nlp-transformer-task02/#self-attention","text":"self-attention\u5bf9\u4e8e\u6bcf\u4e2a\u5411\u91cf\u90fd\u4f1a\u8003\u8651\u6574\u4e2asequence\u7684\u4fe1\u606f\u540e\u8f93\u51fa\u4e00\u4e2a\u5411\u91cf\uff0cself-attention\u7ed3\u6784\u5982\u4e0b\uff1a FC\uff1aFully-connected network \u5168\u8fde\u63a5\u7f51\u7edc ai: \u8f93\u5165\u53d8\u91cf\u3002\u53ef\u80fd\u662f\u6574\u4e2a\u7f51\u7edc\u7684\u8f93\u5165\uff0c\u4e5f\u53ef\u80fd\u662f\u67d0\u4e2a\u9690\u85cf\u5c42\u7684\u8f93\u51fa bi: \u8003\u8651\u6574\u4e2asequence\u4fe1\u606f\u540e\u7684\u8f93\u51fa\u53d8\u91cf \u77e9\u9635\u8ba1\u7b97\uff1a \u76ee\u6807\uff1a\u6839\u636e\u8f93\u5165\u5411\u91cf\u77e9\u9635I\uff0c\u8ba1\u7b97\u8f93\u51fa\u5411\u91cf\u77e9\u9635O\u3002\u77e9\u9635\u8fd0\u7b97\u8fc7\u7a0b\uff1a 1. \u77e9\u9635I\u5206\u522b\u4e58\u4ee5Wq, Wk, Wv\uff08\u53c2\u6570\u77e9\u9635\uff0c\u9700\u8981\u6a21\u578b\u8fdb\u884c\u5b66\u4e60\uff09\uff0c\u5f97\u5230\u77e9\u9635Q, K, V\u3002 2. \u77e9\u9635K\u7684\u8f6c\u7f6e\u4e58\u4ee5Q\uff0c\u5f97\u5230\u6ce8\u610f\u529b\u6743\u91cd\u77e9\u9635A\uff0c\u5f52\u4e00\u5316\u5f97\u5230\u77e9\u9635A\u2019\u3002 3. \u77e9\u9635V\u4e58\u77e9\u9635A\u2018\uff0c\u5f97\u5230\u8f93\u51fa\u5411\u91cf\u77e9\u9635O\u3002","title":"Self-Attention"},{"location":"nlp-transformer-task02/#multi-head-self-attention","text":"\u7b80\u5355\u5730\u8bf4\uff0c\u591a\u4e86\u51e0\u7ec4Q\uff0cK\uff0cV\u3002\u5728Self-Attention\u4e2d\uff0c\u6211\u4eec\u662f\u4f7f\u7528\ud835\udc5e\u53bb\u5bfb\u627e\u4e0e\u4e4b\u76f8\u5173\u7684\ud835\udc58\uff0c\u4f46\u662f\u8fd9\u4e2a\u76f8\u5173\u6027\u5e76\u4e0d\u4e00\u5b9a\u6709\u4e00\u79cd\u3002\u90a3\u591a\u79cd\u76f8\u5173\u6027\u4f53\u73b0\u5230\u8ba1\u7b97\u65b9\u5f0f\u4e0a\u5c31\u662f\u6709\u591a\u4e2a\u77e9\u9635\ud835\udc5e\uff0c\u4e0d\u540c\u7684\ud835\udc5e\u8d1f\u8d23\u4ee3\u8868\u4e0d\u540c\u7684\u76f8\u5173\u6027\u3002 Transformer \u7684\u8bba\u6587\u901a\u8fc7\u589e\u52a0\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff08\u4e00\u7ec4\u6ce8\u610f\u529b\u79f0\u4e3a\u4e00\u4e2a attention head\uff09\uff0c\u8fdb\u4e00\u6b65\u5b8c\u5584\u4e86Self-Attention\u3002\u8fd9\u79cd\u673a\u5236\u4ece\u5982\u4e0b\u4e24\u4e2a\u65b9\u9762\u589e\u5f3a\u4e86attention\u5c42\u7684\u80fd\u529b\uff1a - \u5b83\u6269\u5c55\u4e86\u6a21\u578b\u5173\u6ce8\u4e0d\u540c\u4f4d\u7f6e\u7684\u80fd\u529b\u3002 - \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u8d4b\u4e88attention\u5c42\u591a\u4e2a\u201c\u5b50\u8868\u793a\u7a7a\u95f4\u201d\u3002","title":"Multi Head Self-Attention"},{"location":"nlp-transformer-task02/#_6","text":"\u6b8b\u5dee\u94fe\u63a5\uff1a\u4e00\u79cd\u628ainput\u5411\u91cf\u548coutput\u5411\u91cf\u76f4\u63a5\u52a0\u8d77\u6765\u7684\u67b6\u6784\u3002 \u5f52\u4e00\u5316\uff1a\u628a\u6570\u636e\u6620\u5c04\u52300\uff5e1\u8303\u56f4\u4e4b\u5185\u5904\u7406\u3002","title":"\u6b8b\u5dee\u94fe\u63a5\u548c\u5f52\u4e00\u5316"},{"location":"nlp-transformer-task02/#_7","text":"","title":"\u6a21\u578b\u8f93\u51fa"},{"location":"nlp-transformer-task02/#softmax","text":"Decoder \u6700\u7ec8\u7684\u8f93\u51fa\u662f\u4e00\u4e2a\u5411\u91cf\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5143\u7d20\u662f\u6d6e\u70b9\u6570\u3002\u6211\u4eec\u600e\u4e48\u628a\u8fd9\u4e2a\u5411\u91cf\u8f6c\u6362\u4e3a\u5355\u8bcd\u5462\uff1f\u8fd9\u662f\u7ebf\u6027\u5c42\u548csoftmax\u5b8c\u6210\u7684\u3002 \u7ebf\u6027\u5c42\u5c31\u662f\u4e00\u4e2a\u666e\u901a\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\uff0c\u53ef\u4ee5\u628a\u89e3\u7801\u5668\u8f93\u51fa\u7684\u5411\u91cf\uff0c\u6620\u5c04\u5230\u4e00\u4e2a\u66f4\u5927\u7684\u5411\u91cf\uff0c\u8fd9\u4e2a\u5411\u91cf\u79f0\u4e3a logits \u5411\u91cf\uff1a\u5047\u8bbe\u6211\u4eec\u7684\u6a21\u578b\u6709 10000 \u4e2a\u82f1\u8bed\u5355\u8bcd\uff08\u6a21\u578b\u7684\u8f93\u51fa\u8bcd\u6c47\u8868\uff09\uff0c\u6b64 logits \u5411\u91cf\u4fbf\u4f1a\u6709 10000 \u4e2a\u6570\u5b57\uff0c\u6bcf\u4e2a\u6570\u8868\u793a\u4e00\u4e2a\u5355\u8bcd\u7684\u5206\u6570\u3002 \u7136\u540e\uff0cSoftmax \u5c42\u4f1a\u628a\u8fd9\u4e9b\u5206\u6570\u8f6c\u6362\u4e3a\u6982\u7387\uff08\u628a\u6240\u6709\u7684\u5206\u6570\u8f6c\u6362\u4e3a\u6b63\u6570\uff0c\u5e76\u4e14\u52a0\u8d77\u6765\u7b49\u4e8e 1\uff09\u3002\u7136\u540e\u9009\u62e9\u6700\u9ad8\u6982\u7387\u7684\u90a3\u4e2a\u6570\u5b57\u5bf9\u5e94\u7684\u8bcd\uff0c\u5c31\u662f\u8fd9\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u5355\u8bcd\u3002","title":"\u7ebf\u6027\u5c42\u548csoftmax"},{"location":"nlp-transformer-task02/#_8","text":"Transformer\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u9700\u8981\u5c06\u89e3\u7801\u5668\u7684\u8f93\u51fa\u548clabel\u4e00\u540c\u9001\u5165\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u83b7\u5f97loss\uff0c\u6700\u7ec8\u6a21\u578b\u6839\u636eloss\u8fdb\u884c\u65b9\u5411\u4f20\u64ad\u3002 \u53ea\u8981Transformer\u89e3\u7801\u5668\u9884\u6d4b\u4e86\u7ec4\u6982\u7387\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u628a\u8fd9\u7ec4\u6982\u7387\u548c\u6b63\u786e\u7684\u8f93\u51fa\u6982\u7387\u505a\u5bf9\u6bd4\uff0c\u7136\u540e\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u6765\u8c03\u6574\u6a21\u578b\u7684\u6743\u91cd\uff0c\u4f7f\u5f97\u8f93\u51fa\u7684\u6982\u7387\u5206\u5e03\u66f4\u52a0\u63a5\u8fd1\u6574\u6570\u8f93\u51fa\u3002 \u90a3\u6211\u4eec\u8981\u600e\u4e48\u6bd4\u8f83\u4e24\u4e2a\u6982\u7387\u5206\u5e03\u5462\uff1f\uff1a\u6211\u4eec\u53ef\u4ee5\u7b80\u5355\u7684\u7528\u4e24\u7ec4\u6982\u7387\u5411\u91cf\u7684\u7684\u7a7a\u95f4\u8ddd\u79bb\u4f5c\u4e3aloss\uff08\u5411\u91cf\u76f8\u51cf\uff0c\u7136\u540e\u6c42\u5e73\u65b9\u548c\uff0c\u518d\u5f00\u65b9\uff09\uff0c\u5f53\u7136\u4e5f\u53ef\u4ee5\u4f7f\u7528\u4ea4\u53c9\u71b5(cross-entropy)]\u548cKL \u6563\u5ea6(Kullback\u2013Leibler divergence)\u3002","title":"\u635f\u5931\u51fd\u6570"},{"location":"nlp-transformer-task02/#_9","text":"\u7406\u8bba\u90e8\u5206 [1] (\u5f3a\u63a8)\u674e\u5b8f\u6bc52021\u6625\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b https://www.bilibili.com/video/BV1Wv411h7kN?from=search&seid=17090062977285779802&spm_id_from=333.337.0.0 [2] \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8\uff08\u6db5\u76d6\u4e86\u56fe\u89e3\u7cfb\u5217\u3001annotated transformer\u3001huggingface\uff09 https://github.com/datawhalechina/learn-nlp-with-transformers [3] \u56fe\u89e3transformer|The Illustrated Transformer http://jalammar.github.io/illustrated-transformer/ [4] \u56fe\u89e3seq2seq, attention|Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ \u4ee3\u7801\u90e8\u5206 [5] The Annotated Transformer http://nlp.seas.harvard.edu//2018/04/03/attention.html [6] Huggingface/transformers https://github.com/huggingface/transformers/blob/master/README_zh-hans.md \u8bba\u6587\u90e8\u5206 Attention is all \"we\" need. \u5176\u4ed6\u4e0d\u9519\u7684\u535a\u5ba2\u6216\u6559\u7a0b [7] \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8--\u5728\u7ebf\u9605\u8bfb https://datawhalechina.github.io/learn-nlp-with-transformers/#/ [8] \u674e\u5b8f\u6bc52021\u6625\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b\u7b14\u8bb0\u2014\u2014\u81ea\u6ce8\u610f\u529b\u673a\u5236 https://www.cnblogs.com/sykline/p/14730088.html [9] \u674e\u5b8f\u6bc52021\u6625\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b\u7b14\u8bb0\u2014\u2014Transformer\u6a21\u578b https://www.cnblogs.com/sykline/p/14785552.html [10] \u674e\u5b8f\u6bc5\u673a\u5668\u5b66\u4e60\u5b66\u4e60\u7b14\u8bb0\u2014\u2014\u81ea\u6ce8\u610f\u529b\u673a\u5236 https://blog.csdn.net/p_memory/article/details/116271274 [11] \u8f66\u4e07\u7fd4-\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b0\u8303\u5f0f\uff1a\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u65b9\u6cd5\u3010\u8bb2\u5ea7+PPT\u3011 https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true [12] \u82cf\u5251\u6797-\u300aAttention is All You Need\u300b\u6d45\u8bfb\uff08\u7b80\u4ecb+\u4ee3\u7801\uff09 https://spaces.ac.cn/archives/4765","title":"\u53c2\u8003\u8d44\u6599"},{"location":"nlp-transformer-task03/","text":"Task03 \u5b66\u4e60BERT \u00b6 BERT\u7b80\u4ecb \u00b6 BERT\u9996\u5148\u5728\u5927\u89c4\u6a21\u65e0\u76d1\u7763\u8bed\u6599\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u9884\u8bad\u7ec3\u597d\u7684\u53c2\u6570\u57fa\u7840\u4e0a\u589e\u52a0\u4e00\u4e2a\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u5e76\u5728\u8be5\u4efb\u52a1\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u8bad\uff0c\u6700\u7ec8\u53d6\u5f97\u5f88\u597d\u7684\u6548\u679c\u3002 BERT\u7684\u8fd9\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u53ef\u4ee5\u7b80\u8ff0\u4e3a\uff1a\u9884\u8bad\u7ec3\uff08pre-train\uff09+\u5fae\u8c03\uff08fine-tune/fine-tuning\uff09\uff0c\u5df2\u7ecf\u6210\u4e3a\u6700\u8fd1\u51e0\u5e74\u6700\u6d41\u884c\u7684NLP\u89e3\u51b3\u65b9\u6848\u7684\u8303\u5f0f\u3002 \u5982\u4f55\u76f4\u63a5\u5e94\u7528BERT \u00b6 \u4e0b\u8f7d\u5728\u65e0\u76d1\u7763\u8bed\u6599\u4e0a\u9884\u8bad\u7ec3\u597d\u7684BERT\u6a21\u578b\uff0c\u4e00\u822c\u6765\u8bf4\u5bf9\u5e94\u4e863\u4e2a\u6587\u4ef6\uff1aBERT\u6a21\u578b\u914d\u7f6e\u6587\u4ef6\uff08\u7528\u6765\u786e\u5b9aTransformer\u7684\u5c42\u6570\uff0c\u9690\u85cf\u5c42\u5927\u5c0f\u7b49\uff09\uff0cBERT\u6a21\u578b\u53c2\u6570\uff0cBERT\u8bcd\u8868\uff08BERT\u6240\u80fd\u5904\u7406\u7684\u6240\u6709token\uff09\u3002 \u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u9700\u8981\uff0c\u5728BERT\u6a21\u578b\u4e0a\u589e\u52a0\u4e00\u4e2a\u4efb\u52a1\u76f8\u5173\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u6bd4\u5982\u4e00\u4e2a\u7b80\u5355\u7684\u5206\u7c7b\u5668\uff0c\u7136\u540e\u5728\u7279\u5b9a\u4efb\u52a1\u76d1\u7763\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u8bad\u7ec3\u3002\uff08\u5fae\u8c03\u7684\u4e00\u79cd\u7406\u89e3\uff1a\u5b66\u4e60\u7387\u8f83\u5c0f\uff0c\u8bad\u7ec3epoch\u6570\u91cf\u8f83\u5c11\uff0c\u5bf9\u6a21\u578b\u6574\u4f53\u53c2\u6570\u8fdb\u884c\u8f7b\u5fae\u8c03\u6574\uff09 BERT\u7684\u7ed3\u6784 \u00b6 BERT\u6a21\u578b\u7ed3\u6784\u57fa\u672c\u4e0a\u5c31\u662fTransformer\u7684encoder\u90e8\u5206\u3002 BERT\u7684\u8f93\u5165\u548c\u8f93\u51fa \u00b6 BERT\u6a21\u578b\u8f93\u5165\u6709\u4e00\u70b9\u7279\u6b8a\u7684\u5730\u65b9\u662f\u5728\u4e00\u53e5\u8bdd\u6700\u5f00\u59cb\u62fc\u63a5\u4e86\u4e00\u4e2a[CLS] token\uff0c\u5982\u4e0b\u56fe\u6240\u793a\u3002\u8fd9\u4e2a\u7279\u6b8a\u7684[CLS] token\u7ecf\u8fc7BERT\u5f97\u5230\u7684\u5411\u91cf\u8868\u793a\u901a\u5e38\u88ab\u7528\u4f5c\u5f53\u524d\u7684\u53e5\u5b50\u8868\u793a\u3002\u6211\u4eec\u76f4\u63a5\u4f7f\u7528\u7b2c1\u4e2a\u4f4d\u7f6e\u7684\u5411\u91cf\u8f93\u51fa\uff08\u5bf9\u5e94\u7684\u662f[CLS]\uff09\u4f20\u5165classifier\u7f51\u7edc\uff0c\u7136\u540e\u8fdb\u884c\u5206\u7c7b\u4efb\u52a1\u3002 BERT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1 \u00b6 BERT\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u5b83\u7684\u4efb\u52a1\u662f\u7531\u4e24\u4e2a\u81ea\u76d1\u7763\u4efb\u52a1\u7ec4\u6210\u3002 Masked Language Model\uff08MLM\uff09 \u00b6 MLM\uff1a\u5c06\u8f93\u5165\u6587\u672c\u5e8f\u5217\u7684\u90e8\u5206\uff0815%\uff09\u5355\u8bcd\u968f\u673aMask\u6389\uff0c\u8ba9BERT\u6765\u9884\u6d4b\u8fd9\u4e9b\u88abMask\u7684\u8bcd\u8bed\u3002 \uff08\u53ef\u4ee5\u8bf4\u662f\u5b8c\u5f62\u586b\u7a7a\uff09 Masked Language Model\uff08MLM\uff09\u548c\u6838\u5fc3\u601d\u60f3\u53d6\u81eaWilson Taylor\u57281953\u5e74\u53d1\u8868\u7684\u4e00\u7bc7\u8bba\u6587\u300acloze procedure: A new tool for measuring readability\u300b\u3002\u6240\u8c13MLM\u662f\u6307\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u968f\u5373\u4ece\u8f93\u5165\u9884\u6599\u4e0amask\u6389\u4e00\u4e9b\u5355\u8bcd\uff0c\u7136\u540e\u901a\u8fc7\u7684\u4e0a\u4e0b\u6587\u9884\u6d4b\u8be5\u5355\u8bcd\uff0c\u8be5\u4efb\u52a1\u975e\u5e38\u50cf\u6211\u4eec\u5728\u4e2d\u5b66\u65f6\u671f\u7ecf\u5e38\u505a\u7684\u5b8c\u5f62\u586b\u7a7a\u3002\u6b63\u5982\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u7b97\u6cd5\u548cRNN\u5339\u914d\u90a3\u6837\uff0cMLM\u7684\u8fd9\u4e2a\u6027\u8d28\u548cTransformer\u7684\u7ed3\u6784\u662f\u975e\u5e38\u5339\u914d\u7684\u3002 Next Sentence Prediction\uff08NSP\uff09 \u00b6 NSP\uff1a\u5224\u65ad\u4e24\u4e2a\u53e5\u5b50\u662f\u5426\u662f\u76f8\u90bb\u53e5\u5b50\u3002\u5373\uff0c\u8f93\u5165\u662fsentence A\u548csentence B\uff0c\u7ecf\u8fc7BERT\u7f16\u7801\u4e4b\u540e\uff0c\u4f7f\u7528CLS token\u7684\u5411\u91cf\u8868\u793a\u6765\u9884\u6d4b\u4e24\u4e2a\u53e5\u5b50\u662f\u5426\u662f\u76f8\u90bb\u53e5\u5b50\u3002 Next Sentence Prediction\uff08NSP\uff09\u7684\u4efb\u52a1\u662f\u5224\u65ad\u53e5\u5b50B\u662f\u5426\u662f\u53e5\u5b50A\u7684\u4e0b\u6587\u3002\u5982\u679c\u662f\u7684\u8bdd\u8f93\u51fa\u2019IsNext\u2018\uff0c\u5426\u5219\u8f93\u51fa\u2019NotNext\u2018\u3002\u8bad\u7ec3\u6570\u636e\u7684\u751f\u6210\u65b9\u5f0f\u662f\u4ece\u5e73\u884c\u8bed\u6599\u4e2d\u968f\u673a\u62bd\u53d6\u7684\u8fde\u7eed\u4e24\u53e5\u8bdd\uff0c\u5176\u4e2d50%\u4fdd\u7559\u62bd\u53d6\u7684\u4e24\u53e5\u8bdd\uff0c\u5b83\u4eec\u7b26\u5408IsNext\u5173\u7cfb\uff0c\u53e6\u591650%\u7684\u7b2c\u4e8c\u53e5\u8bdd\u662f\u968f\u673a\u4ece\u9884\u6599\u4e2d\u63d0\u53d6\u7684\uff0c\u5b83\u4eec\u7684\u5173\u7cfb\u662fNotNext\u7684\u3002\u8fd9\u4e2a\u5173\u7cfb\u4fdd\u5b58\u5728[CLS]\u7b26\u53f7\u4e2d\u3002 BERT\u7684\u5e94\u7528 \u00b6 \u7279\u5f81\u63d0\u53d6 \u00b6 \u7531\u4e8eBERT\u6a21\u578b\u53ef\u4ee5\u5f97\u5230\u8f93\u5165\u5e8f\u5217\u6240\u5bf9\u5e94\u7684\u6240\u6709token\u7684\u5411\u91cf\u8868\u793a\uff0c\u56e0\u6b64\u4e0d\u4ec5\u53ef\u4ee5\u4f7f\u7528\u6700\u540e\u4e00\u7a0bBERT\u7684\u8f93\u51fa\u8fde\u63a5\u4e0a\u4efb\u52a1\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\uff0c\u8fd8\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u8fd9\u4e9btoken\u7684\u5411\u91cf\u5f53\u4f5c\u7279\u5f81\u3002\u6bd4\u5982\uff0c\u53ef\u4ee5\u76f4\u63a5\u63d0\u53d6\u6bcf\u4e00\u5c42encoder\u7684token\u8868\u793a\u5f53\u4f5c\u7279\u5f81\uff0c\u8f93\u5165\u73b0\u6709\u7684\u7279\u5b9a\u4efb\u52a1\u795e\u7ecf\u7f51\u7edc\u4e2d\u8fdb\u884c\u8bad\u7ec3\u3002 Pretrain + Fine tune \u00b6 \u53c2\u8003\u8d44\u6599 \u00b6 \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8--\u5728\u7ebf\u9605\u8bfb https://datawhalechina.github.io/learn-nlp-with-transformers/#/ \u674e\u5b8f\u6bc5\u673a\u5668\u5b66\u4e602019-ELMO,BERT,GPT https:// www.bilibili.com/video/BV1Gb411n7dE?p=61","title":"Task03 \u5b66\u4e60BERT"},{"location":"nlp-transformer-task03/#task03-bert","text":"","title":"Task03 \u5b66\u4e60BERT"},{"location":"nlp-transformer-task03/#bert","text":"BERT\u9996\u5148\u5728\u5927\u89c4\u6a21\u65e0\u76d1\u7763\u8bed\u6599\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u9884\u8bad\u7ec3\u597d\u7684\u53c2\u6570\u57fa\u7840\u4e0a\u589e\u52a0\u4e00\u4e2a\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u5e76\u5728\u8be5\u4efb\u52a1\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u8bad\uff0c\u6700\u7ec8\u53d6\u5f97\u5f88\u597d\u7684\u6548\u679c\u3002 BERT\u7684\u8fd9\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u53ef\u4ee5\u7b80\u8ff0\u4e3a\uff1a\u9884\u8bad\u7ec3\uff08pre-train\uff09+\u5fae\u8c03\uff08fine-tune/fine-tuning\uff09\uff0c\u5df2\u7ecf\u6210\u4e3a\u6700\u8fd1\u51e0\u5e74\u6700\u6d41\u884c\u7684NLP\u89e3\u51b3\u65b9\u6848\u7684\u8303\u5f0f\u3002","title":"BERT\u7b80\u4ecb"},{"location":"nlp-transformer-task03/#bert_1","text":"\u4e0b\u8f7d\u5728\u65e0\u76d1\u7763\u8bed\u6599\u4e0a\u9884\u8bad\u7ec3\u597d\u7684BERT\u6a21\u578b\uff0c\u4e00\u822c\u6765\u8bf4\u5bf9\u5e94\u4e863\u4e2a\u6587\u4ef6\uff1aBERT\u6a21\u578b\u914d\u7f6e\u6587\u4ef6\uff08\u7528\u6765\u786e\u5b9aTransformer\u7684\u5c42\u6570\uff0c\u9690\u85cf\u5c42\u5927\u5c0f\u7b49\uff09\uff0cBERT\u6a21\u578b\u53c2\u6570\uff0cBERT\u8bcd\u8868\uff08BERT\u6240\u80fd\u5904\u7406\u7684\u6240\u6709token\uff09\u3002 \u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u9700\u8981\uff0c\u5728BERT\u6a21\u578b\u4e0a\u589e\u52a0\u4e00\u4e2a\u4efb\u52a1\u76f8\u5173\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u6bd4\u5982\u4e00\u4e2a\u7b80\u5355\u7684\u5206\u7c7b\u5668\uff0c\u7136\u540e\u5728\u7279\u5b9a\u4efb\u52a1\u76d1\u7763\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u8bad\u7ec3\u3002\uff08\u5fae\u8c03\u7684\u4e00\u79cd\u7406\u89e3\uff1a\u5b66\u4e60\u7387\u8f83\u5c0f\uff0c\u8bad\u7ec3epoch\u6570\u91cf\u8f83\u5c11\uff0c\u5bf9\u6a21\u578b\u6574\u4f53\u53c2\u6570\u8fdb\u884c\u8f7b\u5fae\u8c03\u6574\uff09","title":"\u5982\u4f55\u76f4\u63a5\u5e94\u7528BERT"},{"location":"nlp-transformer-task03/#bert_2","text":"BERT\u6a21\u578b\u7ed3\u6784\u57fa\u672c\u4e0a\u5c31\u662fTransformer\u7684encoder\u90e8\u5206\u3002","title":"BERT\u7684\u7ed3\u6784"},{"location":"nlp-transformer-task03/#bert_3","text":"BERT\u6a21\u578b\u8f93\u5165\u6709\u4e00\u70b9\u7279\u6b8a\u7684\u5730\u65b9\u662f\u5728\u4e00\u53e5\u8bdd\u6700\u5f00\u59cb\u62fc\u63a5\u4e86\u4e00\u4e2a[CLS] token\uff0c\u5982\u4e0b\u56fe\u6240\u793a\u3002\u8fd9\u4e2a\u7279\u6b8a\u7684[CLS] token\u7ecf\u8fc7BERT\u5f97\u5230\u7684\u5411\u91cf\u8868\u793a\u901a\u5e38\u88ab\u7528\u4f5c\u5f53\u524d\u7684\u53e5\u5b50\u8868\u793a\u3002\u6211\u4eec\u76f4\u63a5\u4f7f\u7528\u7b2c1\u4e2a\u4f4d\u7f6e\u7684\u5411\u91cf\u8f93\u51fa\uff08\u5bf9\u5e94\u7684\u662f[CLS]\uff09\u4f20\u5165classifier\u7f51\u7edc\uff0c\u7136\u540e\u8fdb\u884c\u5206\u7c7b\u4efb\u52a1\u3002","title":"BERT\u7684\u8f93\u5165\u548c\u8f93\u51fa"},{"location":"nlp-transformer-task03/#bert_4","text":"BERT\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u5b83\u7684\u4efb\u52a1\u662f\u7531\u4e24\u4e2a\u81ea\u76d1\u7763\u4efb\u52a1\u7ec4\u6210\u3002","title":"BERT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1"},{"location":"nlp-transformer-task03/#masked-language-modelmlm","text":"MLM\uff1a\u5c06\u8f93\u5165\u6587\u672c\u5e8f\u5217\u7684\u90e8\u5206\uff0815%\uff09\u5355\u8bcd\u968f\u673aMask\u6389\uff0c\u8ba9BERT\u6765\u9884\u6d4b\u8fd9\u4e9b\u88abMask\u7684\u8bcd\u8bed\u3002 \uff08\u53ef\u4ee5\u8bf4\u662f\u5b8c\u5f62\u586b\u7a7a\uff09 Masked Language Model\uff08MLM\uff09\u548c\u6838\u5fc3\u601d\u60f3\u53d6\u81eaWilson Taylor\u57281953\u5e74\u53d1\u8868\u7684\u4e00\u7bc7\u8bba\u6587\u300acloze procedure: A new tool for measuring readability\u300b\u3002\u6240\u8c13MLM\u662f\u6307\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u968f\u5373\u4ece\u8f93\u5165\u9884\u6599\u4e0amask\u6389\u4e00\u4e9b\u5355\u8bcd\uff0c\u7136\u540e\u901a\u8fc7\u7684\u4e0a\u4e0b\u6587\u9884\u6d4b\u8be5\u5355\u8bcd\uff0c\u8be5\u4efb\u52a1\u975e\u5e38\u50cf\u6211\u4eec\u5728\u4e2d\u5b66\u65f6\u671f\u7ecf\u5e38\u505a\u7684\u5b8c\u5f62\u586b\u7a7a\u3002\u6b63\u5982\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u7b97\u6cd5\u548cRNN\u5339\u914d\u90a3\u6837\uff0cMLM\u7684\u8fd9\u4e2a\u6027\u8d28\u548cTransformer\u7684\u7ed3\u6784\u662f\u975e\u5e38\u5339\u914d\u7684\u3002","title":"Masked Language Model\uff08MLM\uff09"},{"location":"nlp-transformer-task03/#next-sentence-predictionnsp","text":"NSP\uff1a\u5224\u65ad\u4e24\u4e2a\u53e5\u5b50\u662f\u5426\u662f\u76f8\u90bb\u53e5\u5b50\u3002\u5373\uff0c\u8f93\u5165\u662fsentence A\u548csentence B\uff0c\u7ecf\u8fc7BERT\u7f16\u7801\u4e4b\u540e\uff0c\u4f7f\u7528CLS token\u7684\u5411\u91cf\u8868\u793a\u6765\u9884\u6d4b\u4e24\u4e2a\u53e5\u5b50\u662f\u5426\u662f\u76f8\u90bb\u53e5\u5b50\u3002 Next Sentence Prediction\uff08NSP\uff09\u7684\u4efb\u52a1\u662f\u5224\u65ad\u53e5\u5b50B\u662f\u5426\u662f\u53e5\u5b50A\u7684\u4e0b\u6587\u3002\u5982\u679c\u662f\u7684\u8bdd\u8f93\u51fa\u2019IsNext\u2018\uff0c\u5426\u5219\u8f93\u51fa\u2019NotNext\u2018\u3002\u8bad\u7ec3\u6570\u636e\u7684\u751f\u6210\u65b9\u5f0f\u662f\u4ece\u5e73\u884c\u8bed\u6599\u4e2d\u968f\u673a\u62bd\u53d6\u7684\u8fde\u7eed\u4e24\u53e5\u8bdd\uff0c\u5176\u4e2d50%\u4fdd\u7559\u62bd\u53d6\u7684\u4e24\u53e5\u8bdd\uff0c\u5b83\u4eec\u7b26\u5408IsNext\u5173\u7cfb\uff0c\u53e6\u591650%\u7684\u7b2c\u4e8c\u53e5\u8bdd\u662f\u968f\u673a\u4ece\u9884\u6599\u4e2d\u63d0\u53d6\u7684\uff0c\u5b83\u4eec\u7684\u5173\u7cfb\u662fNotNext\u7684\u3002\u8fd9\u4e2a\u5173\u7cfb\u4fdd\u5b58\u5728[CLS]\u7b26\u53f7\u4e2d\u3002","title":"Next Sentence Prediction\uff08NSP\uff09"},{"location":"nlp-transformer-task03/#bert_5","text":"","title":"BERT\u7684\u5e94\u7528"},{"location":"nlp-transformer-task03/#_1","text":"\u7531\u4e8eBERT\u6a21\u578b\u53ef\u4ee5\u5f97\u5230\u8f93\u5165\u5e8f\u5217\u6240\u5bf9\u5e94\u7684\u6240\u6709token\u7684\u5411\u91cf\u8868\u793a\uff0c\u56e0\u6b64\u4e0d\u4ec5\u53ef\u4ee5\u4f7f\u7528\u6700\u540e\u4e00\u7a0bBERT\u7684\u8f93\u51fa\u8fde\u63a5\u4e0a\u4efb\u52a1\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\uff0c\u8fd8\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u8fd9\u4e9btoken\u7684\u5411\u91cf\u5f53\u4f5c\u7279\u5f81\u3002\u6bd4\u5982\uff0c\u53ef\u4ee5\u76f4\u63a5\u63d0\u53d6\u6bcf\u4e00\u5c42encoder\u7684token\u8868\u793a\u5f53\u4f5c\u7279\u5f81\uff0c\u8f93\u5165\u73b0\u6709\u7684\u7279\u5b9a\u4efb\u52a1\u795e\u7ecf\u7f51\u7edc\u4e2d\u8fdb\u884c\u8bad\u7ec3\u3002","title":"\u7279\u5f81\u63d0\u53d6"},{"location":"nlp-transformer-task03/#pretrain-fine-tune","text":"","title":"Pretrain + Fine tune"},{"location":"nlp-transformer-task03/#_2","text":"\u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8--\u5728\u7ebf\u9605\u8bfb https://datawhalechina.github.io/learn-nlp-with-transformers/#/ \u674e\u5b8f\u6bc5\u673a\u5668\u5b66\u4e602019-ELMO,BERT,GPT https:// www.bilibili.com/video/BV1Gb411n7dE?p=61","title":"\u53c2\u8003\u8d44\u6599"},{"location":"nlp-transformer-task04/","text":"Task04 \u5b66\u4e60GPT \u00b6 \u4ece\u8bed\u8a00\u6a21\u578b\u8bf4\u8d77 \u00b6 \u81ea\u7f16\u7801\u8bed\u8a00\u6a21\u578b\uff08auto-encoder\uff09 \u00b6 \u81ea\u7f16\u7801\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u968f\u673aMask\u8f93\u5165\u7684\u90e8\u5206\u5355\u8bcd\uff0c\u7136\u540e\u9884\u8bad\u7ec3\u7684\u76ee\u6807\u662f\u9884\u6d4b\u88abMask\u7684\u5355\u8bcd\uff0c\u4e0d\u4ec5\u53ef\u4ee5\u878d\u5165\u4e0a\u6587\u4fe1\u606f\uff0c\u8fd8\u53ef\u4ee5\u81ea\u7136\u7684\u878d\u5165\u4e0b\u6587\u4fe1\u606f\u3002ex. BERT. - \u4f18\u70b9\uff1a\u81ea\u7136\u5730\u878d\u5165\u53cc\u5411\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u770b\u5230\u88ab\u9884\u6d4b\u5355\u8bcd\u7684\u4e0a\u6587\u548c\u4e0b\u6587 - \u7f3a\u70b9\uff1a\u8bad\u7ec3\u548c\u9884\u6d4b\u4e0d\u4e00\u81f4\u3002\u8bad\u7ec3\u7684\u65f6\u5019\u8f93\u5165\u5f15\u5165\u4e86[Mask]\u6807\u8bb0\uff0c\u4f46\u662f\u5728\u9884\u6d4b\u9636\u6bb5\u5f80\u5f80\u6ca1\u6709\u8fd9\u4e2a[Mask]\u6807\u8bb0\uff0c\u5bfc\u81f4\u9884\u8bad\u7ec3\u9636\u6bb5\u548cFine-tuning\u9636\u6bb5\u4e0d\u4e00\u81f4\u3002 \u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\uff08auto-regressive\uff09 \u00b6 \u8bed\u8a00\u6a21\u578b\u6839\u636e\u8f93\u5165\u53e5\u5b50\u7684\u4e00\u90e8\u5206\u6587\u672c\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u3002ex. GPT-2 - \u4f18\u70b9\uff1a\u5bf9\u4e8e\u751f\u6210\u7c7b\u7684NLP\u4efb\u52a1\uff0c\u6bd4\u5982\u6587\u672c\u6458\u8981\uff0c\u673a\u5668\u7ffb\u8bd1\u7b49\uff0c\u4ece\u5de6\u5411\u53f3\u7684\u751f\u6210\u5185\u5bb9\uff0c\u5929\u7136\u548c\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5951\u5408\u3002 - \u7f3a\u70b9\uff1a\u7531\u4e8e\u4e00\u822c\u662f\u4ece\u5de6\u5230\u53f3\uff08\u5f53\u7136\u4e5f\u53ef\u80fd\u4ece\u53f3\u5230\u5de6\uff09\uff0c\u6240\u4ee5\u53ea\u80fd\u5229\u7528\u4e0a\u6587\u6216\u8005\u4e0b\u6587\u7684\u4fe1\u606f\uff0c\u4e0d\u80fd\u540c\u65f6\u5229\u7528\u4e0a\u6587\u548c\u4e0b\u6587\u7684\u4fe1\u606f\u3002 Transformer, BERT, GPT-2\u7684\u5173\u7cfb \u00b6 Transformer\u7684Encoder\u8fdb\u5316\u6210\u4e86BERT\uff0cDecoder\u8fdb\u5316\u6210\u4e86GPT2\u3002 \u5982\u679c\u8981\u4f7f\u7528Transformer\u6765\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\uff0c\u5e76\u4e0d\u9700\u8981\u5b8c\u6574\u7684Encoder\u90e8\u5206\u548cDecoder\u90e8\u5206\uff0c\u4e8e\u662f\u5728\u539f\u59cbTransformer\u4e4b\u540e\u7684\u8bb8\u591a\u7814\u7a76\u5de5\u4f5c\u4e2d\uff0c\u4eba\u4eec\u5c1d\u8bd5\u53ea\u4f7f\u7528Transformer Encoder\u6216\u8005Decoder\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u6bd4\u5982BERT\u53ea\u4f7f\u7528\u4e86Encoder\u90e8\u5206\u8fdb\u884cmasked language model\uff08\u81ea\u7f16\u7801\uff09\u8bad\u7ec3\uff0cGPT-2\u4fbf\u662f\u53ea\u4f7f\u7528\u4e86Decoder\u90e8\u5206\u8fdb\u884c\u81ea\u56de\u5f52\uff08auto regressive\uff09\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u3002 GPT-2\u6982\u8ff0 \u00b6 \u6a21\u578b\u7684\u8f93\u5165 \u00b6 \u8f93\u5165\u7684\u5904\u7406\u5206\u4e3a\u4e24\u6b65\uff1atoken embedding + position encoding\u3002\u5373: 1. \u5728\u5d4c\u5165\u77e9\u9635\u4e2d\u67e5\u627e\u8f93\u5165\u7684\u5355\u8bcd\u7684\u5bf9\u5e94\u7684embedding\u5411\u91cf 2. \u878d\u5165\u4f4d\u7f6e\u7f16\u7801 Decoder\u5c42 \u00b6 \u6bcf\u4e00\u5c42decoder\u7684\u7ec4\u6210\uff1aMasked Self-Attention + Feed Forward Neural Network Self-Attention\u6240\u505a\u7684\u4e8b\u60c5\u662f\uff1a\u5b83\u901a\u8fc7\u5bf9\u53e5\u5b50\u7247\u6bb5\u4e2d\u6bcf\u4e2a\u8bcd\u7684\u76f8\u5173\u6027\u6253\u5206\uff0c\u5e76\u5c06\u8fd9\u4e9b\u8bcd\u7684\u8868\u793a\u5411\u91cf\u6839\u636e\u76f8\u5173\u6027\u52a0\u6743\u6c42\u548c\uff0c\u4ece\u800c\u8ba9\u6a21\u578b\u80fd\u591f\u5c06\u8bcd\u548c\u5176\u4ed6\u76f8\u5173\u8bcd\u5411\u91cf\u7684\u4fe1\u606f\u878d\u5408\u8d77\u6765\u3002 Masked Self-Attention\u505a\u7684\u662f\uff1a\u5c06mask\u4f4d\u7f6e\u5bf9\u5e94\u7684\u7684attention score\u53d8\u6210\u4e00\u4e2a\u975e\u5e38\u5c0f\u7684\u6570\u5b57\u6216\u80050\uff0c\u8ba9\u5176\u4ed6\u5355\u8bcd\u518dself attention\u7684\u65f6\u5019\uff08\u52a0\u6743\u6c42\u548c\u7684\u65f6\u5019\uff09\u4e0d\u8003\u8651\u8fd9\u4e9b\u5355\u8bcd\u3002 \u6a21\u578b\u7684\u8f93\u51fa \u00b6 \u5f53\u6a21\u578b\u9876\u90e8\u7684Decoder\u5c42\u4ea7\u751f\u8f93\u51fa\u5411\u91cf\u65f6\uff0c\u6a21\u578b\u4f1a\u5c06\u8fd9\u4e2a\u5411\u91cf\u4e58\u4ee5\u4e00\u4e2a\u5de8\u5927\u7684\u5d4c\u5165\u77e9\u9635\uff08vocab size x embedding size\uff09\u6765\u8ba1\u7b97\u8be5\u5411\u91cf\u548c\u6240\u6709\u5355\u8bcdembedding\u5411\u91cf\u7684\u76f8\u5173\u5f97\u5206\u3002\u8fd9\u4e2a\u76f8\u4e58\u7684\u7ed3\u679c\uff0c\u88ab\u89e3\u91ca\u4e3a\u6a21\u578b\u8bcd\u6c47\u8868\u4e2d\u6bcf\u4e2a\u8bcd\u7684\u5206\u6570\uff0c\u7ecf\u8fc7softmax\u4e4b\u540e\u88ab\u8f6c\u6362\u6210\u6982\u7387\u3002 \u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u6700\u9ad8\u5206\u6570\u7684 token\uff08top_k=1\uff09\uff0c\u4e5f\u53ef\u4ee5\u540c\u65f6\u8003\u8651\u5176\u4ed6\u8bcd\uff08top k\uff09\u3002\u5047\u8bbe\u6bcf\u4e2a\u4f4d\u7f6e\u8f93\u51fak\u4e2atoken\uff0c\u5047\u8bbe\u603b\u5171\u8f93\u51fan\u4e2atoken\uff0c\u90a3\u4e48\u57fa\u4e8en\u4e2a\u5355\u8bcd\u7684\u8054\u5408\u6982\u7387\u9009\u62e9\u7684\u8f93\u51fa\u5e8f\u5217\u4f1a\u66f4\u597d\u3002 \u6a21\u578b\u5b8c\u6210\u4e00\u6b21\u8fed\u4ee3\uff0c\u8f93\u51fa\u4e00\u4e2a\u5355\u8bcd\u3002\u6a21\u578b\u4f1a\u7ee7\u7eed\u8fed\u4ee3\uff0c\u76f4\u5230\u6240\u6709\u7684\u5355\u8bcd\u90fd\u5df2\u7ecf\u751f\u6210\uff0c\u6216\u8005\u76f4\u5230\u8f93\u51fa\u4e86\u8868\u793a\u53e5\u5b50\u672b\u5c3e\u7684token\u3002 \u5173\u4e8eSelf-Attention, Masked Self-Attention \u00b6 Self-Attention \u00b6 Self-Attention \u4e3b\u8981\u901a\u8fc7 3 \u4e2a\u6b65\u9aa4\u6765\u5b9e\u73b0\uff1a \u4e3a\u6bcf\u4e2a\u8def\u5f84\u521b\u5efa Query\u3001Key\u3001Value \u77e9\u9635\u3002 \u5bf9\u4e8e\u6bcf\u4e2a\u8f93\u5165\u7684token\uff0c\u4f7f\u7528\u5b83\u7684Query\u5411\u91cf\u4e3a\u6240\u6709\u5176\u4ed6\u7684Key\u5411\u91cf\u8fdb\u884c\u6253\u5206\u3002 \u5c06 Value \u5411\u91cf\u4e58\u4ee5\u5b83\u4eec\u5bf9\u5e94\u7684\u5206\u6570\u540e\u6c42\u548c\u3002 Masked Self-Attention \u00b6 \u5728Self-Attention\u7684\u7b2c2\u6b65\uff0c\u628a\u672a\u6765\u7684 token \u8bc4\u5206\u8bbe\u7f6e\u4e3a0\uff0c\u56e0\u6b64\u6a21\u578b\u4e0d\u80fd\u770b\u5230\u672a\u6765\u7684\u8bcd\u3002 \u8fd9\u4e2a\u5c4f\u853d\uff08masking\uff09\u7ecf\u5e38\u7528\u4e00\u4e2a\u77e9\u9635\u6765\u5b9e\u73b0\uff0c\u79f0\u4e3a attention mask\u77e9\u9635\u3002 GPT-2\u4e2d\u7684Self-Attention \u00b6 (skip) \u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u7684\u5e94\u7528 \u00b6 \u5e94\u7528\u5728\u4e0b\u6e38\u5e76\u53d6\u5f97\u4e0d\u9519\u6548\u679c\u7684NLP\u4efb\u52a1\u6709\uff1a\u673a\u5668\u7ffb\u8bd1\u3001\u6458\u8981\u751f\u6210\u3001\u97f3\u4e50\u751f\u6210\u3002 \uff08\u53ef\u89c1\uff0c\u4e3b\u8981\u662f\u8ddf\u9884\u8bad\u7ec3\u4efb\u52a1\u76f8\u4f3c\u7684\u751f\u6210\u7c7b\u4efb\u52a1\u3002\uff09 \u53c2\u8003\u8d44\u6599 \u00b6 \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8--\u5728\u7ebf\u9605\u8bfb https://datawhalechina.github.io/learn-nlp-with-transformers/#/","title":"Task04 \u5b66\u4e60GPT"},{"location":"nlp-transformer-task04/#task04-gpt","text":"","title":"Task04 \u5b66\u4e60GPT"},{"location":"nlp-transformer-task04/#_1","text":"","title":"\u4ece\u8bed\u8a00\u6a21\u578b\u8bf4\u8d77"},{"location":"nlp-transformer-task04/#auto-encoder","text":"\u81ea\u7f16\u7801\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u968f\u673aMask\u8f93\u5165\u7684\u90e8\u5206\u5355\u8bcd\uff0c\u7136\u540e\u9884\u8bad\u7ec3\u7684\u76ee\u6807\u662f\u9884\u6d4b\u88abMask\u7684\u5355\u8bcd\uff0c\u4e0d\u4ec5\u53ef\u4ee5\u878d\u5165\u4e0a\u6587\u4fe1\u606f\uff0c\u8fd8\u53ef\u4ee5\u81ea\u7136\u7684\u878d\u5165\u4e0b\u6587\u4fe1\u606f\u3002ex. BERT. - \u4f18\u70b9\uff1a\u81ea\u7136\u5730\u878d\u5165\u53cc\u5411\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u770b\u5230\u88ab\u9884\u6d4b\u5355\u8bcd\u7684\u4e0a\u6587\u548c\u4e0b\u6587 - \u7f3a\u70b9\uff1a\u8bad\u7ec3\u548c\u9884\u6d4b\u4e0d\u4e00\u81f4\u3002\u8bad\u7ec3\u7684\u65f6\u5019\u8f93\u5165\u5f15\u5165\u4e86[Mask]\u6807\u8bb0\uff0c\u4f46\u662f\u5728\u9884\u6d4b\u9636\u6bb5\u5f80\u5f80\u6ca1\u6709\u8fd9\u4e2a[Mask]\u6807\u8bb0\uff0c\u5bfc\u81f4\u9884\u8bad\u7ec3\u9636\u6bb5\u548cFine-tuning\u9636\u6bb5\u4e0d\u4e00\u81f4\u3002","title":"\u81ea\u7f16\u7801\u8bed\u8a00\u6a21\u578b\uff08auto-encoder\uff09"},{"location":"nlp-transformer-task04/#auto-regressive","text":"\u8bed\u8a00\u6a21\u578b\u6839\u636e\u8f93\u5165\u53e5\u5b50\u7684\u4e00\u90e8\u5206\u6587\u672c\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u3002ex. GPT-2 - \u4f18\u70b9\uff1a\u5bf9\u4e8e\u751f\u6210\u7c7b\u7684NLP\u4efb\u52a1\uff0c\u6bd4\u5982\u6587\u672c\u6458\u8981\uff0c\u673a\u5668\u7ffb\u8bd1\u7b49\uff0c\u4ece\u5de6\u5411\u53f3\u7684\u751f\u6210\u5185\u5bb9\uff0c\u5929\u7136\u548c\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5951\u5408\u3002 - \u7f3a\u70b9\uff1a\u7531\u4e8e\u4e00\u822c\u662f\u4ece\u5de6\u5230\u53f3\uff08\u5f53\u7136\u4e5f\u53ef\u80fd\u4ece\u53f3\u5230\u5de6\uff09\uff0c\u6240\u4ee5\u53ea\u80fd\u5229\u7528\u4e0a\u6587\u6216\u8005\u4e0b\u6587\u7684\u4fe1\u606f\uff0c\u4e0d\u80fd\u540c\u65f6\u5229\u7528\u4e0a\u6587\u548c\u4e0b\u6587\u7684\u4fe1\u606f\u3002","title":"\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\uff08auto-regressive\uff09"},{"location":"nlp-transformer-task04/#transformer-bert-gpt-2","text":"Transformer\u7684Encoder\u8fdb\u5316\u6210\u4e86BERT\uff0cDecoder\u8fdb\u5316\u6210\u4e86GPT2\u3002 \u5982\u679c\u8981\u4f7f\u7528Transformer\u6765\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\uff0c\u5e76\u4e0d\u9700\u8981\u5b8c\u6574\u7684Encoder\u90e8\u5206\u548cDecoder\u90e8\u5206\uff0c\u4e8e\u662f\u5728\u539f\u59cbTransformer\u4e4b\u540e\u7684\u8bb8\u591a\u7814\u7a76\u5de5\u4f5c\u4e2d\uff0c\u4eba\u4eec\u5c1d\u8bd5\u53ea\u4f7f\u7528Transformer Encoder\u6216\u8005Decoder\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u6bd4\u5982BERT\u53ea\u4f7f\u7528\u4e86Encoder\u90e8\u5206\u8fdb\u884cmasked language model\uff08\u81ea\u7f16\u7801\uff09\u8bad\u7ec3\uff0cGPT-2\u4fbf\u662f\u53ea\u4f7f\u7528\u4e86Decoder\u90e8\u5206\u8fdb\u884c\u81ea\u56de\u5f52\uff08auto regressive\uff09\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u3002","title":"Transformer, BERT, GPT-2\u7684\u5173\u7cfb"},{"location":"nlp-transformer-task04/#gpt-2","text":"","title":"GPT-2\u6982\u8ff0"},{"location":"nlp-transformer-task04/#_2","text":"\u8f93\u5165\u7684\u5904\u7406\u5206\u4e3a\u4e24\u6b65\uff1atoken embedding + position encoding\u3002\u5373: 1. \u5728\u5d4c\u5165\u77e9\u9635\u4e2d\u67e5\u627e\u8f93\u5165\u7684\u5355\u8bcd\u7684\u5bf9\u5e94\u7684embedding\u5411\u91cf 2. \u878d\u5165\u4f4d\u7f6e\u7f16\u7801","title":"\u6a21\u578b\u7684\u8f93\u5165"},{"location":"nlp-transformer-task04/#decoder","text":"\u6bcf\u4e00\u5c42decoder\u7684\u7ec4\u6210\uff1aMasked Self-Attention + Feed Forward Neural Network Self-Attention\u6240\u505a\u7684\u4e8b\u60c5\u662f\uff1a\u5b83\u901a\u8fc7\u5bf9\u53e5\u5b50\u7247\u6bb5\u4e2d\u6bcf\u4e2a\u8bcd\u7684\u76f8\u5173\u6027\u6253\u5206\uff0c\u5e76\u5c06\u8fd9\u4e9b\u8bcd\u7684\u8868\u793a\u5411\u91cf\u6839\u636e\u76f8\u5173\u6027\u52a0\u6743\u6c42\u548c\uff0c\u4ece\u800c\u8ba9\u6a21\u578b\u80fd\u591f\u5c06\u8bcd\u548c\u5176\u4ed6\u76f8\u5173\u8bcd\u5411\u91cf\u7684\u4fe1\u606f\u878d\u5408\u8d77\u6765\u3002 Masked Self-Attention\u505a\u7684\u662f\uff1a\u5c06mask\u4f4d\u7f6e\u5bf9\u5e94\u7684\u7684attention score\u53d8\u6210\u4e00\u4e2a\u975e\u5e38\u5c0f\u7684\u6570\u5b57\u6216\u80050\uff0c\u8ba9\u5176\u4ed6\u5355\u8bcd\u518dself attention\u7684\u65f6\u5019\uff08\u52a0\u6743\u6c42\u548c\u7684\u65f6\u5019\uff09\u4e0d\u8003\u8651\u8fd9\u4e9b\u5355\u8bcd\u3002","title":"Decoder\u5c42"},{"location":"nlp-transformer-task04/#_3","text":"\u5f53\u6a21\u578b\u9876\u90e8\u7684Decoder\u5c42\u4ea7\u751f\u8f93\u51fa\u5411\u91cf\u65f6\uff0c\u6a21\u578b\u4f1a\u5c06\u8fd9\u4e2a\u5411\u91cf\u4e58\u4ee5\u4e00\u4e2a\u5de8\u5927\u7684\u5d4c\u5165\u77e9\u9635\uff08vocab size x embedding size\uff09\u6765\u8ba1\u7b97\u8be5\u5411\u91cf\u548c\u6240\u6709\u5355\u8bcdembedding\u5411\u91cf\u7684\u76f8\u5173\u5f97\u5206\u3002\u8fd9\u4e2a\u76f8\u4e58\u7684\u7ed3\u679c\uff0c\u88ab\u89e3\u91ca\u4e3a\u6a21\u578b\u8bcd\u6c47\u8868\u4e2d\u6bcf\u4e2a\u8bcd\u7684\u5206\u6570\uff0c\u7ecf\u8fc7softmax\u4e4b\u540e\u88ab\u8f6c\u6362\u6210\u6982\u7387\u3002 \u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u6700\u9ad8\u5206\u6570\u7684 token\uff08top_k=1\uff09\uff0c\u4e5f\u53ef\u4ee5\u540c\u65f6\u8003\u8651\u5176\u4ed6\u8bcd\uff08top k\uff09\u3002\u5047\u8bbe\u6bcf\u4e2a\u4f4d\u7f6e\u8f93\u51fak\u4e2atoken\uff0c\u5047\u8bbe\u603b\u5171\u8f93\u51fan\u4e2atoken\uff0c\u90a3\u4e48\u57fa\u4e8en\u4e2a\u5355\u8bcd\u7684\u8054\u5408\u6982\u7387\u9009\u62e9\u7684\u8f93\u51fa\u5e8f\u5217\u4f1a\u66f4\u597d\u3002 \u6a21\u578b\u5b8c\u6210\u4e00\u6b21\u8fed\u4ee3\uff0c\u8f93\u51fa\u4e00\u4e2a\u5355\u8bcd\u3002\u6a21\u578b\u4f1a\u7ee7\u7eed\u8fed\u4ee3\uff0c\u76f4\u5230\u6240\u6709\u7684\u5355\u8bcd\u90fd\u5df2\u7ecf\u751f\u6210\uff0c\u6216\u8005\u76f4\u5230\u8f93\u51fa\u4e86\u8868\u793a\u53e5\u5b50\u672b\u5c3e\u7684token\u3002","title":"\u6a21\u578b\u7684\u8f93\u51fa"},{"location":"nlp-transformer-task04/#self-attention-masked-self-attention","text":"","title":"\u5173\u4e8eSelf-Attention, Masked Self-Attention"},{"location":"nlp-transformer-task04/#self-attention","text":"Self-Attention \u4e3b\u8981\u901a\u8fc7 3 \u4e2a\u6b65\u9aa4\u6765\u5b9e\u73b0\uff1a \u4e3a\u6bcf\u4e2a\u8def\u5f84\u521b\u5efa Query\u3001Key\u3001Value \u77e9\u9635\u3002 \u5bf9\u4e8e\u6bcf\u4e2a\u8f93\u5165\u7684token\uff0c\u4f7f\u7528\u5b83\u7684Query\u5411\u91cf\u4e3a\u6240\u6709\u5176\u4ed6\u7684Key\u5411\u91cf\u8fdb\u884c\u6253\u5206\u3002 \u5c06 Value \u5411\u91cf\u4e58\u4ee5\u5b83\u4eec\u5bf9\u5e94\u7684\u5206\u6570\u540e\u6c42\u548c\u3002","title":"Self-Attention"},{"location":"nlp-transformer-task04/#masked-self-attention","text":"\u5728Self-Attention\u7684\u7b2c2\u6b65\uff0c\u628a\u672a\u6765\u7684 token \u8bc4\u5206\u8bbe\u7f6e\u4e3a0\uff0c\u56e0\u6b64\u6a21\u578b\u4e0d\u80fd\u770b\u5230\u672a\u6765\u7684\u8bcd\u3002 \u8fd9\u4e2a\u5c4f\u853d\uff08masking\uff09\u7ecf\u5e38\u7528\u4e00\u4e2a\u77e9\u9635\u6765\u5b9e\u73b0\uff0c\u79f0\u4e3a attention mask\u77e9\u9635\u3002","title":"Masked Self-Attention"},{"location":"nlp-transformer-task04/#gpt-2self-attention","text":"(skip)","title":"GPT-2\u4e2d\u7684Self-Attention"},{"location":"nlp-transformer-task04/#_4","text":"\u5e94\u7528\u5728\u4e0b\u6e38\u5e76\u53d6\u5f97\u4e0d\u9519\u6548\u679c\u7684NLP\u4efb\u52a1\u6709\uff1a\u673a\u5668\u7ffb\u8bd1\u3001\u6458\u8981\u751f\u6210\u3001\u97f3\u4e50\u751f\u6210\u3002 \uff08\u53ef\u89c1\uff0c\u4e3b\u8981\u662f\u8ddf\u9884\u8bad\u7ec3\u4efb\u52a1\u76f8\u4f3c\u7684\u751f\u6210\u7c7b\u4efb\u52a1\u3002\uff09","title":"\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u7684\u5e94\u7528"},{"location":"nlp-transformer-task04/#_5","text":"\u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8--\u5728\u7ebf\u9605\u8bfb https://datawhalechina.github.io/learn-nlp-with-transformers/#/","title":"\u53c2\u8003\u8d44\u6599"},{"location":"nlp-transformer-task05/","text":"Task05 \u7f16\u5199BERT\u6a21\u578b \u00b6 Overview \u00b6 \u672c\u90e8\u5206\u662fBERT\u6e90\u7801\u7684\u89e3\u8bfb\uff0c\u6765\u81eaHuggingFace/transfomers/BERT[1]\u3002 \u5982\u56fe\u6240\u793a\uff0c\u4ee3\u7801\u7ed3\u6784\u548c\u4f5c\u7528\u5982\u4e0b\uff1a BertTokenizer \u9884\u5904\u7406\u548c\u5207\u8bcd BertModel BertEmbeddings \u8bcd\u5d4c\u5165 BertEncoder BertAttention \u6ce8\u610f\u529b\u673a\u5236 BertIntermediate \u5168\u8fde\u63a5\u548c\u6fc0\u6d3b\u51fd\u6570 BertOutput \u5168\u8fde\u63a5\u3001\u6b8b\u5dee\u94fe\u63a5\u548c\u6b63\u5219\u5316 BertPooler \u53d6\u51fa[CLS]\u5bf9\u5e94\u7684\u5411\u91cf\uff0c\u7136\u540e\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u548c\u6fc0\u6d3b\u51fd\u6570\u540e\u8f93\u51fa\u7ed3\u679c BERT\u7684\u5b9e\u73b0 \u00b6 BertConfig \u00b6 classtransformers . BertConfig ( vocab_size = 30522 , hidden_size = 768 , num_hidden_layers = 12 , num_attention_heads = 12 , intermediate_size = 3072 , hidden_act = 'gelu' , hidden_dropout_prob = 0.1 , attention_probs_dropout_prob = 0.1 , max_position_embeddings = 512 , type_vocab_size = 2 , initializer_range = 0.02 , layer_norm_eps = 1e-12 , pad_token_id = 0 , gradient_checkpointing = False , position_embedding_type = 'absolute' , use_cache = True , classifier_dropout = None , ** kwargs ) \u8fd9\u662f\u5b58\u50a8BertModel\uff08Torch.nn.Module\u7684\u5b50\u7c7b\uff09\u6216TFBertModel\uff08tf.keras.Model\u7684\u5b50\u7c7b\uff09\u914d\u7f6e\u7684\u914d\u7f6e\u7c7b\u3002\u5b83\u7528\u4e8e\u6839\u636e\u6307\u5b9a\u7684\u53c2\u6570\u6765\u5b9e\u4f8b\u5316BERT\u6a21\u578b\uff0c\u5b9a\u4e49\u6a21\u578b\u67b6\u6784\u3002 \u914d\u7f6e\u5bf9\u8c61\u4ecePretrainedConfig\u7ee7\u627f\uff0c\u53ef\u7528\u4e8e\u63a7\u5236\u6a21\u578b\u8f93\u51fa\u3002 \u53c2\u8003\u8d44\u6599 \u00b6 HuggingFace/transfomers/BERT https://huggingface.co/transformers/model_doc/bert.html# \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8--\u5728\u7ebf\u9605\u8bfb https://datawhalechina.github.io/learn-nlp-with-transformers/#/","title":"Task05 \u7f16\u5199BERT\u6a21\u578b"},{"location":"nlp-transformer-task05/#task05-bert","text":"","title":"Task05 \u7f16\u5199BERT\u6a21\u578b"},{"location":"nlp-transformer-task05/#overview","text":"\u672c\u90e8\u5206\u662fBERT\u6e90\u7801\u7684\u89e3\u8bfb\uff0c\u6765\u81eaHuggingFace/transfomers/BERT[1]\u3002 \u5982\u56fe\u6240\u793a\uff0c\u4ee3\u7801\u7ed3\u6784\u548c\u4f5c\u7528\u5982\u4e0b\uff1a BertTokenizer \u9884\u5904\u7406\u548c\u5207\u8bcd BertModel BertEmbeddings \u8bcd\u5d4c\u5165 BertEncoder BertAttention \u6ce8\u610f\u529b\u673a\u5236 BertIntermediate \u5168\u8fde\u63a5\u548c\u6fc0\u6d3b\u51fd\u6570 BertOutput \u5168\u8fde\u63a5\u3001\u6b8b\u5dee\u94fe\u63a5\u548c\u6b63\u5219\u5316 BertPooler \u53d6\u51fa[CLS]\u5bf9\u5e94\u7684\u5411\u91cf\uff0c\u7136\u540e\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u548c\u6fc0\u6d3b\u51fd\u6570\u540e\u8f93\u51fa\u7ed3\u679c","title":"Overview"},{"location":"nlp-transformer-task05/#bert","text":"","title":"BERT\u7684\u5b9e\u73b0"},{"location":"nlp-transformer-task05/#bertconfig","text":"classtransformers . BertConfig ( vocab_size = 30522 , hidden_size = 768 , num_hidden_layers = 12 , num_attention_heads = 12 , intermediate_size = 3072 , hidden_act = 'gelu' , hidden_dropout_prob = 0.1 , attention_probs_dropout_prob = 0.1 , max_position_embeddings = 512 , type_vocab_size = 2 , initializer_range = 0.02 , layer_norm_eps = 1e-12 , pad_token_id = 0 , gradient_checkpointing = False , position_embedding_type = 'absolute' , use_cache = True , classifier_dropout = None , ** kwargs ) \u8fd9\u662f\u5b58\u50a8BertModel\uff08Torch.nn.Module\u7684\u5b50\u7c7b\uff09\u6216TFBertModel\uff08tf.keras.Model\u7684\u5b50\u7c7b\uff09\u914d\u7f6e\u7684\u914d\u7f6e\u7c7b\u3002\u5b83\u7528\u4e8e\u6839\u636e\u6307\u5b9a\u7684\u53c2\u6570\u6765\u5b9e\u4f8b\u5316BERT\u6a21\u578b\uff0c\u5b9a\u4e49\u6a21\u578b\u67b6\u6784\u3002 \u914d\u7f6e\u5bf9\u8c61\u4ecePretrainedConfig\u7ee7\u627f\uff0c\u53ef\u7528\u4e8e\u63a7\u5236\u6a21\u578b\u8f93\u51fa\u3002","title":"BertConfig"},{"location":"nlp-transformer-task05/#_1","text":"HuggingFace/transfomers/BERT https://huggingface.co/transformers/model_doc/bert.html# \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8--\u5728\u7ebf\u9605\u8bfb https://datawhalechina.github.io/learn-nlp-with-transformers/#/","title":"\u53c2\u8003\u8d44\u6599"},{"location":"nlp-transformer-task06/","text":"Task06 BERT\u5e94\u7528\u3001\u8bad\u7ec3\u548c\u4f18\u5316 \u00b6 \u8be5\u90e8\u5206\u7684\u5185\u5bb9\u7ffb\u8bd1\u81ea\ud83e\udd17HuggingFace\u5b98\u7f51\u6559\u7a0b\u7b2c1\u90e8\u5206\uff081-4\u7ae0\uff09\uff0c\u89c1 https://huggingface.co/course/chapter1 \u3002\u8be5\u7cfb\u5217\u6559\u7a0b\u75313\u5927\u90e8\u5206\u517112\u7ae0\u7ec4\u6210\uff08\u5982\u56fe\uff09\uff0c\u5176\u4e2d\u7b2c1\u90e8\u5206\u4ecb\u7ecdtransformers\u5e93\u7684\u4e3b\u8981\u6982\u5ff5\u3001\u6a21\u578b\u7684\u5de5\u4f5c\u539f\u7406\u548c\u4f7f\u7528\u65b9\u6cd5\u3001\u600e\u6837\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7b49\u5185\u5bb9\u3002 \u73af\u5883\u642d\u5efa \u00b6 \u7b80\u5355\u7684\u8bf4\uff0c\u6709\u4e24\u79cd\u53ef\u4ee5\u8dd1\u6a21\u578b\u4ee3\u7801\u7684\u65b9\u5f0f\uff1a 1. Google Colab 2. \u672c\u5730\u865a\u62df\u73af\u5883 pip install transformers \u8be6\u89c1 https://huggingface.co/course/chapter0?fw=pt Transformer\u6a21\u578b\u6982\u8ff0 \u00b6 Transformers, \u53ef\u4ee5\u505a\u4ec0\u4e48\uff1f \u00b6 \u76ee\u524d\u53ef\u7528\u7684\u4e00\u4e9bpipeline\u662f\uff1a - feature-extraction \u83b7\u53d6\u6587\u672c\u7684\u5411\u91cf\u8868\u793a - fill-mask \u5b8c\u5f62\u586b\u7a7a - ner (named entity recognition) \u547d\u540d\u5b9e\u4f53\u8bc6\u522b - question-answering \u95ee\u7b54 - sentiment-analysis \u60c5\u611f\u5206\u6790 - summarization \u6458\u8981\u751f\u6210 - text-generation \u6587\u672c\u751f\u6210 - translation \u7ffb\u8bd1 - zero-shot-classification \u96f6\u6837\u672c\u5206\u7c7b pipeline: \u76f4\u8bd1\u7ba1\u9053/\u6d41\u6c34\u7ebf\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u6d41\u7a0b\u3002 Transformers, \u5982\u4f55\u5de5\u4f5c\uff1f \u00b6 Transformer\u7b80\u53f2 \u00b6 Transformer \u67b6\u6784\u4e8e 2017 \u5e74 6 \u6708\u63a8\u51fa\u3002\u539f\u59cb\u7814\u7a76\u7684\u91cd\u70b9\u662f\u7ffb\u8bd1\u4efb\u52a1\u3002\u968f\u540e\u63a8\u51fa\u4e86\u51e0\u4e2a\u6709\u5f71\u54cd\u529b\u7684\u6a21\u578b\uff0c\u5305\u62ec\uff1a - 2018 \u5e74 6 \u6708\uff1aGPT\uff0c\u7b2c\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684 Transformer \u6a21\u578b\uff0c\u7528\u4e8e\u5404\u79cd NLP \u4efb\u52a1\u7684\u5fae\u8c03\u5e76\u83b7\u5f97\u6700\u5148\u8fdb\u7684\u7ed3\u679c - 2018 \u5e74 10 \u6708\uff1aBERT\uff0c\u53e6\u4e00\u4e2a\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u65e8\u5728\u751f\u6210\u66f4\u597d\u7684\u53e5\u5b50\u6458\u8981 - 2019 \u5e74 2 \u6708\uff1aGPT-2\uff0cGPT \u7684\u6539\u8fdb\uff08\u548c\u66f4\u5927\uff09\u7248\u672c - 2019 \u5e74 10 \u6708\uff1aDistilBERT\uff0cBERT \u7684\u84b8\u998f\u7248\u672c\uff0c\u901f\u5ea6\u63d0\u9ad8 60%\uff0c\u5185\u5b58\u51cf\u8f7b 40%\uff0c\u4f46\u4ecd\u4fdd\u7559 BERT 97% \u7684\u6027\u80fd - 2019 \u5e74 10 \u6708\uff1aBART \u548c T5\uff0c\u4e24\u4e2a\u4f7f\u7528\u4e0e\u539f\u59cb Transformer \u6a21\u578b\u76f8\u540c\u67b6\u6784\u7684\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u7b2c\u4e00\u4e2a\u8fd9\u6837\u505a\uff09 - 2020 \u5e74 5 \u6708\uff0cGPT-3\uff0cGPT-2 \u7684\u66f4\u5927\u7248\u672c\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff08\u79f0\u4e3a\u96f6\u6837\u672c\u5b66\u4e60zero-shot learning\uff09 \u5927\u4f53\u4e0a\uff0c\u5b83\u4eec\u53ef\u4ee5\u5206\u4e3a\u4e09\u7c7b\uff1a - GPT\u7c7b\uff08\u53c8\u79f0\u4e3a\u81ea\u56de\u5f52 Transformer \u6a21\u578b\uff09\uff1a\u53ea\u4f7f\u7528transformer-decoder\u90e8\u5206 - BERT\u7c7b\uff08\u53c8\u79f0\u4e3a\u81ea\u7f16\u7801 Transformer \u6a21\u578b\uff09\uff1a\u53ea\u4f7f\u7528transformer-encoder\u90e8\u5206 - BART/T5\u7c7b\uff08\u53c8\u79f0\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217 Transformer \u6a21\u578b\uff09\uff1a\u4f7f\u7528Transformer-encoder-decoder\u90e8\u5206 \u5b83\u4eec\u7684\u5206\u7c7b\u3001\u5177\u4f53\u6a21\u578b\u3001\u4e3b\u8981\u5e94\u7528\u4efb\u52a1\u5982\u4e0b\uff1a \u5176\u4ed6\u9700\u8981\u77e5\u9053\u7684\uff1a - Transformers\u662f\u8bed\u8a00\u6a21\u578b - Transformers\u662f\u5927\u6a21\u578b - Transformers\u7684\u5e94\u7528\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e24\u4e2a\u8fc7\u7a0b \u540d\u8bcd\u89e3\u91ca\uff1aArchitecture\u548cCheckpoints \u00b6 Architecture/\u67b6\u6784 \uff1a\u5b9a\u4e49\u4e86\u6a21\u578b\u7684\u57fa\u672c\u7ed3\u6784\u548c\u57fa\u672c\u8fd0\u7b97\u3002 Checkpoints/\u68c0\u67e5\u70b9 \uff1a\u6a21\u578b\u7684\u67d0\u4e2a\u8bad\u7ec3\u72b6\u6001\uff0c\u52a0\u8f7d\u6b64checkpoint\u4f1a\u52a0\u8f7d\u6b64\u65f6\u7684\u6743\u91cd\u3002\u8bad\u7ec3\u65f6\u53ef\u4ee5\u9009\u62e9\u81ea\u52a8\u4fdd\u5b58checkpoint\u3002\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u53ef\u4ee5\u8bbe\u7f6e\u81ea\u52a8\u4fdd\u5b58\u4e8e\u67d0\u4e2a\u65f6\u95f4\u70b9\uff08\u6bd4\u5982\u6a21\u578b\u8bad\u7ec3\u4e86\u4e00\u8f6eepoch\uff0c\u66f4\u65b0\u4e86\u53c2\u6570\uff0c\u5c06\u8fd9\u4e2a\u72b6\u6001\u7684\u6a21\u578b\u4fdd\u5b58\u4e0b\u6765\uff0c\u4e3a\u4e00\u4e2acheckpoint\u3002\uff09 \u6240\u4ee5\u6bcf\u4e2acheckpoint\u5bf9\u5e94\u6a21\u578b\u7684\u4e00\u4e2a\u72b6\u6001\uff0c\u4e00\u7ec4\u6743\u91cd\u3002 \u4f7f\u7528Transformers \u00b6 3\u4e2a\u5904\u7406\u6b65\u9aa4 \u00b6 \u5c06\u4e00\u4e9b\u6587\u672c\u4f20\u9012\u5230pipeline\u65f6\u6d89\u53ca3\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a 1. \u6587\u672c\u88ab\u9884\u5904\u7406\u4e3a\u6a21\u578b\u53ef\u4ee5\u7406\u89e3\u7684\u683c\u5f0f\u3002 2. \u9884\u5904\u7406\u540e\u7684\u8f93\u5165\u4f20\u9012\u7ed9\u6a21\u578b\u3002 3. \u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u88ab\u540e\u5904\u7406\u4e3a\u4eba\u7c7b\u53ef\u4ee5\u7406\u89e3\u7684\u683c\u5f0f\u3002 Pipeline\u5c063\u4e2a\u6b65\u9aa4\u7ec4\u5408\u5728\u4e00\u8d77\uff1a\u9884\u5904\u7406/Tokenizer\u3001\u901a\u8fc7\u6a21\u578b\u4f20\u9012\u8f93\u5165/Model\u548c\u540e\u5904\u7406/Post-Processing\uff1a Tokenizer/\u9884\u5904\u7406 \u00b6 Tokenizer\u7684\u4f5c\u7528\uff1a - \u5c06\u8f93\u5165\u62c6\u5206\u4e3a\u79f0\u4e3atoken\u7684\u5355\u8bcd\u3001\u5b50\u8bcd/subword\u6216\u7b26\u53f7/symbols\uff08\u5982\u6807\u70b9\u7b26\u53f7\uff09 - \u5c06\u6bcf\u4e2atoken\u6620\u5c04\u5230\u4e00\u4e2a\u6574\u6570 - \u6dfb\u52a0\u53ef\u80fd\u5bf9\u6a21\u578b\u6709\u7528\u7684\u5176\u4ed6\u8f93\u5165 Going Through Models/\u7a7f\u8fc7\u6a21\u578b \u00b6 \u6a21\u578b\u5b9e\u4f8b\u5316 \u00b6 from transformers import AutoModel checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" model = AutoModel . from_pretrained ( checkpoint ) \u5728\u8fd9\u6bb5\u4ee3\u7801\u4e2d\uff0c\u6211\u4eec\u4e0b\u8f7d\u4e86\u5728pipeline\u4e2d\u4f7f\u7528\u7684\u76f8\u540c\u68c0\u67e5\u70b9\uff08\u5b9e\u9645\u4e0a\u5df2\u7ecf\u7f13\u5b58\uff09\u5e76\u5c06\u6a21\u578b\u5b9e\u4f8b\u5316\u3002 \u6a21\u578b\u7684\u8f93\u51fa\uff1a\u9ad8\u7ef4\u5411\u91cf \u00b6 \u6a21\u578b\u7684\u8f93\u51fa\u5411\u91cf\u901a\u5e38\u6709\u4e09\u4e2a\u7ef4\u5ea6\uff1a - Batch size: \u4e00\u6b21\u5904\u7406\u7684\u5e8f\u5217\u6570 - Sequence length: \u5e8f\u5217\u5411\u91cf\u7684\u957f\u5ea6 - Hidden size: \u6bcf\u4e2a\u6a21\u578b\u8f93\u5165\u5904\u7406\u540e\u7684\u5411\u91cf\u7ef4\u5ea6\uff08hidden state vector\uff09 Model Heads\uff1a\u4e3a\u4e86\u5904\u7406\u4e0d\u540c\u7684\u4efb\u52a1 \u00b6 Model heads:\u5c06\u9690\u85cf\u72b6\u6001\u7684\u9ad8\u7ef4\u5411\u91cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u5c06\u5b83\u4eec\u6295\u5f71\u5230\u4e0d\u540c\u7684\u7ef4\u5ea6\u4e0a\u3002\u5b83\u4eec\u901a\u5e38\u7531\u4e00\u4e2a\u6216\u51e0\u4e2a\u7ebf\u6027\u5c42\u7ec4\u6210\u3002 \u5982\u4e0a\u56fe\u6240\u793a\uff0c\u7d2b\u8272\u4ee3\u8868\u5411\u91cf\uff0c\u7c89\u8272\u4ee3\u8868\u6a21\u7ec4\uff0cEmbeddings+layers\u8868\u793aTransformer\u7684\u67b6\u6784\uff0c\u7ecf\u8fc7\u8fd9\u5c42\u67b6\u6784\u540e\u7684\u8f93\u51fa\u9001\u5165Model Head\u8fdb\u884c\u5904\u7406\uff0c\u4ece\u800c\u5e94\u7528\u5230\u4e0d\u540c\u7684\u4e0b\u6e38\u4efb\u52a1\u3002 \ud83e\udd17 Transformers \u4e2d\u6709\u8bb8\u591a\u4e0d\u540c\u7684Head\u67b6\u6784\u53ef\u7528\uff0c\u6bcf\u4e00\u79cd\u67b6\u6784\u90fd\u56f4\u7ed5\u7740\u5904\u7406\u7279\u5b9a\u4efb\u52a1\u800c\u8bbe\u8ba1\u3002 \u4e0b\u9762\u5217\u4e3e\u4e86\u90e8\u5206Model heads\uff1a *Model (retrieve the hidden states) *ForCausalLM *ForMaskedLM *ForMultipleChoice *ForQuestionAnswering *ForSequenceClassification *ForTokenClassification and others \ud83e\udd17 Post-processing/\u540e\u5904\u7406 \u00b6 \u4ece\u6a21\u578b\u4e2d\u83b7\u5f97\u7684\u4f5c\u4e3a\u8f93\u51fa\u7684\u503c\u672c\u8eab\u5e76\u4e0d\u4e00\u5b9a\u6709\u610f\u4e49\u3002\u8981\u8f6c\u6362\u4e3a\u6982\u7387\uff0c\u5b83\u4eec\u9700\u8981\u7ecf\u8fc7\u4e00\u4e2a SoftMax \u5c42\u3002 \u5fae\u8c03\u4e00\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b \u00b6 \u6570\u636e\u5904\u7406 \u00b6 \u5728\u672c\u8282\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528MRPC\uff08Microsoft Research Praphrase Corpus\uff09\u6570\u636e\u96c6\u4f5c\u4e3a\u793a\u4f8b\u3002\u8be5DataSet\u75315,801\u5bf9\u53e5\u5b50\u7ec4\u6210\uff0c\u6807\u7b7e\u6307\u793a\u5b83\u4eec\u662f\u5426\u662f\u540c\u4e49\u53e5\uff08\u5373\u4e24\u4e2a\u53e5\u5b50\u662f\u5426\u8868\u793a\u76f8\u540c\u7684\u610f\u601d\uff09\u3002 \u6211\u4eec\u9009\u62e9\u5b83\u662f\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u5c0f\u578b\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u53ef\u4ee5\u8f7b\u677e\u8bad\u7ec3\u3002 \u4eceHub\u4e0a\u52a0\u8f7d\u6570\u636e\u96c6 \u00b6 Hub\u4e0d\u4ec5\u5305\u542b\u6a21\u578b\uff0c\u8fd8\u542b\u6709\u591a\u79cd\u8bed\u8a00\u7684datasets\u3002 \u4f8b\u5982\uff0cMRPC\u6570\u636e\u96c6\u662f\u6784\u6210 GLUE benchmark\u7684 10 \u4e2a\u6570\u636e\u96c6\u4e4b\u4e00\u3002GLUE\uff08General Language Understanding Evaluation\uff09\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u548c\u5206\u6790\u5e73\u53f0\u3002GLUE\u5305\u542b\u4e5d\u9879NLU\u4efb\u52a1\uff0c\u8bed\u8a00\u5747\u4e3a\u82f1\u8bed\u3002GLUE\u4e5d\u9879\u4efb\u52a1\u6d89\u53ca\u5230\u81ea\u7136\u8bed\u8a00\u63a8\u65ad\u3001\u6587\u672c\u8574\u542b\u3001\u60c5\u611f\u5206\u6790\u3001\u8bed\u4e49\u76f8\u4f3c\u7b49\u591a\u4e2a\u4efb\u52a1\u3002\u50cfBERT\u3001XLNet\u3001RoBERTa\u3001ERINE\u3001T5\u7b49\u77e5\u540d\u6a21\u578b\u90fd\u4f1a\u5728\u6b64\u57fa\u51c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002 \ud83e\udd17 Datasets\u5e93\u63d0\u4f9b\u4e86\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684\u547d\u4ee4\u6765\u4e0b\u8f7d\u548c\u7f13\u5b58Hub\u4e0a\u7684dataset\u3002 \u6211\u4eec\u53ef\u4ee5\u50cf\u8fd9\u6837\u4e0b\u8f7d MRPC \u6570\u636e\u96c6\uff1a >>> from datasets import load_dataset >>> raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) >>> raw_datasets \u8f93\u51fa\u5982\u4e0b\uff1a DatasetDict ({ train : Dataset ({ features : [ 'sentence1' , 'sentence2' , 'label' , 'idx' ], num_rows : 3668 }) validation : Dataset ({ features : [ 'sentence1' , 'sentence2' , 'label' , 'idx' ], num_rows : 408 }) test : Dataset ({ features : [ 'sentence1' , 'sentence2' , 'label' , 'idx' ], num_rows : 1725 }) }) \u8fd9\u6837\u5c31\u5f97\u5230\u4e00\u4e2aDatasetDict\u5bf9\u8c61\uff0c\u5305\u542b\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u8bad\u7ec3\u96c6\u4e2d\u67093,668 \u4e2a\u53e5\u5b50\u5bf9\uff0c\u9a8c\u8bc1\u96c6\u4e2d\u6709408\u5bf9\uff0c\u6d4b\u8bd5\u96c6\u4e2d\u67091,725 \u5bf9\u3002\u6bcf\u4e2a\u53e5\u5b50\u5bf9\u5305\u542b\u56db\u4e2a\u5b57\u6bb5\uff1a'sentence1', 'sentence2', 'label'\u548c 'idx'\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u7d22\u5f15\u8bbf\u95eeraw_datasets \u7684\u53e5\u5b50\u5bf9\uff1a >>> raw_train_dataset = raw_datasets [ \"train\" ] >>> raw_train_dataset [ 0 ] \u8f93\u51fa\u5982\u4e0b\uff1a { 'sentence1' : 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .' , 'sentence2' : 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .' , 'label' : 1 , 'idx' : 0 } \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7features\u83b7\u5f97\u6570\u636e\u96c6\u7684\u5b57\u6bb5\u7c7b\u578b\uff1a >>> raw_train_dataset . features \u8f93\u51fa\u5982\u4e0b\uff1a { 'sentence1' : Value ( dtype = 'string' , id = None ), 'sentence2' : Value ( dtype = 'string' , id = None ), 'label' : ClassLabel ( num_classes = 2 , names = [ 'not_equivalent' , 'equivalent' ], names_file = None , id = None ), 'idx' : Value ( dtype = 'int32' , id = None )} TIPS\uff1a 1. \u6ca1\u6709\u6570\u636e\u96c6\u7684\u8bdd\u9996\u5148\u5b89\u88c5\u4e00\u4e0b\uff1a pip install datasets 2. \u8fd9\u91cc\u5f88\u5bb9\u6613\u51fa\u73b0\u8fde\u63a5\u9519\u8bef\uff0c\u89e3\u51b3\u65b9\u6cd5\u5982\u4e0b\uff1a https://blog.csdn.net/qq_20849045/article/details/117462846?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link \u6570\u636e\u96c6\u9884\u5904\u7406 \u00b6 \u901a\u8fc7\u6570\u636e\u96c6\u9884\u5904\u7406\uff0c\u6211\u4eec\u5c06\u6587\u672c\u8f6c\u6362\u6210\u6a21\u578b\u80fd\u7406\u89e3\u7684\u5411\u91cf\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u901a\u8fc7Tokenizer\u5b9e\u73b0\uff1a >>> from transformers import AutoTokenizer >>> checkpoint = \"bert-base-uncased\" >>> tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) >>> tokenized_sentences_1 = tokenizer ( raw_datasets [ \"train\" ][ \"sentence1\" ]) >>> tokenized_sentences_2 = tokenizer ( raw_datasets [ \"train\" ][ \"sentence2\" ]) \uff08TODO\uff09 \u4f7f\u7528Trainer API\u5fae\u8c03\u4e00\u4e2a\u6a21\u578b \u00b6 \u8bad\u7ec3 \u00b6 \u8bc4\u4f30\u51fd\u6570 \u00b6 \u8865\u5145\u90e8\u5206 \u00b6 \u4e3a\u4ec0\u4e484\u4e2d\u7528Trainer\u6765\u5fae\u8c03\u6a21\u578b\uff1f \u00b6 Training Arguments\u4e3b\u8981\u53c2\u6570 \u00b6 \u4e0d\u540c\u6a21\u578b\u7684\u52a0\u8f7d\u65b9\u5f0f \u00b6 Dynamic Padding\u2014\u2014\u52a8\u6001\u586b\u5145\u6280\u672f \u00b6 \u53c2\u8003\u8d44\u6599 \u00b6 \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8--\u5728\u7ebf\u9605\u8bfb https://datawhalechina.github.io/learn-nlp-with-transformers/#/ Huggingface\u5b98\u65b9\u6559\u7a0b https://huggingface.co/course/chapter1","title":"Task06 BERT\u5e94\u7528\u3001\u8bad\u7ec3\u548c\u4f18\u5316"},{"location":"nlp-transformer-task06/#task06-bert","text":"\u8be5\u90e8\u5206\u7684\u5185\u5bb9\u7ffb\u8bd1\u81ea\ud83e\udd17HuggingFace\u5b98\u7f51\u6559\u7a0b\u7b2c1\u90e8\u5206\uff081-4\u7ae0\uff09\uff0c\u89c1 https://huggingface.co/course/chapter1 \u3002\u8be5\u7cfb\u5217\u6559\u7a0b\u75313\u5927\u90e8\u5206\u517112\u7ae0\u7ec4\u6210\uff08\u5982\u56fe\uff09\uff0c\u5176\u4e2d\u7b2c1\u90e8\u5206\u4ecb\u7ecdtransformers\u5e93\u7684\u4e3b\u8981\u6982\u5ff5\u3001\u6a21\u578b\u7684\u5de5\u4f5c\u539f\u7406\u548c\u4f7f\u7528\u65b9\u6cd5\u3001\u600e\u6837\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7b49\u5185\u5bb9\u3002","title":"Task06 BERT\u5e94\u7528\u3001\u8bad\u7ec3\u548c\u4f18\u5316"},{"location":"nlp-transformer-task06/#_1","text":"\u7b80\u5355\u7684\u8bf4\uff0c\u6709\u4e24\u79cd\u53ef\u4ee5\u8dd1\u6a21\u578b\u4ee3\u7801\u7684\u65b9\u5f0f\uff1a 1. Google Colab 2. \u672c\u5730\u865a\u62df\u73af\u5883 pip install transformers \u8be6\u89c1 https://huggingface.co/course/chapter0?fw=pt","title":"\u73af\u5883\u642d\u5efa"},{"location":"nlp-transformer-task06/#transformer","text":"","title":"Transformer\u6a21\u578b\u6982\u8ff0"},{"location":"nlp-transformer-task06/#transformers","text":"\u76ee\u524d\u53ef\u7528\u7684\u4e00\u4e9bpipeline\u662f\uff1a - feature-extraction \u83b7\u53d6\u6587\u672c\u7684\u5411\u91cf\u8868\u793a - fill-mask \u5b8c\u5f62\u586b\u7a7a - ner (named entity recognition) \u547d\u540d\u5b9e\u4f53\u8bc6\u522b - question-answering \u95ee\u7b54 - sentiment-analysis \u60c5\u611f\u5206\u6790 - summarization \u6458\u8981\u751f\u6210 - text-generation \u6587\u672c\u751f\u6210 - translation \u7ffb\u8bd1 - zero-shot-classification \u96f6\u6837\u672c\u5206\u7c7b pipeline: \u76f4\u8bd1\u7ba1\u9053/\u6d41\u6c34\u7ebf\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u6d41\u7a0b\u3002","title":"Transformers, \u53ef\u4ee5\u505a\u4ec0\u4e48\uff1f"},{"location":"nlp-transformer-task06/#transformers_1","text":"","title":"Transformers, \u5982\u4f55\u5de5\u4f5c\uff1f"},{"location":"nlp-transformer-task06/#transformer_1","text":"Transformer \u67b6\u6784\u4e8e 2017 \u5e74 6 \u6708\u63a8\u51fa\u3002\u539f\u59cb\u7814\u7a76\u7684\u91cd\u70b9\u662f\u7ffb\u8bd1\u4efb\u52a1\u3002\u968f\u540e\u63a8\u51fa\u4e86\u51e0\u4e2a\u6709\u5f71\u54cd\u529b\u7684\u6a21\u578b\uff0c\u5305\u62ec\uff1a - 2018 \u5e74 6 \u6708\uff1aGPT\uff0c\u7b2c\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684 Transformer \u6a21\u578b\uff0c\u7528\u4e8e\u5404\u79cd NLP \u4efb\u52a1\u7684\u5fae\u8c03\u5e76\u83b7\u5f97\u6700\u5148\u8fdb\u7684\u7ed3\u679c - 2018 \u5e74 10 \u6708\uff1aBERT\uff0c\u53e6\u4e00\u4e2a\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u65e8\u5728\u751f\u6210\u66f4\u597d\u7684\u53e5\u5b50\u6458\u8981 - 2019 \u5e74 2 \u6708\uff1aGPT-2\uff0cGPT \u7684\u6539\u8fdb\uff08\u548c\u66f4\u5927\uff09\u7248\u672c - 2019 \u5e74 10 \u6708\uff1aDistilBERT\uff0cBERT \u7684\u84b8\u998f\u7248\u672c\uff0c\u901f\u5ea6\u63d0\u9ad8 60%\uff0c\u5185\u5b58\u51cf\u8f7b 40%\uff0c\u4f46\u4ecd\u4fdd\u7559 BERT 97% \u7684\u6027\u80fd - 2019 \u5e74 10 \u6708\uff1aBART \u548c T5\uff0c\u4e24\u4e2a\u4f7f\u7528\u4e0e\u539f\u59cb Transformer \u6a21\u578b\u76f8\u540c\u67b6\u6784\u7684\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u7b2c\u4e00\u4e2a\u8fd9\u6837\u505a\uff09 - 2020 \u5e74 5 \u6708\uff0cGPT-3\uff0cGPT-2 \u7684\u66f4\u5927\u7248\u672c\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff08\u79f0\u4e3a\u96f6\u6837\u672c\u5b66\u4e60zero-shot learning\uff09 \u5927\u4f53\u4e0a\uff0c\u5b83\u4eec\u53ef\u4ee5\u5206\u4e3a\u4e09\u7c7b\uff1a - GPT\u7c7b\uff08\u53c8\u79f0\u4e3a\u81ea\u56de\u5f52 Transformer \u6a21\u578b\uff09\uff1a\u53ea\u4f7f\u7528transformer-decoder\u90e8\u5206 - BERT\u7c7b\uff08\u53c8\u79f0\u4e3a\u81ea\u7f16\u7801 Transformer \u6a21\u578b\uff09\uff1a\u53ea\u4f7f\u7528transformer-encoder\u90e8\u5206 - BART/T5\u7c7b\uff08\u53c8\u79f0\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217 Transformer \u6a21\u578b\uff09\uff1a\u4f7f\u7528Transformer-encoder-decoder\u90e8\u5206 \u5b83\u4eec\u7684\u5206\u7c7b\u3001\u5177\u4f53\u6a21\u578b\u3001\u4e3b\u8981\u5e94\u7528\u4efb\u52a1\u5982\u4e0b\uff1a \u5176\u4ed6\u9700\u8981\u77e5\u9053\u7684\uff1a - Transformers\u662f\u8bed\u8a00\u6a21\u578b - Transformers\u662f\u5927\u6a21\u578b - Transformers\u7684\u5e94\u7528\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e24\u4e2a\u8fc7\u7a0b","title":"Transformer\u7b80\u53f2"},{"location":"nlp-transformer-task06/#architecturecheckpoints","text":"Architecture/\u67b6\u6784 \uff1a\u5b9a\u4e49\u4e86\u6a21\u578b\u7684\u57fa\u672c\u7ed3\u6784\u548c\u57fa\u672c\u8fd0\u7b97\u3002 Checkpoints/\u68c0\u67e5\u70b9 \uff1a\u6a21\u578b\u7684\u67d0\u4e2a\u8bad\u7ec3\u72b6\u6001\uff0c\u52a0\u8f7d\u6b64checkpoint\u4f1a\u52a0\u8f7d\u6b64\u65f6\u7684\u6743\u91cd\u3002\u8bad\u7ec3\u65f6\u53ef\u4ee5\u9009\u62e9\u81ea\u52a8\u4fdd\u5b58checkpoint\u3002\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u53ef\u4ee5\u8bbe\u7f6e\u81ea\u52a8\u4fdd\u5b58\u4e8e\u67d0\u4e2a\u65f6\u95f4\u70b9\uff08\u6bd4\u5982\u6a21\u578b\u8bad\u7ec3\u4e86\u4e00\u8f6eepoch\uff0c\u66f4\u65b0\u4e86\u53c2\u6570\uff0c\u5c06\u8fd9\u4e2a\u72b6\u6001\u7684\u6a21\u578b\u4fdd\u5b58\u4e0b\u6765\uff0c\u4e3a\u4e00\u4e2acheckpoint\u3002\uff09 \u6240\u4ee5\u6bcf\u4e2acheckpoint\u5bf9\u5e94\u6a21\u578b\u7684\u4e00\u4e2a\u72b6\u6001\uff0c\u4e00\u7ec4\u6743\u91cd\u3002","title":"\u540d\u8bcd\u89e3\u91ca\uff1aArchitecture\u548cCheckpoints"},{"location":"nlp-transformer-task06/#transformers_2","text":"","title":"\u4f7f\u7528Transformers"},{"location":"nlp-transformer-task06/#3","text":"\u5c06\u4e00\u4e9b\u6587\u672c\u4f20\u9012\u5230pipeline\u65f6\u6d89\u53ca3\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a 1. \u6587\u672c\u88ab\u9884\u5904\u7406\u4e3a\u6a21\u578b\u53ef\u4ee5\u7406\u89e3\u7684\u683c\u5f0f\u3002 2. \u9884\u5904\u7406\u540e\u7684\u8f93\u5165\u4f20\u9012\u7ed9\u6a21\u578b\u3002 3. \u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u88ab\u540e\u5904\u7406\u4e3a\u4eba\u7c7b\u53ef\u4ee5\u7406\u89e3\u7684\u683c\u5f0f\u3002 Pipeline\u5c063\u4e2a\u6b65\u9aa4\u7ec4\u5408\u5728\u4e00\u8d77\uff1a\u9884\u5904\u7406/Tokenizer\u3001\u901a\u8fc7\u6a21\u578b\u4f20\u9012\u8f93\u5165/Model\u548c\u540e\u5904\u7406/Post-Processing\uff1a","title":"3\u4e2a\u5904\u7406\u6b65\u9aa4"},{"location":"nlp-transformer-task06/#tokenizer","text":"Tokenizer\u7684\u4f5c\u7528\uff1a - \u5c06\u8f93\u5165\u62c6\u5206\u4e3a\u79f0\u4e3atoken\u7684\u5355\u8bcd\u3001\u5b50\u8bcd/subword\u6216\u7b26\u53f7/symbols\uff08\u5982\u6807\u70b9\u7b26\u53f7\uff09 - \u5c06\u6bcf\u4e2atoken\u6620\u5c04\u5230\u4e00\u4e2a\u6574\u6570 - \u6dfb\u52a0\u53ef\u80fd\u5bf9\u6a21\u578b\u6709\u7528\u7684\u5176\u4ed6\u8f93\u5165","title":"Tokenizer/\u9884\u5904\u7406"},{"location":"nlp-transformer-task06/#going-through-models","text":"","title":"Going Through Models/\u7a7f\u8fc7\u6a21\u578b"},{"location":"nlp-transformer-task06/#_2","text":"from transformers import AutoModel checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" model = AutoModel . from_pretrained ( checkpoint ) \u5728\u8fd9\u6bb5\u4ee3\u7801\u4e2d\uff0c\u6211\u4eec\u4e0b\u8f7d\u4e86\u5728pipeline\u4e2d\u4f7f\u7528\u7684\u76f8\u540c\u68c0\u67e5\u70b9\uff08\u5b9e\u9645\u4e0a\u5df2\u7ecf\u7f13\u5b58\uff09\u5e76\u5c06\u6a21\u578b\u5b9e\u4f8b\u5316\u3002","title":"\u6a21\u578b\u5b9e\u4f8b\u5316"},{"location":"nlp-transformer-task06/#_3","text":"\u6a21\u578b\u7684\u8f93\u51fa\u5411\u91cf\u901a\u5e38\u6709\u4e09\u4e2a\u7ef4\u5ea6\uff1a - Batch size: \u4e00\u6b21\u5904\u7406\u7684\u5e8f\u5217\u6570 - Sequence length: \u5e8f\u5217\u5411\u91cf\u7684\u957f\u5ea6 - Hidden size: \u6bcf\u4e2a\u6a21\u578b\u8f93\u5165\u5904\u7406\u540e\u7684\u5411\u91cf\u7ef4\u5ea6\uff08hidden state vector\uff09","title":"\u6a21\u578b\u7684\u8f93\u51fa\uff1a\u9ad8\u7ef4\u5411\u91cf"},{"location":"nlp-transformer-task06/#model-heads","text":"Model heads:\u5c06\u9690\u85cf\u72b6\u6001\u7684\u9ad8\u7ef4\u5411\u91cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u5c06\u5b83\u4eec\u6295\u5f71\u5230\u4e0d\u540c\u7684\u7ef4\u5ea6\u4e0a\u3002\u5b83\u4eec\u901a\u5e38\u7531\u4e00\u4e2a\u6216\u51e0\u4e2a\u7ebf\u6027\u5c42\u7ec4\u6210\u3002 \u5982\u4e0a\u56fe\u6240\u793a\uff0c\u7d2b\u8272\u4ee3\u8868\u5411\u91cf\uff0c\u7c89\u8272\u4ee3\u8868\u6a21\u7ec4\uff0cEmbeddings+layers\u8868\u793aTransformer\u7684\u67b6\u6784\uff0c\u7ecf\u8fc7\u8fd9\u5c42\u67b6\u6784\u540e\u7684\u8f93\u51fa\u9001\u5165Model Head\u8fdb\u884c\u5904\u7406\uff0c\u4ece\u800c\u5e94\u7528\u5230\u4e0d\u540c\u7684\u4e0b\u6e38\u4efb\u52a1\u3002 \ud83e\udd17 Transformers \u4e2d\u6709\u8bb8\u591a\u4e0d\u540c\u7684Head\u67b6\u6784\u53ef\u7528\uff0c\u6bcf\u4e00\u79cd\u67b6\u6784\u90fd\u56f4\u7ed5\u7740\u5904\u7406\u7279\u5b9a\u4efb\u52a1\u800c\u8bbe\u8ba1\u3002 \u4e0b\u9762\u5217\u4e3e\u4e86\u90e8\u5206Model heads\uff1a *Model (retrieve the hidden states) *ForCausalLM *ForMaskedLM *ForMultipleChoice *ForQuestionAnswering *ForSequenceClassification *ForTokenClassification and others \ud83e\udd17","title":"Model Heads\uff1a\u4e3a\u4e86\u5904\u7406\u4e0d\u540c\u7684\u4efb\u52a1"},{"location":"nlp-transformer-task06/#post-processing","text":"\u4ece\u6a21\u578b\u4e2d\u83b7\u5f97\u7684\u4f5c\u4e3a\u8f93\u51fa\u7684\u503c\u672c\u8eab\u5e76\u4e0d\u4e00\u5b9a\u6709\u610f\u4e49\u3002\u8981\u8f6c\u6362\u4e3a\u6982\u7387\uff0c\u5b83\u4eec\u9700\u8981\u7ecf\u8fc7\u4e00\u4e2a SoftMax \u5c42\u3002","title":"Post-processing/\u540e\u5904\u7406"},{"location":"nlp-transformer-task06/#_4","text":"","title":"\u5fae\u8c03\u4e00\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b"},{"location":"nlp-transformer-task06/#_5","text":"\u5728\u672c\u8282\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528MRPC\uff08Microsoft Research Praphrase Corpus\uff09\u6570\u636e\u96c6\u4f5c\u4e3a\u793a\u4f8b\u3002\u8be5DataSet\u75315,801\u5bf9\u53e5\u5b50\u7ec4\u6210\uff0c\u6807\u7b7e\u6307\u793a\u5b83\u4eec\u662f\u5426\u662f\u540c\u4e49\u53e5\uff08\u5373\u4e24\u4e2a\u53e5\u5b50\u662f\u5426\u8868\u793a\u76f8\u540c\u7684\u610f\u601d\uff09\u3002 \u6211\u4eec\u9009\u62e9\u5b83\u662f\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u5c0f\u578b\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u53ef\u4ee5\u8f7b\u677e\u8bad\u7ec3\u3002","title":"\u6570\u636e\u5904\u7406"},{"location":"nlp-transformer-task06/#hub","text":"Hub\u4e0d\u4ec5\u5305\u542b\u6a21\u578b\uff0c\u8fd8\u542b\u6709\u591a\u79cd\u8bed\u8a00\u7684datasets\u3002 \u4f8b\u5982\uff0cMRPC\u6570\u636e\u96c6\u662f\u6784\u6210 GLUE benchmark\u7684 10 \u4e2a\u6570\u636e\u96c6\u4e4b\u4e00\u3002GLUE\uff08General Language Understanding Evaluation\uff09\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u548c\u5206\u6790\u5e73\u53f0\u3002GLUE\u5305\u542b\u4e5d\u9879NLU\u4efb\u52a1\uff0c\u8bed\u8a00\u5747\u4e3a\u82f1\u8bed\u3002GLUE\u4e5d\u9879\u4efb\u52a1\u6d89\u53ca\u5230\u81ea\u7136\u8bed\u8a00\u63a8\u65ad\u3001\u6587\u672c\u8574\u542b\u3001\u60c5\u611f\u5206\u6790\u3001\u8bed\u4e49\u76f8\u4f3c\u7b49\u591a\u4e2a\u4efb\u52a1\u3002\u50cfBERT\u3001XLNet\u3001RoBERTa\u3001ERINE\u3001T5\u7b49\u77e5\u540d\u6a21\u578b\u90fd\u4f1a\u5728\u6b64\u57fa\u51c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002 \ud83e\udd17 Datasets\u5e93\u63d0\u4f9b\u4e86\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684\u547d\u4ee4\u6765\u4e0b\u8f7d\u548c\u7f13\u5b58Hub\u4e0a\u7684dataset\u3002 \u6211\u4eec\u53ef\u4ee5\u50cf\u8fd9\u6837\u4e0b\u8f7d MRPC \u6570\u636e\u96c6\uff1a >>> from datasets import load_dataset >>> raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) >>> raw_datasets \u8f93\u51fa\u5982\u4e0b\uff1a DatasetDict ({ train : Dataset ({ features : [ 'sentence1' , 'sentence2' , 'label' , 'idx' ], num_rows : 3668 }) validation : Dataset ({ features : [ 'sentence1' , 'sentence2' , 'label' , 'idx' ], num_rows : 408 }) test : Dataset ({ features : [ 'sentence1' , 'sentence2' , 'label' , 'idx' ], num_rows : 1725 }) }) \u8fd9\u6837\u5c31\u5f97\u5230\u4e00\u4e2aDatasetDict\u5bf9\u8c61\uff0c\u5305\u542b\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u8bad\u7ec3\u96c6\u4e2d\u67093,668 \u4e2a\u53e5\u5b50\u5bf9\uff0c\u9a8c\u8bc1\u96c6\u4e2d\u6709408\u5bf9\uff0c\u6d4b\u8bd5\u96c6\u4e2d\u67091,725 \u5bf9\u3002\u6bcf\u4e2a\u53e5\u5b50\u5bf9\u5305\u542b\u56db\u4e2a\u5b57\u6bb5\uff1a'sentence1', 'sentence2', 'label'\u548c 'idx'\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u7d22\u5f15\u8bbf\u95eeraw_datasets \u7684\u53e5\u5b50\u5bf9\uff1a >>> raw_train_dataset = raw_datasets [ \"train\" ] >>> raw_train_dataset [ 0 ] \u8f93\u51fa\u5982\u4e0b\uff1a { 'sentence1' : 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .' , 'sentence2' : 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .' , 'label' : 1 , 'idx' : 0 } \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7features\u83b7\u5f97\u6570\u636e\u96c6\u7684\u5b57\u6bb5\u7c7b\u578b\uff1a >>> raw_train_dataset . features \u8f93\u51fa\u5982\u4e0b\uff1a { 'sentence1' : Value ( dtype = 'string' , id = None ), 'sentence2' : Value ( dtype = 'string' , id = None ), 'label' : ClassLabel ( num_classes = 2 , names = [ 'not_equivalent' , 'equivalent' ], names_file = None , id = None ), 'idx' : Value ( dtype = 'int32' , id = None )} TIPS\uff1a 1. \u6ca1\u6709\u6570\u636e\u96c6\u7684\u8bdd\u9996\u5148\u5b89\u88c5\u4e00\u4e0b\uff1a pip install datasets 2. \u8fd9\u91cc\u5f88\u5bb9\u6613\u51fa\u73b0\u8fde\u63a5\u9519\u8bef\uff0c\u89e3\u51b3\u65b9\u6cd5\u5982\u4e0b\uff1a https://blog.csdn.net/qq_20849045/article/details/117462846?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link","title":"\u4eceHub\u4e0a\u52a0\u8f7d\u6570\u636e\u96c6"},{"location":"nlp-transformer-task06/#_6","text":"\u901a\u8fc7\u6570\u636e\u96c6\u9884\u5904\u7406\uff0c\u6211\u4eec\u5c06\u6587\u672c\u8f6c\u6362\u6210\u6a21\u578b\u80fd\u7406\u89e3\u7684\u5411\u91cf\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u901a\u8fc7Tokenizer\u5b9e\u73b0\uff1a >>> from transformers import AutoTokenizer >>> checkpoint = \"bert-base-uncased\" >>> tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) >>> tokenized_sentences_1 = tokenizer ( raw_datasets [ \"train\" ][ \"sentence1\" ]) >>> tokenized_sentences_2 = tokenizer ( raw_datasets [ \"train\" ][ \"sentence2\" ]) \uff08TODO\uff09","title":"\u6570\u636e\u96c6\u9884\u5904\u7406"},{"location":"nlp-transformer-task06/#trainer-api","text":"","title":"\u4f7f\u7528Trainer API\u5fae\u8c03\u4e00\u4e2a\u6a21\u578b"},{"location":"nlp-transformer-task06/#_7","text":"","title":"\u8bad\u7ec3"},{"location":"nlp-transformer-task06/#_8","text":"","title":"\u8bc4\u4f30\u51fd\u6570"},{"location":"nlp-transformer-task06/#_9","text":"","title":"\u8865\u5145\u90e8\u5206"},{"location":"nlp-transformer-task06/#4trainer","text":"","title":"\u4e3a\u4ec0\u4e484\u4e2d\u7528Trainer\u6765\u5fae\u8c03\u6a21\u578b\uff1f"},{"location":"nlp-transformer-task06/#training-arguments","text":"","title":"Training Arguments\u4e3b\u8981\u53c2\u6570"},{"location":"nlp-transformer-task06/#_10","text":"","title":"\u4e0d\u540c\u6a21\u578b\u7684\u52a0\u8f7d\u65b9\u5f0f"},{"location":"nlp-transformer-task06/#dynamic-padding","text":"","title":"Dynamic Padding\u2014\u2014\u52a8\u6001\u586b\u5145\u6280\u672f"},{"location":"nlp-transformer-task06/#_11","text":"\u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8--\u5728\u7ebf\u9605\u8bfb https://datawhalechina.github.io/learn-nlp-with-transformers/#/ Huggingface\u5b98\u65b9\u6559\u7a0b https://huggingface.co/course/chapter1","title":"\u53c2\u8003\u8d44\u6599"},{"location":"nlp-transformer-task07/","text":"Task07 \u4f7f\u7528Transformers\u89e3\u51b3\u6587\u672c\u5206\u7c7b\u4efb\u52a1 \u00b6 \u8be5\u90e8\u5206\u7684\u5185\u5bb9\u7ffb\u8bd1\u81ea\ud83e\udd17HuggingFace/notebooks https://github.com/huggingface/notebooks/tree/master/examples \u4e2d\u6587\u7ffb\u8bd1\uff1aDatawhale/learn-nlp-with-transformers/4.1-\u6587\u672c\u5206\u7c7b Datawhale/learn-nlp-with-transformers/4.1-\u6587\u672c\u5206\u7c7b \u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u6587\u672c\u5206\u7c7b \u00b6 \u6211\u4eec\u5c06\u4f7f\u7528 \ud83e\udd17 Transformers\u4ee3\u7801\u5e93\u4e2d\u7684\u6a21\u578b\u6765\u89e3\u51b3\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff0c\u4efb\u52a1\u6765\u6e90\u4e8eGLUE Benchmark. GLUE\u699c\u5355\u5305\u542b\u4e869\u4e2a\u53e5\u5b50\u7ea7\u522b\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u5206\u522b\u662f\uff1a - CoLA (Corpus of Linguistic Acceptability) \u9274\u522b\u4e00\u4e2a\u53e5\u5b50\u662f\u5426\u8bed\u6cd5\u6b63\u786e. - MNLI (Multi-Genre Natural Language Inference) \u7ed9\u5b9a\u4e00\u4e2a\u5047\u8bbe\uff0c\u5224\u65ad\u53e6\u4e00\u4e2a\u53e5\u5b50\u4e0e\u8be5\u5047\u8bbe\u7684\u5173\u7cfb\uff1aentails, contradicts \u6216\u8005 unrelated\u3002 - MRPC (Microsoft Research Paraphrase Corpus) \u5224\u65ad\u4e24\u4e2a\u53e5\u5b50\u662f\u5426\u4e92\u4e3aparaphrases. - QNLI (Question-answering Natural Language Inference) \u5224\u65ad\u7b2c2\u53e5\u662f\u5426\u5305\u542b\u7b2c1\u53e5\u95ee\u9898\u7684\u7b54\u6848\u3002 - QQP (Quora Question Pairs2) \u5224\u65ad\u4e24\u4e2a\u95ee\u53e5\u662f\u5426\u8bed\u4e49\u76f8\u540c\u3002 - RTE (Recognizing Textual Entailment)\u5224\u65ad\u4e00\u4e2a\u53e5\u5b50\u662f\u5426\u4e0e\u5047\u8bbe\u6210entail\u5173\u7cfb\u3002 - SST-2 (Stanford Sentiment Treebank) \u5224\u65ad\u4e00\u4e2a\u53e5\u5b50\u7684\u60c5\u611f\u6b63\u8d1f\u5411. - STS-B (Semantic Textual Similarity Benchmark) \u5224\u65ad\u4e24\u4e2a\u53e5\u5b50\u7684\u76f8\u4f3c\u6027\uff08\u5206\u6570\u4e3a1-5\u5206\uff09\u3002 - WNLI (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. \u5bf9\u4e8e\u4ee5\u4e0a\u4efb\u52a1\uff0c\u6211\u4eec\u5c06\u5c55\u793a\u5982\u4f55\u4f7f\u7528\u7b80\u5355\u7684Dataset\u5e93\u52a0\u8f7d\u6570\u636e\u96c6\uff0c\u540c\u65f6\u4f7f\u7528transformer\u4e2d\u7684Trainer\u63a5\u53e3\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002 GLUE_TASKS = [ \"cola\" , \"mnli\" , \"mnli-mm\" , \"mrpc\" , \"qnli\" , \"qqp\" , \"rte\" , \"sst2\" , \"stsb\" , \"wnli\" ] This notebook is built to run on any of the tasks in the list above, with any model checkpoint from the Model Hub as long as that model has a version with a classification head. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly: \u672cnotebook\u7406\u8bba\u4e0a\u53ef\u4ee5\u4f7f\u7528\u5404\u79cd\u5404\u6837\u7684transformer\u6a21\u578b\uff08\u6a21\u578b\u9762\u677f\uff09\uff0c\u89e3\u51b3\u4efb\u4f55\u6587\u672c\u5206\u7c7b\u5206\u7c7b\u4efb\u52a1\u3002\u5982\u679c\u60a8\u6240\u5904\u7406\u7684\u4efb\u52a1\u6709\u6240\u4e0d\u540c\uff0c\u5927\u6982\u7387\u53ea\u9700\u8981\u5f88\u5c0f\u7684\u6539\u52a8\u4fbf\u53ef\u4ee5\u4f7f\u7528\u672cnotebook\u8fdb\u884c\u5904\u7406\u3002\u540c\u65f6\uff0c\u60a8\u5e94\u8be5\u6839\u636e\u60a8\u7684GPU\u663e\u5b58\u6765\u8c03\u6574\u5fae\u8c03\u8bad\u7ec3\u6240\u9700\u8981\u7684btach size\u5927\u5c0f\uff0c\u907f\u514d\u663e\u5b58\u6ea2\u51fa\u3002 task = \"cola\" model_checkpoint = \"distilbert-base-uncased\" batch_size = 16 \u52a0\u8f7d\u6570\u636e\u96c6 \u00b6 We will use the \ud83e\udd17 Datasets library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions load_dataset and load_metric . \u6211\u4eec\u5c06\u4f1a\u4f7f\u7528\ud83e\udd17 Datasets\u5e93\u6765\u52a0\u8f7d\u6570\u636e\u548c\u5bf9\u5e94\u7684\u8bc4\u6d4b\u65b9\u5f0f\u3002\u6570\u636e\u52a0\u8f7d\u548c\u8bc4\u6d4b\u65b9\u5f0f\u52a0\u8f7d\u53ea\u9700\u8981\u7b80\u5355\u4f7f\u7528load_dataset\u548cload_metric\u5373\u53ef\u3002 from datasets import load_dataset, load_metric Apart from mnli-mm being a special code, we can directly pass our task name to those functions. load_dataset will cache the dataset to avoid downloading it again the next time you run this cell. \u9664\u4e86mnli-mm\u4ee5\u5916\uff0c\u5176\u4ed6\u4efb\u52a1\u90fd\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7\u4efb\u52a1\u540d\u5b57\u8fdb\u884c\u52a0\u8f7d\u3002\u6570\u636e\u52a0\u8f7d\u4e4b\u540e\u4f1a\u81ea\u52a8\u7f13\u5b58\u3002 actual_task = \"mnli\" if task == \"mnli-mm\" else task dataset = load_dataset ( \"glue\" , actual_task ) metric = load_metric ( 'glue' , actual_task ) \u4e0a\u8282\u8bb2\u8fc7\uff0c\u8fd9\u91cc\uff0c\u6700\u597d\u624b\u52a8\u4e0b\u8f7dglue.py\u548cgule_metric.py\uff0c\u4e0d\u4e0b\u8f7d\u5230\u672c\u5730\u7684\u8bdd\uff0c\u5bb9\u6613\u51fa\u73b0\u8fde\u63a5\u9519\u8bef\u3002 The dataset object itself is DatasetDict, which contains one key for the training, validation and test set (with more keys for the mismatched validation and test set in the special case of mnli). \u8fd9\u4e2adatasets\u5bf9\u8c61\u672c\u8eab\u662f\u4e00\u79cdDatasetDict\u6570\u636e\u7ed3\u6784.\u5bf9\u4e8e\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u53ea\u9700\u8981\u4f7f\u7528\u5bf9\u5e94\u7684key\uff08train\uff0cvalidation\uff0ctest\uff09\u5373\u53ef\u5f97\u5230\u76f8\u5e94\u7684\u6570\u636e\u3002 >>> dataset DatasetDict ({ train : Dataset ({ features : [ 'sentence' , 'label' , 'idx' ], num_rows : 8551 }) validation : Dataset ({ features : [ 'sentence' , 'label' , 'idx' ], num_rows : 1043 }) test : Dataset ({ features : [ 'sentence' , 'label' , 'idx' ], num_rows : 1063 }) }) >>> dataset [ \"train\" ][ 0 ] { 'sentence' : \"Our friends won't buy this analysis, let alone the next one we propose.\" , 'label' : 1 , 'idx' : 0 } To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset. \u4e3a\u4e86\u80fd\u591f\u8fdb\u4e00\u6b65\u7406\u89e3\u6570\u636e\u957f\u4ec0\u4e48\u6837\u5b50\uff0c\u4e0b\u9762\u7684\u51fd\u6570\u5c06\u4ece\u6570\u636e\u96c6\u91cc\u968f\u673a\u9009\u62e9\u51e0\u4e2a\u4f8b\u5b50\u8fdb\u884c\u5c55\u793a\u3002 import datasets import random import pandas as pd from IPython.display import display , HTML def show_random_elements ( dataset , num_examples = 10 ): assert num_examples <= len ( dataset ), \"Can't pick more elements than there are in the dataset.\" picks = [] for _ in range ( num_examples ): pick = random . randint ( 0 , len ( dataset ) - 1 ) while pick in picks : pick = random . randint ( 0 , len ( dataset ) - 1 ) picks . append ( pick ) df = pd . DataFrame ( dataset [ picks ]) for column , typ in dataset . features . items (): if isinstance ( typ , datasets . ClassLabel ): df [ column ] = df [ column ] . transform ( lambda i : typ . names [ i ]) display ( HTML ( df . to_html ())) show_random_elements ( dataset [ \"train\" ]) The metric is an instance of datasets.Metric: pass You can call its compute method with your predictions and labels directly and it will return a dictionary with the metric(s) value: \u76f4\u63a5\u8c03\u7528metric\u7684compute\u65b9\u6cd5\uff0c\u4f20\u5165labels\u548cpredictions\u5373\u53ef\u5f97\u5230metric\u7684\u503c\uff1a import numpy as np fake_preds = np . random . randint ( 0 , 2 , size = ( 64 ,)) fake_labels = np . random . randint ( 0 , 2 , size = ( 64 ,)) metric . compute ( predictions = fake_preds , references = fake_labels ) Note that load_metric has loaded the proper metric associated to your task, which is: \u6bcf\u4e00\u4e2a\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u6240\u5bf9\u5e94\u7684metic\u6709\u6240\u4e0d\u540c\uff0c\u5177\u4f53\u5982\u4e0b: - for CoLA: Matthews Correlation Coefficient - for MNLI (matched or mismatched): Accuracy - for MRPC: Accuracy and F1 score - for QNLI: Accuracy - for QQP: Accuracy and F1 score - for RTE: Accuracy - for SST-2: Accuracy - for STS-B: Pearson Correlation Coefficient and Spearman's_Rank_Correlation_Coefficient - for WNLI: Accuracy so the metric object only computes the one(s) needed for your task. \u6570\u636e\u9884\u5904\u7406 \u00b6 Before we can feed those texts to our model, we need to preprocess them. This is done by a \ud83e\udd17 Transformers Tokenizer which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires. \u5728\u5c06\u6570\u636e\u5582\u5165\u6a21\u578b\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u9884\u5904\u7406\u3002\u9884\u5904\u7406\u7684\u5de5\u5177\u53ebTokenizer\u3002Tokenizer\u9996\u5148\u5bf9\u8f93\u5165\u8fdb\u884ctokenize\uff0c\u7136\u540e\u5c06tokens\u8f6c\u5316\u4e3a\u9884\u6a21\u578b\u4e2d\u9700\u8981\u5bf9\u5e94\u7684token ID\uff0c\u518d\u8f6c\u5316\u4e3a\u6a21\u578b\u9700\u8981\u7684\u8f93\u5165\u683c\u5f0f\u3002 To do all of this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which will ensure: - we get a tokenizer that corresponds to the model architecture we want to use, - we download the vocabulary used when pretraining this specific checkpoint. \u4e3a\u4e86\u8fbe\u5230\u6570\u636e\u9884\u5904\u7406\u7684\u76ee\u7684\uff0c\u6211\u4eec\u4f7f\u7528AutoTokenizer.from_pretrained\u65b9\u6cd5\u5b9e\u4f8b\u5316\u6211\u4eec\u7684tokenizer\uff0c\u8fd9\u6837\u53ef\u4ee5\u786e\u4fdd\uff1a - \u6211\u4eec\u5f97\u5230\u4e00\u4e2a\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u4e00\u4e00\u5bf9\u5e94\u7684tokenizer\u3002 - \u4f7f\u7528\u6307\u5b9a\u7684\u6a21\u578bcheckpoint\u5bf9\u5e94\u7684tokenizer\u7684\u65f6\u5019\uff0c\u6211\u4eec\u4e5f\u4e0b\u8f7d\u4e86\u6a21\u578b\u9700\u8981\u7684\u8bcd\u8868\u5e93vocabulary\uff0c\u51c6\u786e\u6765\u8bf4\u662ftokens vocabulary\u3002 That vocabulary will be cached, so it's not downloaded again the next time we run the cell. \u8fd9\u4e2a\u88ab\u4e0b\u8f7d\u7684tokens vocabulary\u4f1a\u88ab\u7f13\u5b58\u8d77\u6765\uff0c\u4ece\u800c\u518d\u6b21\u4f7f\u7528\u7684\u65f6\u5019\u4e0d\u4f1a\u91cd\u65b0\u4e0b\u8f7d\u3002 from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( model_checkpoint , use_fast = True ) You can directly call this tokenizer on one sentence or a pair of sentences: tokenizer ( \"Hello, this one sentence!\" , \"And this sentence goes with it.\" ) \u8f93\u51fa\u4e3a\uff1a pass To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names: task_to_keys = { \"cola\" : ( \"sentence\" , None ), \"mnli\" : ( \"premise\" , \"hypothesis\" ), \"mnli-mm\" : ( \"premise\" , \"hypothesis\" ), \"mrpc\" : ( \"sentence1\" , \"sentence2\" ), \"qnli\" : ( \"question\" , \"sentence\" ), \"qqp\" : ( \"question1\" , \"question2\" ), \"rte\" : ( \"sentence1\" , \"sentence2\" ), \"sst2\" : ( \"sentence\" , None ), \"stsb\" : ( \"sentence1\" , \"sentence2\" ), \"wnli\" : ( \"sentence1\" , \"sentence2\" ), } We can double check it does work on our current dataset: sentence1_key , sentence2_key = task_to_keys [ task ] if sentence2_key is None : print ( f \"Sentence: { dataset [ 'train' ][ 0 ][ sentence1_key ] } \" ) else : print ( f \"Sentence 1: { dataset [ 'train' ][ 0 ][ sentence1_key ] } \" ) print ( f \"Sentence 2: { dataset [ 'train' ][ 0 ][ sentence2_key ] } \" ) \u8f93\u51fa\u4e3a\uff1a Sentence: Our friends won't buy this analysis, let alone the next one we propose. We can them write the function that will preprocess our samples. We just feed them to the tokenizer with the argument truncation=True . This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. def preprocess_function ( examples ): if sentence2_key is None : return tokenizer ( examples [ sentence1_key ], truncation = True ) return tokenizer ( examples [ sentence1_key ], examples [ sentence2_key ], truncation = True ) >>> preprocess_function ( dataset [ 'train' ][: 5 ]) { 'input_ids' : [[ 101 , 2256 , 2814 , 2180 , 1005 , 1056 , 4965 , 2023 , 4106 , 1010 , 2292 , 2894 , 1996 , 2279 , 2028 , 2057 , 16599 , 1012 , 102 ], [ 101 , 2028 , 2062 , 18404 , 2236 , 3989 , 1998 , 1045 , 1005 , 1049 , 3228 , 2039 , 1012 , 102 ], [ 101 , 2028 , 2062 , 18404 , 2236 , 3989 , 2030 , 1045 , 1005 , 1049 , 3228 , 2039 , 1012 , 102 ], [ 101 , 1996 , 2062 , 2057 , 2817 , 16025 , 1010 , 1996 , 13675 , 16103 , 2121 , 2027 , 2131 , 1012 , 102 ], [ 101 , 2154 , 2011 , 2154 , 1996 , 8866 , 2024 , 2893 , 14163 , 8024 , 3771 , 1012 , 102 ]], 'attention_mask' : [[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]]} To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the map method of our dataset object we created earlier. This will apply the function on all the elements of all the splits in dataset , so our training, validation and testing data will be preprocessed in one single command. \u63a5\u4e0b\u6765\u5bf9\u6570\u636e\u96c6datasets\u91cc\u9762\u7684\u6240\u6709\u6837\u672c\u8fdb\u884c\u9884\u5904\u7406\uff0c\u5904\u7406\u7684\u65b9\u5f0f\u662f\u4f7f\u7528map\u51fd\u6570\uff0c\u5c06\u9884\u5904\u7406\u51fd\u6570prepare_train_features\u5e94\u7528\u5230\uff08map)\u6240\u6709\u6837\u672c\u4e0a\u3002 encoded_dataset = dataset . map ( preprocess_function , batched = True ) \u5fae\u8c03\u6a21\u578b \u00b6 Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about sentence classification, we use the AutoModelForSequenceClassification class. Like with the tokenizer, the from_pretrained method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which is always 2, except for STS-B which is a regression problem and MNLI where we have 3 labels): \u65e2\u7136\u6570\u636e\u5df2\u7ecf\u51c6\u5907\u597d\u4e86\uff0c\u73b0\u5728\u6211\u4eec\u9700\u8981\u4e0b\u8f7d\u5e76\u52a0\u8f7d\u6211\u4eec\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u7136\u540e\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u65e2\u7136\u6211\u4eec\u662f\u505aseq2seq\u4efb\u52a1\uff0c\u90a3\u4e48\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u80fd\u89e3\u51b3\u8fd9\u4e2a\u4efb\u52a1\u7684\u6a21\u578b\u7c7b\u3002\u6211\u4eec\u4f7f\u7528AutoModelForSequenceClassification \u8fd9\u4e2a\u7c7b\u3002\u548ctokenizer\u76f8\u4f3c\uff0cfrom_pretrained\u65b9\u6cd5\u540c\u6837\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u4e0b\u8f7d\u5e76\u52a0\u8f7d\u6a21\u578b\uff0c\u540c\u65f6\u4e5f\u4f1a\u5bf9\u6a21\u578b\u8fdb\u884c\u7f13\u5b58\uff0c\u5c31\u4e0d\u4f1a\u91cd\u590d\u4e0b\u8f7d\u6a21\u578b\u5566\u3002 from transformers import AutoModelForSequenceClassification , TrainingArguments , Trainer num_labels = 3 if task . startswith ( \"mnli\" ) else 1 if task == \"stsb\" else 2 model = AutoModelForSequenceClassification . from_pretrained ( model_checkpoint , num_labels = num_labels ) \u8f93\u51fa\u4e3a\uff1a Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight'] - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. The warning is telling us we are throwing away some weights (the vocab_transform and vocab_layer_norm layers) and randomly initializing some other (the pre_classifier and classifier layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do. \u7531\u4e8e\u6211\u4eec\u5fae\u8c03\u7684\u4efb\u52a1\u662f\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff0c\u800c\u6211\u4eec\u52a0\u8f7d\u7684\u662f\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u6240\u4ee5\u4f1a\u63d0\u793a\u6211\u4eec\u52a0\u8f7d\u6a21\u578b\u7684\u65f6\u5019\u6254\u6389\u4e86\u4e00\u4e9b\u4e0d\u5339\u914d\u7684\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\uff08\u6bd4\u5982\uff1a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u795e\u7ecf\u7f51\u7edchead\u88ab\u6254\u6389\u4e86\uff0c\u540c\u65f6\u968f\u673a\u521d\u59cb\u5316\u4e86\u6587\u672c\u5206\u7c7b\u7684\u795e\u7ecf\u7f51\u7edchead\uff09\u3002 To instantiate a Trainer , we will need to define two more things. The most important is the TrainingArguments , which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional: \u4e3a\u4e86\u80fd\u591f\u5f97\u5230\u4e00\u4e2aTrainer\u8bad\u7ec3\u5de5\u5177\uff0c\u6211\u4eec\u8fd8\u9700\u89813\u4e2a\u8981\u7d20\uff0c\u5176\u4e2d\u6700\u91cd\u8981\u7684\u662f\u8bad\u7ec3\u7684\u8bbe\u5b9a/\u53c2\u6570 TrainingArguments\u3002\u8fd9\u4e2a\u8bad\u7ec3\u8bbe\u5b9a\u5305\u542b\u4e86\u80fd\u591f\u5b9a\u4e49\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6240\u6709\u5c5e\u6027\u3002 metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\" model_name = model_checkpoint . split ( \"/\" )[ - 1 ] args = TrainingArguments ( \"test-glue\" , evaluation_strategy = \"epoch\" , save_strategy = \"epoch\" , learning_rate = 2e-5 , per_device_train_batch_size = batch_size , per_device_eval_batch_size = batch_size , num_train_epochs = 5 , weight_decay = 0.01 , load_best_model_at_end = True , metric_for_best_model = metric_name , push_to_hub = False , push_to_hub_model_id = f \" { model_name } -finetuned- { task } \" , ) Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the batch_size defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay. Since the best model might not be the one at the end of training, we ask the Trainer to load the best model it saved (according to metric_name ) at the end of training. \u4e0a\u9762evaluation_strategy = \"epoch\"\u53c2\u6570\u544a\u8bc9\u8bad\u7ec3\u4ee3\u7801\uff1a\u6211\u4eec\u6bcf\u4e2aepcoh\u4f1a\u505a\u4e00\u6b21\u9a8c\u8bc1\u8bc4\u4f30\u3002 \u4e0a\u9762batch_size\u5728\u8fd9\u4e2anotebook\u4e4b\u524d\u5b9a\u4e49\u597d\u4e86\u3002 The last two arguments are to setup everything so we can push the model to the Hub at the end of training. Remove the two of them if you didn't follow the installation steps at the top of the notebook, otherwise you can change the value of push_to_hub_model_id to something you would prefer. (\u540e\u9762\u9700\u8981\u8fde\u63a5\u5230hub\u5ba2\u6237\u7aef\uff0c\u592a\u9ebb\u70e6\uff0c\u6240\u4ee5\u5148\u8bbe\u4e3aFalse) The last thing to define for our Trainer is how to compute the metrics from the predictions. We need to define a function for this, which will just use the metric we loaded earlier, the only preprocessing we have to do is to take the argmax of our predicted logits (our just squeeze the last axis in the case of STS-B): \u6700\u540e\uff0c\u7531\u4e8e\u4e0d\u540c\u7684\u4efb\u52a1\u9700\u8981\u4e0d\u540c\u7684\u8bc4\u6d4b\u6307\u6807\uff0c\u6211\u4eec\u5b9a\u4e00\u4e2a\u51fd\u6570\u6765\u6839\u636e\u4efb\u52a1\u540d\u5b57\u5f97\u5230\u8bc4\u4ef7\u65b9\u6cd5: def compute_metrics ( eval_pred ): predictions , labels = eval_pred if task != \"stsb\" : predictions = np . argmax ( predictions , axis = 1 ) else : predictions = predictions [:, 0 ] return metric . compute ( predictions = predictions , references = labels ) Then we just need to pass all of this along with our datasets to the Trainer: validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\" trainer = Trainer ( model , args , train_dataset = encoded_dataset [ \"train\" ], eval_dataset = encoded_dataset [ validation_key ], tokenizer = tokenizer , compute_metrics = compute_metrics ) BUG: ValueError: You must login to the Hugging Face hub on this computer by typing transformers-cli login and entering your credentials to use use_auth_token=True . Alternatively, you can pass your own token as the use_auth_token argument. \u628aargs\u91cc\u9762\u7684\u8be5\u9879\u53c2\u6570\u6539\u4e3aFalse push_to_hub=False, \u3002 We can now finetune our model by just calling the train method: trainer . train () \u8f93\u51fa\u4e3a\uff1a pass We can check with the evaluate method that our Trainer did reload the best model properly (if it was not the last one): trainer . evaluate () \u8f93\u51fa\u4e3a\uff1a pass \u8d85\u53c2\u641c\u7d22 \u00b6 The Trainer supports hyperparameter search using optuna or Ray Tune. pip install optuna pip install ray [ tune ] During hyperparameter search, the Trainer will run several trainings, so it needs to have the model defined via a function (so it can be reinitialized at each new run) instead of just having it passed. We jsut use the same function as before: \u8d85\u53c2\u641c\u7d22\u65f6\uff0cTrainer\u5c06\u4f1a\u8fd4\u56de\u591a\u4e2a\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u6240\u4ee5\u9700\u8981\u4f20\u5165\u4e00\u4e2a\u5b9a\u4e49\u597d\u7684\u6a21\u578b\u4ece\u800c\u8ba9Trainer\u53ef\u4ee5\u4e0d\u65ad\u91cd\u65b0\u521d\u59cb\u5316\u8be5\u4f20\u5165\u7684\u6a21\u578b\uff1a def model_init (): return AutoModelForSequenceClassification . from_pretrained ( model_checkpoint , num_labels = num_labels ) And we can instantiate our Trainer like before: trainer = Trainer ( model_init = model_init , args = args , train_dataset = encoded_dataset [ \"train\" ], eval_dataset = encoded_dataset [ validation_key ], tokenizer = tokenizer , compute_metrics = compute_metrics ) The method we call this time is hyperparameter_search . Note that it can take a long time to run on the full dataset for some of the tasks. You can try to find some good hyperparameter on a portion of the training dataset by replacing the train_dataset line above by: train_dataset = encoded_dataset[\"train\"].shard(index=1, num_shards=10) for 1/10 th of the dataset. Then you can run a full training on the best hyperparameters picked by the search. \u8c03\u7528\u65b9\u6cd5hyperparameter_search\u3002\u6ce8\u610f\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u53ef\u80fd\u5f88\u4e45\uff0c\u6211\u4eec\u53ef\u4ee5\u5148\u7528\u90e8\u5206\u6570\u636e\u96c6\u8fdb\u884c\u8d85\u53c2\u641c\u7d22\uff0c\u518d\u8fdb\u884c\u5168\u91cf\u8bad\u7ec3\u3002 \u6bd4\u5982\u4f7f\u75281/10\u7684\u6570\u636e\u8fdb\u884c\u641c\u7d22\uff1a best_run = trainer . hyperparameter_search ( n_trials = 10 , direction = \"maximize\" ) The hyperparameter_search method returns a BestRun objects, which contains the value of the objective maximized (by default the sum of all metrics) and the hyperparameters it used for that run. hyperparameter_search\u4f1a\u8fd4\u56de\u6548\u679c\u6700\u597d\u7684\u6a21\u578b\u76f8\u5173\u7684\u53c2\u6570\uff1a >>> best_run To reproduce the best training, just set the hyperparameters in your TrainingArgument before creating a Trainer: \u5c06Trainner\u8bbe\u7f6e\u4e3a\u641c\u7d22\u5230\u7684\u6700\u597d\u53c2\u6570\uff0c\u8fdb\u884c\u8bad\u7ec3\uff1a for n , v in best_run . hyperparameters . items (): setattr ( trainer . args , n , v ) trainer . train () \u53c2\u8003\u8d44\u6599 \u00b6 HuggingFace/transfomers/BERT https://huggingface.co/transformers/model_doc/bert.html# \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8--\u5728\u7ebf\u9605\u8bfb https://datawhalechina.github.io/learn-nlp-with-transformers/#/","title":"Task07 \u4f7f\u7528Transformers\u89e3\u51b3\u6587\u672c\u5206\u7c7b\u4efb\u52a1"},{"location":"nlp-transformer-task07/#task07-transformers","text":"\u8be5\u90e8\u5206\u7684\u5185\u5bb9\u7ffb\u8bd1\u81ea\ud83e\udd17HuggingFace/notebooks https://github.com/huggingface/notebooks/tree/master/examples \u4e2d\u6587\u7ffb\u8bd1\uff1aDatawhale/learn-nlp-with-transformers/4.1-\u6587\u672c\u5206\u7c7b Datawhale/learn-nlp-with-transformers/4.1-\u6587\u672c\u5206\u7c7b","title":"Task07 \u4f7f\u7528Transformers\u89e3\u51b3\u6587\u672c\u5206\u7c7b\u4efb\u52a1"},{"location":"nlp-transformer-task07/#_1","text":"\u6211\u4eec\u5c06\u4f7f\u7528 \ud83e\udd17 Transformers\u4ee3\u7801\u5e93\u4e2d\u7684\u6a21\u578b\u6765\u89e3\u51b3\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff0c\u4efb\u52a1\u6765\u6e90\u4e8eGLUE Benchmark. GLUE\u699c\u5355\u5305\u542b\u4e869\u4e2a\u53e5\u5b50\u7ea7\u522b\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u5206\u522b\u662f\uff1a - CoLA (Corpus of Linguistic Acceptability) \u9274\u522b\u4e00\u4e2a\u53e5\u5b50\u662f\u5426\u8bed\u6cd5\u6b63\u786e. - MNLI (Multi-Genre Natural Language Inference) \u7ed9\u5b9a\u4e00\u4e2a\u5047\u8bbe\uff0c\u5224\u65ad\u53e6\u4e00\u4e2a\u53e5\u5b50\u4e0e\u8be5\u5047\u8bbe\u7684\u5173\u7cfb\uff1aentails, contradicts \u6216\u8005 unrelated\u3002 - MRPC (Microsoft Research Paraphrase Corpus) \u5224\u65ad\u4e24\u4e2a\u53e5\u5b50\u662f\u5426\u4e92\u4e3aparaphrases. - QNLI (Question-answering Natural Language Inference) \u5224\u65ad\u7b2c2\u53e5\u662f\u5426\u5305\u542b\u7b2c1\u53e5\u95ee\u9898\u7684\u7b54\u6848\u3002 - QQP (Quora Question Pairs2) \u5224\u65ad\u4e24\u4e2a\u95ee\u53e5\u662f\u5426\u8bed\u4e49\u76f8\u540c\u3002 - RTE (Recognizing Textual Entailment)\u5224\u65ad\u4e00\u4e2a\u53e5\u5b50\u662f\u5426\u4e0e\u5047\u8bbe\u6210entail\u5173\u7cfb\u3002 - SST-2 (Stanford Sentiment Treebank) \u5224\u65ad\u4e00\u4e2a\u53e5\u5b50\u7684\u60c5\u611f\u6b63\u8d1f\u5411. - STS-B (Semantic Textual Similarity Benchmark) \u5224\u65ad\u4e24\u4e2a\u53e5\u5b50\u7684\u76f8\u4f3c\u6027\uff08\u5206\u6570\u4e3a1-5\u5206\uff09\u3002 - WNLI (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. \u5bf9\u4e8e\u4ee5\u4e0a\u4efb\u52a1\uff0c\u6211\u4eec\u5c06\u5c55\u793a\u5982\u4f55\u4f7f\u7528\u7b80\u5355\u7684Dataset\u5e93\u52a0\u8f7d\u6570\u636e\u96c6\uff0c\u540c\u65f6\u4f7f\u7528transformer\u4e2d\u7684Trainer\u63a5\u53e3\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002 GLUE_TASKS = [ \"cola\" , \"mnli\" , \"mnli-mm\" , \"mrpc\" , \"qnli\" , \"qqp\" , \"rte\" , \"sst2\" , \"stsb\" , \"wnli\" ] This notebook is built to run on any of the tasks in the list above, with any model checkpoint from the Model Hub as long as that model has a version with a classification head. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly: \u672cnotebook\u7406\u8bba\u4e0a\u53ef\u4ee5\u4f7f\u7528\u5404\u79cd\u5404\u6837\u7684transformer\u6a21\u578b\uff08\u6a21\u578b\u9762\u677f\uff09\uff0c\u89e3\u51b3\u4efb\u4f55\u6587\u672c\u5206\u7c7b\u5206\u7c7b\u4efb\u52a1\u3002\u5982\u679c\u60a8\u6240\u5904\u7406\u7684\u4efb\u52a1\u6709\u6240\u4e0d\u540c\uff0c\u5927\u6982\u7387\u53ea\u9700\u8981\u5f88\u5c0f\u7684\u6539\u52a8\u4fbf\u53ef\u4ee5\u4f7f\u7528\u672cnotebook\u8fdb\u884c\u5904\u7406\u3002\u540c\u65f6\uff0c\u60a8\u5e94\u8be5\u6839\u636e\u60a8\u7684GPU\u663e\u5b58\u6765\u8c03\u6574\u5fae\u8c03\u8bad\u7ec3\u6240\u9700\u8981\u7684btach size\u5927\u5c0f\uff0c\u907f\u514d\u663e\u5b58\u6ea2\u51fa\u3002 task = \"cola\" model_checkpoint = \"distilbert-base-uncased\" batch_size = 16","title":"\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u6587\u672c\u5206\u7c7b"},{"location":"nlp-transformer-task07/#_2","text":"We will use the \ud83e\udd17 Datasets library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions load_dataset and load_metric . \u6211\u4eec\u5c06\u4f1a\u4f7f\u7528\ud83e\udd17 Datasets\u5e93\u6765\u52a0\u8f7d\u6570\u636e\u548c\u5bf9\u5e94\u7684\u8bc4\u6d4b\u65b9\u5f0f\u3002\u6570\u636e\u52a0\u8f7d\u548c\u8bc4\u6d4b\u65b9\u5f0f\u52a0\u8f7d\u53ea\u9700\u8981\u7b80\u5355\u4f7f\u7528load_dataset\u548cload_metric\u5373\u53ef\u3002 from datasets import load_dataset, load_metric Apart from mnli-mm being a special code, we can directly pass our task name to those functions. load_dataset will cache the dataset to avoid downloading it again the next time you run this cell. \u9664\u4e86mnli-mm\u4ee5\u5916\uff0c\u5176\u4ed6\u4efb\u52a1\u90fd\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7\u4efb\u52a1\u540d\u5b57\u8fdb\u884c\u52a0\u8f7d\u3002\u6570\u636e\u52a0\u8f7d\u4e4b\u540e\u4f1a\u81ea\u52a8\u7f13\u5b58\u3002 actual_task = \"mnli\" if task == \"mnli-mm\" else task dataset = load_dataset ( \"glue\" , actual_task ) metric = load_metric ( 'glue' , actual_task ) \u4e0a\u8282\u8bb2\u8fc7\uff0c\u8fd9\u91cc\uff0c\u6700\u597d\u624b\u52a8\u4e0b\u8f7dglue.py\u548cgule_metric.py\uff0c\u4e0d\u4e0b\u8f7d\u5230\u672c\u5730\u7684\u8bdd\uff0c\u5bb9\u6613\u51fa\u73b0\u8fde\u63a5\u9519\u8bef\u3002 The dataset object itself is DatasetDict, which contains one key for the training, validation and test set (with more keys for the mismatched validation and test set in the special case of mnli). \u8fd9\u4e2adatasets\u5bf9\u8c61\u672c\u8eab\u662f\u4e00\u79cdDatasetDict\u6570\u636e\u7ed3\u6784.\u5bf9\u4e8e\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u53ea\u9700\u8981\u4f7f\u7528\u5bf9\u5e94\u7684key\uff08train\uff0cvalidation\uff0ctest\uff09\u5373\u53ef\u5f97\u5230\u76f8\u5e94\u7684\u6570\u636e\u3002 >>> dataset DatasetDict ({ train : Dataset ({ features : [ 'sentence' , 'label' , 'idx' ], num_rows : 8551 }) validation : Dataset ({ features : [ 'sentence' , 'label' , 'idx' ], num_rows : 1043 }) test : Dataset ({ features : [ 'sentence' , 'label' , 'idx' ], num_rows : 1063 }) }) >>> dataset [ \"train\" ][ 0 ] { 'sentence' : \"Our friends won't buy this analysis, let alone the next one we propose.\" , 'label' : 1 , 'idx' : 0 } To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset. \u4e3a\u4e86\u80fd\u591f\u8fdb\u4e00\u6b65\u7406\u89e3\u6570\u636e\u957f\u4ec0\u4e48\u6837\u5b50\uff0c\u4e0b\u9762\u7684\u51fd\u6570\u5c06\u4ece\u6570\u636e\u96c6\u91cc\u968f\u673a\u9009\u62e9\u51e0\u4e2a\u4f8b\u5b50\u8fdb\u884c\u5c55\u793a\u3002 import datasets import random import pandas as pd from IPython.display import display , HTML def show_random_elements ( dataset , num_examples = 10 ): assert num_examples <= len ( dataset ), \"Can't pick more elements than there are in the dataset.\" picks = [] for _ in range ( num_examples ): pick = random . randint ( 0 , len ( dataset ) - 1 ) while pick in picks : pick = random . randint ( 0 , len ( dataset ) - 1 ) picks . append ( pick ) df = pd . DataFrame ( dataset [ picks ]) for column , typ in dataset . features . items (): if isinstance ( typ , datasets . ClassLabel ): df [ column ] = df [ column ] . transform ( lambda i : typ . names [ i ]) display ( HTML ( df . to_html ())) show_random_elements ( dataset [ \"train\" ]) The metric is an instance of datasets.Metric: pass You can call its compute method with your predictions and labels directly and it will return a dictionary with the metric(s) value: \u76f4\u63a5\u8c03\u7528metric\u7684compute\u65b9\u6cd5\uff0c\u4f20\u5165labels\u548cpredictions\u5373\u53ef\u5f97\u5230metric\u7684\u503c\uff1a import numpy as np fake_preds = np . random . randint ( 0 , 2 , size = ( 64 ,)) fake_labels = np . random . randint ( 0 , 2 , size = ( 64 ,)) metric . compute ( predictions = fake_preds , references = fake_labels ) Note that load_metric has loaded the proper metric associated to your task, which is: \u6bcf\u4e00\u4e2a\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u6240\u5bf9\u5e94\u7684metic\u6709\u6240\u4e0d\u540c\uff0c\u5177\u4f53\u5982\u4e0b: - for CoLA: Matthews Correlation Coefficient - for MNLI (matched or mismatched): Accuracy - for MRPC: Accuracy and F1 score - for QNLI: Accuracy - for QQP: Accuracy and F1 score - for RTE: Accuracy - for SST-2: Accuracy - for STS-B: Pearson Correlation Coefficient and Spearman's_Rank_Correlation_Coefficient - for WNLI: Accuracy so the metric object only computes the one(s) needed for your task.","title":"\u52a0\u8f7d\u6570\u636e\u96c6"},{"location":"nlp-transformer-task07/#_3","text":"Before we can feed those texts to our model, we need to preprocess them. This is done by a \ud83e\udd17 Transformers Tokenizer which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires. \u5728\u5c06\u6570\u636e\u5582\u5165\u6a21\u578b\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u9884\u5904\u7406\u3002\u9884\u5904\u7406\u7684\u5de5\u5177\u53ebTokenizer\u3002Tokenizer\u9996\u5148\u5bf9\u8f93\u5165\u8fdb\u884ctokenize\uff0c\u7136\u540e\u5c06tokens\u8f6c\u5316\u4e3a\u9884\u6a21\u578b\u4e2d\u9700\u8981\u5bf9\u5e94\u7684token ID\uff0c\u518d\u8f6c\u5316\u4e3a\u6a21\u578b\u9700\u8981\u7684\u8f93\u5165\u683c\u5f0f\u3002 To do all of this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which will ensure: - we get a tokenizer that corresponds to the model architecture we want to use, - we download the vocabulary used when pretraining this specific checkpoint. \u4e3a\u4e86\u8fbe\u5230\u6570\u636e\u9884\u5904\u7406\u7684\u76ee\u7684\uff0c\u6211\u4eec\u4f7f\u7528AutoTokenizer.from_pretrained\u65b9\u6cd5\u5b9e\u4f8b\u5316\u6211\u4eec\u7684tokenizer\uff0c\u8fd9\u6837\u53ef\u4ee5\u786e\u4fdd\uff1a - \u6211\u4eec\u5f97\u5230\u4e00\u4e2a\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u4e00\u4e00\u5bf9\u5e94\u7684tokenizer\u3002 - \u4f7f\u7528\u6307\u5b9a\u7684\u6a21\u578bcheckpoint\u5bf9\u5e94\u7684tokenizer\u7684\u65f6\u5019\uff0c\u6211\u4eec\u4e5f\u4e0b\u8f7d\u4e86\u6a21\u578b\u9700\u8981\u7684\u8bcd\u8868\u5e93vocabulary\uff0c\u51c6\u786e\u6765\u8bf4\u662ftokens vocabulary\u3002 That vocabulary will be cached, so it's not downloaded again the next time we run the cell. \u8fd9\u4e2a\u88ab\u4e0b\u8f7d\u7684tokens vocabulary\u4f1a\u88ab\u7f13\u5b58\u8d77\u6765\uff0c\u4ece\u800c\u518d\u6b21\u4f7f\u7528\u7684\u65f6\u5019\u4e0d\u4f1a\u91cd\u65b0\u4e0b\u8f7d\u3002 from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( model_checkpoint , use_fast = True ) You can directly call this tokenizer on one sentence or a pair of sentences: tokenizer ( \"Hello, this one sentence!\" , \"And this sentence goes with it.\" ) \u8f93\u51fa\u4e3a\uff1a pass To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names: task_to_keys = { \"cola\" : ( \"sentence\" , None ), \"mnli\" : ( \"premise\" , \"hypothesis\" ), \"mnli-mm\" : ( \"premise\" , \"hypothesis\" ), \"mrpc\" : ( \"sentence1\" , \"sentence2\" ), \"qnli\" : ( \"question\" , \"sentence\" ), \"qqp\" : ( \"question1\" , \"question2\" ), \"rte\" : ( \"sentence1\" , \"sentence2\" ), \"sst2\" : ( \"sentence\" , None ), \"stsb\" : ( \"sentence1\" , \"sentence2\" ), \"wnli\" : ( \"sentence1\" , \"sentence2\" ), } We can double check it does work on our current dataset: sentence1_key , sentence2_key = task_to_keys [ task ] if sentence2_key is None : print ( f \"Sentence: { dataset [ 'train' ][ 0 ][ sentence1_key ] } \" ) else : print ( f \"Sentence 1: { dataset [ 'train' ][ 0 ][ sentence1_key ] } \" ) print ( f \"Sentence 2: { dataset [ 'train' ][ 0 ][ sentence2_key ] } \" ) \u8f93\u51fa\u4e3a\uff1a Sentence: Our friends won't buy this analysis, let alone the next one we propose. We can them write the function that will preprocess our samples. We just feed them to the tokenizer with the argument truncation=True . This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. def preprocess_function ( examples ): if sentence2_key is None : return tokenizer ( examples [ sentence1_key ], truncation = True ) return tokenizer ( examples [ sentence1_key ], examples [ sentence2_key ], truncation = True ) >>> preprocess_function ( dataset [ 'train' ][: 5 ]) { 'input_ids' : [[ 101 , 2256 , 2814 , 2180 , 1005 , 1056 , 4965 , 2023 , 4106 , 1010 , 2292 , 2894 , 1996 , 2279 , 2028 , 2057 , 16599 , 1012 , 102 ], [ 101 , 2028 , 2062 , 18404 , 2236 , 3989 , 1998 , 1045 , 1005 , 1049 , 3228 , 2039 , 1012 , 102 ], [ 101 , 2028 , 2062 , 18404 , 2236 , 3989 , 2030 , 1045 , 1005 , 1049 , 3228 , 2039 , 1012 , 102 ], [ 101 , 1996 , 2062 , 2057 , 2817 , 16025 , 1010 , 1996 , 13675 , 16103 , 2121 , 2027 , 2131 , 1012 , 102 ], [ 101 , 2154 , 2011 , 2154 , 1996 , 8866 , 2024 , 2893 , 14163 , 8024 , 3771 , 1012 , 102 ]], 'attention_mask' : [[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]]} To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the map method of our dataset object we created earlier. This will apply the function on all the elements of all the splits in dataset , so our training, validation and testing data will be preprocessed in one single command. \u63a5\u4e0b\u6765\u5bf9\u6570\u636e\u96c6datasets\u91cc\u9762\u7684\u6240\u6709\u6837\u672c\u8fdb\u884c\u9884\u5904\u7406\uff0c\u5904\u7406\u7684\u65b9\u5f0f\u662f\u4f7f\u7528map\u51fd\u6570\uff0c\u5c06\u9884\u5904\u7406\u51fd\u6570prepare_train_features\u5e94\u7528\u5230\uff08map)\u6240\u6709\u6837\u672c\u4e0a\u3002 encoded_dataset = dataset . map ( preprocess_function , batched = True )","title":"\u6570\u636e\u9884\u5904\u7406"},{"location":"nlp-transformer-task07/#_4","text":"Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about sentence classification, we use the AutoModelForSequenceClassification class. Like with the tokenizer, the from_pretrained method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which is always 2, except for STS-B which is a regression problem and MNLI where we have 3 labels): \u65e2\u7136\u6570\u636e\u5df2\u7ecf\u51c6\u5907\u597d\u4e86\uff0c\u73b0\u5728\u6211\u4eec\u9700\u8981\u4e0b\u8f7d\u5e76\u52a0\u8f7d\u6211\u4eec\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u7136\u540e\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u65e2\u7136\u6211\u4eec\u662f\u505aseq2seq\u4efb\u52a1\uff0c\u90a3\u4e48\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u80fd\u89e3\u51b3\u8fd9\u4e2a\u4efb\u52a1\u7684\u6a21\u578b\u7c7b\u3002\u6211\u4eec\u4f7f\u7528AutoModelForSequenceClassification \u8fd9\u4e2a\u7c7b\u3002\u548ctokenizer\u76f8\u4f3c\uff0cfrom_pretrained\u65b9\u6cd5\u540c\u6837\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u4e0b\u8f7d\u5e76\u52a0\u8f7d\u6a21\u578b\uff0c\u540c\u65f6\u4e5f\u4f1a\u5bf9\u6a21\u578b\u8fdb\u884c\u7f13\u5b58\uff0c\u5c31\u4e0d\u4f1a\u91cd\u590d\u4e0b\u8f7d\u6a21\u578b\u5566\u3002 from transformers import AutoModelForSequenceClassification , TrainingArguments , Trainer num_labels = 3 if task . startswith ( \"mnli\" ) else 1 if task == \"stsb\" else 2 model = AutoModelForSequenceClassification . from_pretrained ( model_checkpoint , num_labels = num_labels ) \u8f93\u51fa\u4e3a\uff1a Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight'] - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. The warning is telling us we are throwing away some weights (the vocab_transform and vocab_layer_norm layers) and randomly initializing some other (the pre_classifier and classifier layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do. \u7531\u4e8e\u6211\u4eec\u5fae\u8c03\u7684\u4efb\u52a1\u662f\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff0c\u800c\u6211\u4eec\u52a0\u8f7d\u7684\u662f\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u6240\u4ee5\u4f1a\u63d0\u793a\u6211\u4eec\u52a0\u8f7d\u6a21\u578b\u7684\u65f6\u5019\u6254\u6389\u4e86\u4e00\u4e9b\u4e0d\u5339\u914d\u7684\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\uff08\u6bd4\u5982\uff1a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u795e\u7ecf\u7f51\u7edchead\u88ab\u6254\u6389\u4e86\uff0c\u540c\u65f6\u968f\u673a\u521d\u59cb\u5316\u4e86\u6587\u672c\u5206\u7c7b\u7684\u795e\u7ecf\u7f51\u7edchead\uff09\u3002 To instantiate a Trainer , we will need to define two more things. The most important is the TrainingArguments , which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional: \u4e3a\u4e86\u80fd\u591f\u5f97\u5230\u4e00\u4e2aTrainer\u8bad\u7ec3\u5de5\u5177\uff0c\u6211\u4eec\u8fd8\u9700\u89813\u4e2a\u8981\u7d20\uff0c\u5176\u4e2d\u6700\u91cd\u8981\u7684\u662f\u8bad\u7ec3\u7684\u8bbe\u5b9a/\u53c2\u6570 TrainingArguments\u3002\u8fd9\u4e2a\u8bad\u7ec3\u8bbe\u5b9a\u5305\u542b\u4e86\u80fd\u591f\u5b9a\u4e49\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6240\u6709\u5c5e\u6027\u3002 metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\" model_name = model_checkpoint . split ( \"/\" )[ - 1 ] args = TrainingArguments ( \"test-glue\" , evaluation_strategy = \"epoch\" , save_strategy = \"epoch\" , learning_rate = 2e-5 , per_device_train_batch_size = batch_size , per_device_eval_batch_size = batch_size , num_train_epochs = 5 , weight_decay = 0.01 , load_best_model_at_end = True , metric_for_best_model = metric_name , push_to_hub = False , push_to_hub_model_id = f \" { model_name } -finetuned- { task } \" , ) Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the batch_size defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay. Since the best model might not be the one at the end of training, we ask the Trainer to load the best model it saved (according to metric_name ) at the end of training. \u4e0a\u9762evaluation_strategy = \"epoch\"\u53c2\u6570\u544a\u8bc9\u8bad\u7ec3\u4ee3\u7801\uff1a\u6211\u4eec\u6bcf\u4e2aepcoh\u4f1a\u505a\u4e00\u6b21\u9a8c\u8bc1\u8bc4\u4f30\u3002 \u4e0a\u9762batch_size\u5728\u8fd9\u4e2anotebook\u4e4b\u524d\u5b9a\u4e49\u597d\u4e86\u3002 The last two arguments are to setup everything so we can push the model to the Hub at the end of training. Remove the two of them if you didn't follow the installation steps at the top of the notebook, otherwise you can change the value of push_to_hub_model_id to something you would prefer. (\u540e\u9762\u9700\u8981\u8fde\u63a5\u5230hub\u5ba2\u6237\u7aef\uff0c\u592a\u9ebb\u70e6\uff0c\u6240\u4ee5\u5148\u8bbe\u4e3aFalse) The last thing to define for our Trainer is how to compute the metrics from the predictions. We need to define a function for this, which will just use the metric we loaded earlier, the only preprocessing we have to do is to take the argmax of our predicted logits (our just squeeze the last axis in the case of STS-B): \u6700\u540e\uff0c\u7531\u4e8e\u4e0d\u540c\u7684\u4efb\u52a1\u9700\u8981\u4e0d\u540c\u7684\u8bc4\u6d4b\u6307\u6807\uff0c\u6211\u4eec\u5b9a\u4e00\u4e2a\u51fd\u6570\u6765\u6839\u636e\u4efb\u52a1\u540d\u5b57\u5f97\u5230\u8bc4\u4ef7\u65b9\u6cd5: def compute_metrics ( eval_pred ): predictions , labels = eval_pred if task != \"stsb\" : predictions = np . argmax ( predictions , axis = 1 ) else : predictions = predictions [:, 0 ] return metric . compute ( predictions = predictions , references = labels ) Then we just need to pass all of this along with our datasets to the Trainer: validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\" trainer = Trainer ( model , args , train_dataset = encoded_dataset [ \"train\" ], eval_dataset = encoded_dataset [ validation_key ], tokenizer = tokenizer , compute_metrics = compute_metrics ) BUG: ValueError: You must login to the Hugging Face hub on this computer by typing transformers-cli login and entering your credentials to use use_auth_token=True . Alternatively, you can pass your own token as the use_auth_token argument. \u628aargs\u91cc\u9762\u7684\u8be5\u9879\u53c2\u6570\u6539\u4e3aFalse push_to_hub=False, \u3002 We can now finetune our model by just calling the train method: trainer . train () \u8f93\u51fa\u4e3a\uff1a pass We can check with the evaluate method that our Trainer did reload the best model properly (if it was not the last one): trainer . evaluate () \u8f93\u51fa\u4e3a\uff1a pass","title":"\u5fae\u8c03\u6a21\u578b"},{"location":"nlp-transformer-task07/#_5","text":"The Trainer supports hyperparameter search using optuna or Ray Tune. pip install optuna pip install ray [ tune ] During hyperparameter search, the Trainer will run several trainings, so it needs to have the model defined via a function (so it can be reinitialized at each new run) instead of just having it passed. We jsut use the same function as before: \u8d85\u53c2\u641c\u7d22\u65f6\uff0cTrainer\u5c06\u4f1a\u8fd4\u56de\u591a\u4e2a\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u6240\u4ee5\u9700\u8981\u4f20\u5165\u4e00\u4e2a\u5b9a\u4e49\u597d\u7684\u6a21\u578b\u4ece\u800c\u8ba9Trainer\u53ef\u4ee5\u4e0d\u65ad\u91cd\u65b0\u521d\u59cb\u5316\u8be5\u4f20\u5165\u7684\u6a21\u578b\uff1a def model_init (): return AutoModelForSequenceClassification . from_pretrained ( model_checkpoint , num_labels = num_labels ) And we can instantiate our Trainer like before: trainer = Trainer ( model_init = model_init , args = args , train_dataset = encoded_dataset [ \"train\" ], eval_dataset = encoded_dataset [ validation_key ], tokenizer = tokenizer , compute_metrics = compute_metrics ) The method we call this time is hyperparameter_search . Note that it can take a long time to run on the full dataset for some of the tasks. You can try to find some good hyperparameter on a portion of the training dataset by replacing the train_dataset line above by: train_dataset = encoded_dataset[\"train\"].shard(index=1, num_shards=10) for 1/10 th of the dataset. Then you can run a full training on the best hyperparameters picked by the search. \u8c03\u7528\u65b9\u6cd5hyperparameter_search\u3002\u6ce8\u610f\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u53ef\u80fd\u5f88\u4e45\uff0c\u6211\u4eec\u53ef\u4ee5\u5148\u7528\u90e8\u5206\u6570\u636e\u96c6\u8fdb\u884c\u8d85\u53c2\u641c\u7d22\uff0c\u518d\u8fdb\u884c\u5168\u91cf\u8bad\u7ec3\u3002 \u6bd4\u5982\u4f7f\u75281/10\u7684\u6570\u636e\u8fdb\u884c\u641c\u7d22\uff1a best_run = trainer . hyperparameter_search ( n_trials = 10 , direction = \"maximize\" ) The hyperparameter_search method returns a BestRun objects, which contains the value of the objective maximized (by default the sum of all metrics) and the hyperparameters it used for that run. hyperparameter_search\u4f1a\u8fd4\u56de\u6548\u679c\u6700\u597d\u7684\u6a21\u578b\u76f8\u5173\u7684\u53c2\u6570\uff1a >>> best_run To reproduce the best training, just set the hyperparameters in your TrainingArgument before creating a Trainer: \u5c06Trainner\u8bbe\u7f6e\u4e3a\u641c\u7d22\u5230\u7684\u6700\u597d\u53c2\u6570\uff0c\u8fdb\u884c\u8bad\u7ec3\uff1a for n , v in best_run . hyperparameters . items (): setattr ( trainer . args , n , v ) trainer . train ()","title":"\u8d85\u53c2\u641c\u7d22"},{"location":"nlp-transformer-task07/#_6","text":"HuggingFace/transfomers/BERT https://huggingface.co/transformers/model_doc/bert.html# \u57fa\u4e8etransformers\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u5165\u95e8--\u5728\u7ebf\u9605\u8bfb https://datawhalechina.github.io/learn-nlp-with-transformers/#/","title":"\u53c2\u8003\u8d44\u6599"},{"location":"open-source-projects/","text":"Open Source Projects by Timelines \u00b6 (\u4ee52022\u5e74\u4e3a\u51c6) \u6708\u4efd \u9879\u76ee \u91cd\u8981\u65f6\u95f4\u70b9\uff08\u4ee5\u201c\u7533\u8bf7\u5f00\u59cb\u65f6\u95f4\u201d\u6392\u5e8f\uff09 1\u6708 \u6bd4\u7279\u4e4b\u590f\uff08Summer of Bitcoin\uff09 \u7533\u8bf7\u65f6\u95f4\uff1aJanuary 1 - March 5 \u6280\u672f\u8bc4\u4f30\uff1a January 1- March 15 \u7533\u8bf7\u4e66\u8bc4\u4f30\uff1aFeb 15 - April 19 \u5f55\u53d6\u7ed3\u679c\u901a\u77e5\uff1aMay 1 2\u6708 Linux\u57fa\u91d1\u4f1a\u5f00\u6e90\u5b9e\u4e60\uff08\u6625\u5b63\uff09 \u6625\u5b63\u5b9e\u4e60\u671f: March 1 st - May 31 st \u9879\u76ee\u4e0a\u7ebf\u65f6\u95f4: 1\u6708\u4e2d\u65ec \u9879\u76ee\u7533\u8bf7\u65f6\u95f4\uff1a\u5b9e\u4e60\u671f\u5f00\u59cb\u524d6\u5468\u5de6\u53f3 \u7533\u8bf7\u8bc4\u4f30\u548c\u5f55\u53d6: \u5b9e\u4e60\u671f\u5f00\u59cb\u524d2\u5468\u5de6\u53f3 Outreachy\u590f\u5b63\u5b9e\u4e60 \u7533\u8bf7\u65f6\u95f4\uff1a2\u6708-3\u6708 \u5b9e\u4e60\u65f6\u95f4\uff1a5\u6708-8\u6708 GirlScript Summer of Code\uff08GSSOC\uff09 \u9879\u76ee\u65f6\u95f4\uff1a3\u67081\u65e5-5\u670831\u65e5 4\u6708 \u8c37\u6b4c\u7f16\u7a0b\u4e4b\u590f\uff08Google Summer of Code\uff09 \u793e\u533a\u9879\u76ee\u516c\u5e03\uff1aMarch 7 \u63d0\u4ea4\u9879\u76ee\u7533\u8bf7\uff1aApril 4 - April 19 5\u6708 Linux\u57fa\u91d1\u4f1a\u5f00\u6e90\u5b9e\u4e60\uff08\u590f\u5b63\uff09 \u590f\u5b63\u5b9e\u4e60\u671f: June 1 st - August 31 st \u9879\u76ee\u4e0a\u7ebf\u65f6\u95f4\uff1a4\u6708\u4e2d\u65ec \u9879\u76ee\u7533\u8bf7\u65f6\u95f4\uff1a\u5b9e\u4e60\u671f\u5f00\u59cb\u524d6\u5468\u5de6\u53f3 \u7533\u8bf7\u8bc4\u4f30\u548c\u5f55\u53d6: \u5b9e\u4e60\u671f\u5f00\u59cb\u524d2\u5468\u5de6\u53f3 \u8c37\u6b4cseason of docs \u793e\u533a\u9879\u76ee\u516c\u5e03\uff1aApril 14 \u7533\u8bf7\u622a\u6b62\u65e5\u671f\uff1a May 16 \u4e2d\u79d1\u9662\u5f00\u6e90\u4e4b\u590f\uff08OSPP\uff09 \u793e\u533a\u4e0a\u7ebf\u9879\u76ee\u65f6\u95f4\uff1a4\u670821\u65e5-5\u670810\u65e5 \u5b66\u751f\u63d0\u4ea4\u9879\u76ee\u7533\u8bf7\u4e66\uff1a05 \u6708 21 \u65e5 - 06 \u6708 04 \u65e5 GitLink\u7f16\u7a0b\u590f\u4ee4\u8425\uff08GLCC\uff09 \u516c\u793a\u793e\u533a\u9879\u76ee\u540d\u5355\uff1a5.20-5.25 \u5b66\u751f\u62a5\u540d\uff0c\u63d0\u4ea4Proposal\uff1a 5.26-6.24 \u817e\u8baf\u7280\u725b\u9e1f\u5f00\u6e90\u57f9\u8bad\u8ba1\u5212 \u62a5\u540d\u65f6\u95f4\uff1a5\u670830\u65e5-6\u670830\u65e5 \u963f\u91cc\u5df4\u5df4\u7f16\u7a0b\u4e4b\u590f\uff08ASOC\uff09 \u62a5\u540d\u65f6\u95f4\uff1a5\u670830\u65e5-6\u670826\u65e5 8\u6708 Linux\u57fa\u91d1\u4f1a\u5f00\u6e90\u5b9e\u4e60\uff08\u79cb\u5b63\uff09 \u79cb\u5b63\u5b9e\u4e60\u671f: September 1 st - Nov 30 th \u9879\u76ee\u4e0a\u7ebf\u65f6\u95f4: 7\u6708\u4e2d\u65ec \u9879\u76ee\u7533\u8bf7\u65f6\u95f4\uff1a\u5b9e\u4e60\u671f\u5f00\u59cb\u524d6\u5468\u5de6\u53f3 \u7533\u8bf7\u8bc4\u4f30\u548c\u5f55\u53d6: \u5b9e\u4e60\u671f\u5f00\u59cb\u524d2\u5468\u5de6\u53f3 GirlScript Winter of Contributing\uff08GWOC\uff09 \u7533\u8bf7\u65f6\u95f4\uff1a8\u67081\u65e5-8\u670815\u65e5 9\u6708 Outreachy\u51ac\u5b63\u5b9e\u4e60 \u7533\u8bf7\u65f6\u95f4\uff1a9\u6708-10\u6708 \u5b9e\u4e60\u65f6\u95f4\uff1a12\u6708-\u4e0b\u4e00\u5e743\u6708","title":"Open Source Projects"},{"location":"open-source-projects/#open-source-projects-by-timelines","text":"(\u4ee52022\u5e74\u4e3a\u51c6) \u6708\u4efd \u9879\u76ee \u91cd\u8981\u65f6\u95f4\u70b9\uff08\u4ee5\u201c\u7533\u8bf7\u5f00\u59cb\u65f6\u95f4\u201d\u6392\u5e8f\uff09 1\u6708 \u6bd4\u7279\u4e4b\u590f\uff08Summer of Bitcoin\uff09 \u7533\u8bf7\u65f6\u95f4\uff1aJanuary 1 - March 5 \u6280\u672f\u8bc4\u4f30\uff1a January 1- March 15 \u7533\u8bf7\u4e66\u8bc4\u4f30\uff1aFeb 15 - April 19 \u5f55\u53d6\u7ed3\u679c\u901a\u77e5\uff1aMay 1 2\u6708 Linux\u57fa\u91d1\u4f1a\u5f00\u6e90\u5b9e\u4e60\uff08\u6625\u5b63\uff09 \u6625\u5b63\u5b9e\u4e60\u671f: March 1 st - May 31 st \u9879\u76ee\u4e0a\u7ebf\u65f6\u95f4: 1\u6708\u4e2d\u65ec \u9879\u76ee\u7533\u8bf7\u65f6\u95f4\uff1a\u5b9e\u4e60\u671f\u5f00\u59cb\u524d6\u5468\u5de6\u53f3 \u7533\u8bf7\u8bc4\u4f30\u548c\u5f55\u53d6: \u5b9e\u4e60\u671f\u5f00\u59cb\u524d2\u5468\u5de6\u53f3 Outreachy\u590f\u5b63\u5b9e\u4e60 \u7533\u8bf7\u65f6\u95f4\uff1a2\u6708-3\u6708 \u5b9e\u4e60\u65f6\u95f4\uff1a5\u6708-8\u6708 GirlScript Summer of Code\uff08GSSOC\uff09 \u9879\u76ee\u65f6\u95f4\uff1a3\u67081\u65e5-5\u670831\u65e5 4\u6708 \u8c37\u6b4c\u7f16\u7a0b\u4e4b\u590f\uff08Google Summer of Code\uff09 \u793e\u533a\u9879\u76ee\u516c\u5e03\uff1aMarch 7 \u63d0\u4ea4\u9879\u76ee\u7533\u8bf7\uff1aApril 4 - April 19 5\u6708 Linux\u57fa\u91d1\u4f1a\u5f00\u6e90\u5b9e\u4e60\uff08\u590f\u5b63\uff09 \u590f\u5b63\u5b9e\u4e60\u671f: June 1 st - August 31 st \u9879\u76ee\u4e0a\u7ebf\u65f6\u95f4\uff1a4\u6708\u4e2d\u65ec \u9879\u76ee\u7533\u8bf7\u65f6\u95f4\uff1a\u5b9e\u4e60\u671f\u5f00\u59cb\u524d6\u5468\u5de6\u53f3 \u7533\u8bf7\u8bc4\u4f30\u548c\u5f55\u53d6: \u5b9e\u4e60\u671f\u5f00\u59cb\u524d2\u5468\u5de6\u53f3 \u8c37\u6b4cseason of docs \u793e\u533a\u9879\u76ee\u516c\u5e03\uff1aApril 14 \u7533\u8bf7\u622a\u6b62\u65e5\u671f\uff1a May 16 \u4e2d\u79d1\u9662\u5f00\u6e90\u4e4b\u590f\uff08OSPP\uff09 \u793e\u533a\u4e0a\u7ebf\u9879\u76ee\u65f6\u95f4\uff1a4\u670821\u65e5-5\u670810\u65e5 \u5b66\u751f\u63d0\u4ea4\u9879\u76ee\u7533\u8bf7\u4e66\uff1a05 \u6708 21 \u65e5 - 06 \u6708 04 \u65e5 GitLink\u7f16\u7a0b\u590f\u4ee4\u8425\uff08GLCC\uff09 \u516c\u793a\u793e\u533a\u9879\u76ee\u540d\u5355\uff1a5.20-5.25 \u5b66\u751f\u62a5\u540d\uff0c\u63d0\u4ea4Proposal\uff1a 5.26-6.24 \u817e\u8baf\u7280\u725b\u9e1f\u5f00\u6e90\u57f9\u8bad\u8ba1\u5212 \u62a5\u540d\u65f6\u95f4\uff1a5\u670830\u65e5-6\u670830\u65e5 \u963f\u91cc\u5df4\u5df4\u7f16\u7a0b\u4e4b\u590f\uff08ASOC\uff09 \u62a5\u540d\u65f6\u95f4\uff1a5\u670830\u65e5-6\u670826\u65e5 8\u6708 Linux\u57fa\u91d1\u4f1a\u5f00\u6e90\u5b9e\u4e60\uff08\u79cb\u5b63\uff09 \u79cb\u5b63\u5b9e\u4e60\u671f: September 1 st - Nov 30 th \u9879\u76ee\u4e0a\u7ebf\u65f6\u95f4: 7\u6708\u4e2d\u65ec \u9879\u76ee\u7533\u8bf7\u65f6\u95f4\uff1a\u5b9e\u4e60\u671f\u5f00\u59cb\u524d6\u5468\u5de6\u53f3 \u7533\u8bf7\u8bc4\u4f30\u548c\u5f55\u53d6: \u5b9e\u4e60\u671f\u5f00\u59cb\u524d2\u5468\u5de6\u53f3 GirlScript Winter of Contributing\uff08GWOC\uff09 \u7533\u8bf7\u65f6\u95f4\uff1a8\u67081\u65e5-8\u670815\u65e5 9\u6708 Outreachy\u51ac\u5b63\u5b9e\u4e60 \u7533\u8bf7\u65f6\u95f4\uff1a9\u6708-10\u6708 \u5b9e\u4e60\u65f6\u95f4\uff1a12\u6708-\u4e0b\u4e00\u5e743\u6708","title":"Open Source Projects by Timelines"},{"location":"post-types-of-paper/","text":"Types of CS Paper \u00b6 from the Internet. Types of Machine Learning Paper \u00b6 Types of NLP Paper \u00b6 Types of Computer Vision Paper \u00b6 Types of AI Ethics Paper \u00b6 Types of Distributed Systems Paper \u00b6 Types of Database Paper \u00b6 Types of Robotics Paper \u00b6 Types of Privacy Paper \u00b6","title":"Types of CS Paper"},{"location":"post-types-of-paper/#types-of-cs-paper","text":"from the Internet.","title":"Types of CS Paper"},{"location":"post-types-of-paper/#types-of-machine-learning-paper","text":"","title":"Types of Machine Learning Paper"},{"location":"post-types-of-paper/#types-of-nlp-paper","text":"","title":"Types of NLP Paper"},{"location":"post-types-of-paper/#types-of-computer-vision-paper","text":"","title":"Types of Computer Vision Paper"},{"location":"post-types-of-paper/#types-of-ai-ethics-paper","text":"","title":"Types of AI Ethics Paper"},{"location":"post-types-of-paper/#types-of-distributed-systems-paper","text":"","title":"Types of Distributed Systems Paper"},{"location":"post-types-of-paper/#types-of-database-paper","text":"","title":"Types of Database Paper"},{"location":"post-types-of-paper/#types-of-robotics-paper","text":"","title":"Types of Robotics Paper"},{"location":"post-types-of-paper/#types-of-privacy-paper","text":"","title":"Types of Privacy Paper"},{"location":"pytorch-chap01-02/","text":"Chapter01-02 PyTorch\u7684\u7b80\u4ecb\u548c\u5b89\u88c5\u3001PyTorch\u57fa\u7840\u77e5\u8bc6 \u00b6 \u7b2c\u4e00\u7ae0 PyTorch\u7684\u7b80\u4ecb\u548c\u5b89\u88c5 \u00b6 PyTorch\u7b80\u4ecb \u00b6 PyTorch\u662f\u7531Facebook\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u5c0f\u7ec4\u5f00\u53d1\u7684\u4e00\u79cd\u57fa\u4e8eLua\u7f16\u5199\u7684Torch\u5e93\u7684Python\u5b9e\u73b0\u7684\u6df1\u5ea6\u5b66\u4e60\u5e93\uff0c\u76ee\u524d\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\uff0c\u800c\u968f\u7740Caffe2\u9879\u76ee\u5e76\u5165Pytorch\uff0c Pytorch\u5f00\u59cb\u5f71\u54cd\u5230TensorFlow\u5728\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u6846\u67b6\u9886\u57df\u7684\u5730\u4f4d\u3002\u603b\u7684\u6765\u8bf4\uff0cPyTorch\u662f\u5f53\u524d\u96be\u5f97\u7684\u7b80\u6d01\u4f18\u96c5\u4e14\u9ad8\u6548\u5feb\u901f\u7684\u6846\u67b6\u3002\u56e0\u6b64\u672c\u8bfe\u7a0b\u6211\u4eec\u9009\u62e9\u4e86PyTorch\u6765\u8fdb\u884c\u5f00\u6e90\u5b66\u4e60\u3002 PyTorch\u7684\u5b89\u88c5 \u00b6 PyTorch\u5b98\u7f51\uff1a https://pytorch.org/ PyTorch\u7684\u53d1\u5c55\u548c\u4f18\u52bf \u00b6 \u201cAll in Pytorch\u201d. PyTorch VS TensorFlow \u00b6 \u7b2c\u4e8c\u7ae0 PyTorch\u7684\u57fa\u7840\u77e5\u8bc6 \u00b6 Tensor/\u5f20\u91cf \u00b6 \u5f20\u91cf\u662f\u57fa\u4e8e\u5411\u91cf\u548c\u77e9\u9635\u7684\u63a8\u5e7f\uff0c\u6bd4\u5982\u6211\u4eec\u53ef\u4ee5\u5c06\u6807\u91cf\u89c6\u4e3a\u96f6\u9636\u5f20\u91cf\uff0c\u77e2\u91cf\u53ef\u4ee5\u89c6\u4e3a\u4e00\u9636\u5f20\u91cf\uff0c\u77e9\u9635\u5c31\u662f\u4e8c\u9636\u5f20\u91cf\u3002 - 0\u7ef4\u5f20\u91cf/\u6807\u91cf \u6807\u91cf\u662f1\u4e2a\u6570\u5b57 - 1\u7ef4\u5f20\u91cf/\u5411\u91cf 1\u7ef4\u5f20\u91cf\u79f0\u4e3a\u201c\u5411\u91cf\u201d - 2\u7ef4\u5f20\u91cf 2\u7ef4\u5f20\u91cf\u79f0\u4e3a\u201c\u77e9\u9635\u201d - 3\u7ef4\u5f20\u91cf \u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3001\u80a1\u4ef7\u3001\u6587\u672c\u6570\u636e\u3001\u5f69\u8272\u56fe\u7247(RGB) - 4\u7ef4=\u56fe\u50cf - 5\u7ef4=\u89c6\u9891 \u5728PyTorch\u4e2d\uff0c torch.Tensor \u662f\u5b58\u50a8\u548c\u53d8\u6362\u6570\u636e\u7684\u4e3b\u8981\u5de5\u5177\u3002 tensor-\u6784\u9020 \u00b6 \u521b\u5efa\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\u7684\u77e9\u9635\uff1a x = torch . rand ( 4 , 3 ) # \u6784\u9020\u5f20\u91cf print ( x . size ()) # \u83b7\u53d6\u7ef4\u5ea6\u4fe1\u606f print ( x . shape ) # \u83b7\u53d6\u7ef4\u5ea6\u4fe1\u606f \u8fd8\u6709\u4e00\u4e9b\u5e38\u89c1\u7684\u6784\u9020Tensor\u7684\u51fd\u6570\uff1a PyTorch\u4e2d\u7684Tensor\u652f\u6301\u8d85\u8fc7\u4e00\u767e\u79cd\u64cd\u4f5c\uff0c\u5305\u62ec\u8f6c\u7f6e\u3001\u7d22\u5f15\u3001\u5207\u7247\u3001\u6570\u5b66\u8fd0\u7b97\u3001\u7ebf\u6027\u4ee3\u6570\u3001\u968f\u673a\u6570\u7b49\u7b49\uff0c\u53ef\u53c2\u8003\u5b98\u65b9\u6587\u6863\u3002 tensor-squeeze \u589e\u52a0/\u5220\u9664\u4e00\u4e2a\u7ef4\u5ea6 \u00b6 tensor-transpose \u8f6c\u7f6e \u00b6 tensor-cat concatenate\u591a\u4e2atensor \u00b6 \u81ea\u52a8\u6c42\u5bfc/\u81ea\u52a8\u5fae\u5206 \u00b6 PyTorch\u4e2d\uff0c\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7684\u6838\u5fc3\u662fautograd\u5305\u3002autograd\u5305\u4e3a\u5f20\u91cf\u4e0a\u7684\u6240\u6709\u64cd\u4f5c\u63d0\u4f9b\u4e86\u81ea\u52a8\u6c42\u5bfc\u673a\u5236\u3002 How to Calculate Gradient \u00b6 >>> x = torch . tensor ([[ 1. , 0. ], [ - 1. , 1. ]], requires_grad = True ) >>> x tensor ([[ 1. , 0. ], [ - 1. , 1. ]], requires_grad = True ) >>> z = x . pow ( 2 ) >>> z tensor ([[ 1. , 0. ], [ 1. , 1. ]], grad_fn =< PowBackward0 > ) >>> z = z . sum () >>> z tensor ( 3. , grad_fn =< SumBackward0 > ) >>> z . backward () >>> z tensor ( 3. , grad_fn =< SumBackward0 > ) >>> x . grad tensor ([[ 2. , 0. ], [ - 2. , 2. ]]) \u5e76\u884c\u8ba1\u7b97\u7b80\u4ecb \u00b6 \u5728\u5229\u7528PyTorch\u505a\u6df1\u5ea6\u5b66\u4e60\u7684\u8fc7\u7a0b\u4e2d\uff0c\u53ef\u80fd\u4f1a\u9047\u5230\u6570\u636e\u91cf\u8f83\u5927\u65e0\u6cd5\u5728\u5355\u5757GPU\u4e0a\u5b8c\u6210\uff0c\u6216\u8005\u9700\u8981\u63d0\u5347\u8ba1\u7b97\u901f\u5ea6\u7684\u573a\u666f\uff0c\u8fd9\u65f6\u5c31\u9700\u8981\u7528\u5230\u5e76\u884c\u8ba1\u7b97\u3002 GPU\u7684\u51fa\u73b0\u8ba9\u6211\u4eec\u53ef\u4ee5\u8bad\u7ec3\u7684\u66f4\u5feb\uff0c\u66f4\u597d\u3002PyTorch\u53ef\u4ee5\u5728\u7f16\u5199\u5b8c\u6a21\u578b\u4e4b\u540e\uff0c\u8ba9\u591a\u4e2aGPU\u6765\u53c2\u4e0e\u8bad\u7ec3\u3002 CUDA \u662f\u6211\u4eec\u4f7f\u7528GPU\u7684\u63d0\u4f9b\u5546\u2014\u2014NVIDIA\u63d0\u4f9b\u7684GPU\u5e76\u884c\u8ba1\u7b97\u6846\u67b6\u3002\u5bf9\u4e8eGPU\u672c\u8eab\u7684\u7f16\u7a0b\uff0c\u4f7f\u7528\u7684\u662f CUDA \u8bed\u8a00\u6765\u5b9e\u73b0\u7684\u3002\u4f46\u662f\uff0c\u5728\u6211\u4eec\u4f7f\u7528PyTorch\u7f16\u5199\u6df1\u5ea6\u5b66\u4e60\u4ee3\u7801\u65f6\uff0c\u4f7f\u7528\u7684 CUDA \u53c8\u662f\u53e6\u4e00\u4e2a\u610f\u601d\u3002\u5728PyTorch\u4f7f\u7528 CUDA \u8868\u793a\u8981\u5f00\u59cb\u8981\u6c42\u6211\u4eec\u7684\u6a21\u578b\u6216\u8005\u6570\u636e\u5f00\u59cb\u4f7f\u7528GPU\u4e86\u3002 \u5728\u7f16\u5199\u7a0b\u5e8f\u4e2d\uff0c\u5f53\u6211\u4eec\u4f7f\u7528\u4e86 cuda() \u65f6\uff0c\u5176\u529f\u80fd\u662f\u8ba9\u6211\u4eec\u7684\u6a21\u578b\u6216\u8005\u6570\u636e\u8fc1\u79fb\u5230GPU\u5f53\u4e2d\uff0c\u901a\u8fc7GPU\u5f00\u59cb\u8ba1\u7b97\u3002 \u4e0d\u540c\u7684\u6570\u636e\u5206\u5e03\u5230\u4e0d\u540c\u7684\u8bbe\u5907\u4e2d\uff0c\u6267\u884c\u76f8\u540c\u7684\u4efb\u52a1(Data parallelism): \u53c2\u8003\u8d44\u6599 \u00b6 Datawhale\u5f00\u6e90\u9879\u76ee\uff1a\u6df1\u5165\u6d45\u51faPyTorch https://github.com/datawhalechina/thorough-pytorch/ \u674e\u5b8f\u6bc5\u673a\u5668\u5b66\u4e602021\u6625-PyTorch Tutorial https://www.bilibili.com/video/BV1Wv411h7kN?p=5 What is a gpu and do you need one in deep learning https://towardsdatascience.com/what-is-a-gpu-and-do-you-need-one-in-deep-learning-718b9597aa0d \u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60pytorch\u7248 https://zh-v2.d2l.ai/chapter_preface/index.html","title":"Chapter01-02 PyTorch\u7684\u7b80\u4ecb\u548c\u5b89\u88c5\u3001PyTorch\u57fa\u7840\u77e5\u8bc6"},{"location":"pytorch-chap01-02/#chapter01-02-pytorchpytorch","text":"","title":"Chapter01-02 PyTorch\u7684\u7b80\u4ecb\u548c\u5b89\u88c5\u3001PyTorch\u57fa\u7840\u77e5\u8bc6"},{"location":"pytorch-chap01-02/#pytorch","text":"","title":"\u7b2c\u4e00\u7ae0 PyTorch\u7684\u7b80\u4ecb\u548c\u5b89\u88c5"},{"location":"pytorch-chap01-02/#pytorch_1","text":"PyTorch\u662f\u7531Facebook\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u5c0f\u7ec4\u5f00\u53d1\u7684\u4e00\u79cd\u57fa\u4e8eLua\u7f16\u5199\u7684Torch\u5e93\u7684Python\u5b9e\u73b0\u7684\u6df1\u5ea6\u5b66\u4e60\u5e93\uff0c\u76ee\u524d\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\uff0c\u800c\u968f\u7740Caffe2\u9879\u76ee\u5e76\u5165Pytorch\uff0c Pytorch\u5f00\u59cb\u5f71\u54cd\u5230TensorFlow\u5728\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u6846\u67b6\u9886\u57df\u7684\u5730\u4f4d\u3002\u603b\u7684\u6765\u8bf4\uff0cPyTorch\u662f\u5f53\u524d\u96be\u5f97\u7684\u7b80\u6d01\u4f18\u96c5\u4e14\u9ad8\u6548\u5feb\u901f\u7684\u6846\u67b6\u3002\u56e0\u6b64\u672c\u8bfe\u7a0b\u6211\u4eec\u9009\u62e9\u4e86PyTorch\u6765\u8fdb\u884c\u5f00\u6e90\u5b66\u4e60\u3002","title":"PyTorch\u7b80\u4ecb"},{"location":"pytorch-chap01-02/#pytorch_2","text":"PyTorch\u5b98\u7f51\uff1a https://pytorch.org/","title":"PyTorch\u7684\u5b89\u88c5"},{"location":"pytorch-chap01-02/#pytorch_3","text":"\u201cAll in Pytorch\u201d.","title":"PyTorch\u7684\u53d1\u5c55\u548c\u4f18\u52bf"},{"location":"pytorch-chap01-02/#pytorch-vs-tensorflow","text":"","title":"PyTorch VS TensorFlow"},{"location":"pytorch-chap01-02/#pytorch_4","text":"","title":"\u7b2c\u4e8c\u7ae0 PyTorch\u7684\u57fa\u7840\u77e5\u8bc6"},{"location":"pytorch-chap01-02/#tensor","text":"\u5f20\u91cf\u662f\u57fa\u4e8e\u5411\u91cf\u548c\u77e9\u9635\u7684\u63a8\u5e7f\uff0c\u6bd4\u5982\u6211\u4eec\u53ef\u4ee5\u5c06\u6807\u91cf\u89c6\u4e3a\u96f6\u9636\u5f20\u91cf\uff0c\u77e2\u91cf\u53ef\u4ee5\u89c6\u4e3a\u4e00\u9636\u5f20\u91cf\uff0c\u77e9\u9635\u5c31\u662f\u4e8c\u9636\u5f20\u91cf\u3002 - 0\u7ef4\u5f20\u91cf/\u6807\u91cf \u6807\u91cf\u662f1\u4e2a\u6570\u5b57 - 1\u7ef4\u5f20\u91cf/\u5411\u91cf 1\u7ef4\u5f20\u91cf\u79f0\u4e3a\u201c\u5411\u91cf\u201d - 2\u7ef4\u5f20\u91cf 2\u7ef4\u5f20\u91cf\u79f0\u4e3a\u201c\u77e9\u9635\u201d - 3\u7ef4\u5f20\u91cf \u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3001\u80a1\u4ef7\u3001\u6587\u672c\u6570\u636e\u3001\u5f69\u8272\u56fe\u7247(RGB) - 4\u7ef4=\u56fe\u50cf - 5\u7ef4=\u89c6\u9891 \u5728PyTorch\u4e2d\uff0c torch.Tensor \u662f\u5b58\u50a8\u548c\u53d8\u6362\u6570\u636e\u7684\u4e3b\u8981\u5de5\u5177\u3002","title":"Tensor/\u5f20\u91cf"},{"location":"pytorch-chap01-02/#tensor-","text":"\u521b\u5efa\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\u7684\u77e9\u9635\uff1a x = torch . rand ( 4 , 3 ) # \u6784\u9020\u5f20\u91cf print ( x . size ()) # \u83b7\u53d6\u7ef4\u5ea6\u4fe1\u606f print ( x . shape ) # \u83b7\u53d6\u7ef4\u5ea6\u4fe1\u606f \u8fd8\u6709\u4e00\u4e9b\u5e38\u89c1\u7684\u6784\u9020Tensor\u7684\u51fd\u6570\uff1a PyTorch\u4e2d\u7684Tensor\u652f\u6301\u8d85\u8fc7\u4e00\u767e\u79cd\u64cd\u4f5c\uff0c\u5305\u62ec\u8f6c\u7f6e\u3001\u7d22\u5f15\u3001\u5207\u7247\u3001\u6570\u5b66\u8fd0\u7b97\u3001\u7ebf\u6027\u4ee3\u6570\u3001\u968f\u673a\u6570\u7b49\u7b49\uff0c\u53ef\u53c2\u8003\u5b98\u65b9\u6587\u6863\u3002","title":"tensor-\u6784\u9020"},{"location":"pytorch-chap01-02/#tensor-squeeze","text":"","title":"tensor-squeeze \u589e\u52a0/\u5220\u9664\u4e00\u4e2a\u7ef4\u5ea6"},{"location":"pytorch-chap01-02/#tensor-transpose","text":"","title":"tensor-transpose \u8f6c\u7f6e"},{"location":"pytorch-chap01-02/#tensor-cat-concatenatetensor","text":"","title":"tensor-cat concatenate\u591a\u4e2atensor"},{"location":"pytorch-chap01-02/#_1","text":"PyTorch\u4e2d\uff0c\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7684\u6838\u5fc3\u662fautograd\u5305\u3002autograd\u5305\u4e3a\u5f20\u91cf\u4e0a\u7684\u6240\u6709\u64cd\u4f5c\u63d0\u4f9b\u4e86\u81ea\u52a8\u6c42\u5bfc\u673a\u5236\u3002","title":"\u81ea\u52a8\u6c42\u5bfc/\u81ea\u52a8\u5fae\u5206"},{"location":"pytorch-chap01-02/#how-to-calculate-gradient","text":">>> x = torch . tensor ([[ 1. , 0. ], [ - 1. , 1. ]], requires_grad = True ) >>> x tensor ([[ 1. , 0. ], [ - 1. , 1. ]], requires_grad = True ) >>> z = x . pow ( 2 ) >>> z tensor ([[ 1. , 0. ], [ 1. , 1. ]], grad_fn =< PowBackward0 > ) >>> z = z . sum () >>> z tensor ( 3. , grad_fn =< SumBackward0 > ) >>> z . backward () >>> z tensor ( 3. , grad_fn =< SumBackward0 > ) >>> x . grad tensor ([[ 2. , 0. ], [ - 2. , 2. ]])","title":"How to Calculate Gradient"},{"location":"pytorch-chap01-02/#_2","text":"\u5728\u5229\u7528PyTorch\u505a\u6df1\u5ea6\u5b66\u4e60\u7684\u8fc7\u7a0b\u4e2d\uff0c\u53ef\u80fd\u4f1a\u9047\u5230\u6570\u636e\u91cf\u8f83\u5927\u65e0\u6cd5\u5728\u5355\u5757GPU\u4e0a\u5b8c\u6210\uff0c\u6216\u8005\u9700\u8981\u63d0\u5347\u8ba1\u7b97\u901f\u5ea6\u7684\u573a\u666f\uff0c\u8fd9\u65f6\u5c31\u9700\u8981\u7528\u5230\u5e76\u884c\u8ba1\u7b97\u3002 GPU\u7684\u51fa\u73b0\u8ba9\u6211\u4eec\u53ef\u4ee5\u8bad\u7ec3\u7684\u66f4\u5feb\uff0c\u66f4\u597d\u3002PyTorch\u53ef\u4ee5\u5728\u7f16\u5199\u5b8c\u6a21\u578b\u4e4b\u540e\uff0c\u8ba9\u591a\u4e2aGPU\u6765\u53c2\u4e0e\u8bad\u7ec3\u3002 CUDA \u662f\u6211\u4eec\u4f7f\u7528GPU\u7684\u63d0\u4f9b\u5546\u2014\u2014NVIDIA\u63d0\u4f9b\u7684GPU\u5e76\u884c\u8ba1\u7b97\u6846\u67b6\u3002\u5bf9\u4e8eGPU\u672c\u8eab\u7684\u7f16\u7a0b\uff0c\u4f7f\u7528\u7684\u662f CUDA \u8bed\u8a00\u6765\u5b9e\u73b0\u7684\u3002\u4f46\u662f\uff0c\u5728\u6211\u4eec\u4f7f\u7528PyTorch\u7f16\u5199\u6df1\u5ea6\u5b66\u4e60\u4ee3\u7801\u65f6\uff0c\u4f7f\u7528\u7684 CUDA \u53c8\u662f\u53e6\u4e00\u4e2a\u610f\u601d\u3002\u5728PyTorch\u4f7f\u7528 CUDA \u8868\u793a\u8981\u5f00\u59cb\u8981\u6c42\u6211\u4eec\u7684\u6a21\u578b\u6216\u8005\u6570\u636e\u5f00\u59cb\u4f7f\u7528GPU\u4e86\u3002 \u5728\u7f16\u5199\u7a0b\u5e8f\u4e2d\uff0c\u5f53\u6211\u4eec\u4f7f\u7528\u4e86 cuda() \u65f6\uff0c\u5176\u529f\u80fd\u662f\u8ba9\u6211\u4eec\u7684\u6a21\u578b\u6216\u8005\u6570\u636e\u8fc1\u79fb\u5230GPU\u5f53\u4e2d\uff0c\u901a\u8fc7GPU\u5f00\u59cb\u8ba1\u7b97\u3002 \u4e0d\u540c\u7684\u6570\u636e\u5206\u5e03\u5230\u4e0d\u540c\u7684\u8bbe\u5907\u4e2d\uff0c\u6267\u884c\u76f8\u540c\u7684\u4efb\u52a1(Data parallelism):","title":"\u5e76\u884c\u8ba1\u7b97\u7b80\u4ecb"},{"location":"pytorch-chap01-02/#_3","text":"Datawhale\u5f00\u6e90\u9879\u76ee\uff1a\u6df1\u5165\u6d45\u51faPyTorch https://github.com/datawhalechina/thorough-pytorch/ \u674e\u5b8f\u6bc5\u673a\u5668\u5b66\u4e602021\u6625-PyTorch Tutorial https://www.bilibili.com/video/BV1Wv411h7kN?p=5 What is a gpu and do you need one in deep learning https://towardsdatascience.com/what-is-a-gpu-and-do-you-need-one-in-deep-learning-718b9597aa0d \u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60pytorch\u7248 https://zh-v2.d2l.ai/chapter_preface/index.html","title":"\u53c2\u8003\u8d44\u6599"},{"location":"pytorch-chap03/","text":"\u7b2c\u4e09\u7ae0 PyTorch\u7684\u4e3b\u8981\u7ec4\u6210\u6a21\u5757 \u00b6 \u5b8c\u6210\u6df1\u5ea6\u5b66\u4e60\u7684\u5fc5\u8981\u90e8\u5206 \u00b6 \u673a\u5668\u5b66\u4e60\uff1a \u6570\u636e\u9884\u5904\u7406\uff08\u6570\u636e\u683c\u5f0f\u3001\u6570\u636e\u8f6c\u6362\u3001\u5212\u5206\u6570\u636e\u96c6\uff09 \u9009\u62e9\u6a21\u578b\uff0c\u8bbe\u5b9a\u635f\u5931\u548c\u4f18\u5316\u51fd\u6570\uff0c\u8bbe\u7f6e\u8d85\u53c2\u6570 \u8bad\u7ec3\u6a21\u578b\uff0c\u62df\u5408\u8bad\u7ec3\u96c6 \u8bc4\u4f30\u6a21\u578b\uff0c\u5728\u5e76\u5728\u9a8c\u8bc1\u96c6/\u6d4b\u8bd5\u96c6\u4e0a\u8ba1\u7b97\u6a21\u578b\u8868\u73b0 \u6df1\u5ea6\u5b66\u4e60\u7684\u6ce8\u610f\u4e8b\u9879\uff1a \u6570\u636e\u9884\u5904\u7406\uff08\u6570\u636e\u52a0\u8f7d\u3001\u6279\u5904\u7406\uff09 \u9010\u5c42\u642d\u5efa\u6a21\u578b\uff0c\u7ec4\u88c5\u4e0d\u540c\u6a21\u5757 GPU\u7684\u914d\u7f6e\u548c\u64cd\u4f5c \u57fa\u672c\u914d\u7f6e \u00b6 \u5bfc\u5165\u5fc5\u987b\u7684\u5305\uff1a import os import numpy as np import torch import torch.nn as nn from torch.utils.data import Dataset , DataLoader import torch.optim as optimizer \u8d85\u53c2\u6570\u8bbe\u7f6e\uff1a batch_size = 16 # batch size lr = 1e-4 # \u521d\u59cb\u5b66\u4e60\u7387 max_epochs = 100 # \u8bad\u7ec3\u6b21\u6570 GPU\u7684\u8bbe\u7f6e\uff1a # \u65b9\u6848\u4e00\uff1a\u4f7f\u7528os.environ\uff0c\u8fd9\u79cd\u60c5\u51b5\u5982\u679c\u4f7f\u7528GPU\u4e0d\u9700\u8981\u8bbe\u7f6e os . environ [ 'CUDA_VISIBLE_DEVICES' ] = '0,1' # \u65b9\u6848\u4e8c\uff1a\u4f7f\u7528\u201cdevice\u201d\uff0c\u540e\u7eed\u5bf9\u8981\u4f7f\u7528GPU\u7684\u53d8\u91cf\u7528.to(device)\u5373\u53ef device = torch . device ( \"cuda:1\" if torch . cuda . is_available () else \"cpu\" ) \u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406 \u00b6 PyTorch\u6570\u636e\u8bfb\u5165\u662f\u901a\u8fc7Dataset+Dataloader\u7684\u65b9\u5f0f\u5b8c\u6210\u7684\uff0cDataset\u5b9a\u4e49\u597d\u6570\u636e\u7684\u683c\u5f0f\u548c\u6570\u636e\u53d8\u6362\u5f62\u5f0f\uff0cDataloader\u7528iterative\u7684\u65b9\u5f0f\u4e0d\u65ad\u8bfb\u5165\u6279\u6b21\u6570\u636e\u3002 \u6211\u4eec\u53ef\u4ee5\u5b9a\u4e49\u81ea\u5df1\u7684Dataset\u7c7b\u6765\u5b9e\u73b0\u7075\u6d3b\u7684\u6570\u636e\u8bfb\u53d6\uff0c\u5b9a\u4e49\u7684\u7c7b\u9700\u8981\u7ee7\u627fPyTorch\u81ea\u8eab\u7684Dataset\u7c7b\u3002\u4e3b\u8981\u5305\u542b\u4e09\u4e2a\u51fd\u6570\uff1a __init__ : \u7528\u4e8e\u5411\u7c7b\u4e2d\u4f20\u5165\u5916\u90e8\u53c2\u6570\uff0c\u540c\u65f6\u5b9a\u4e49\u6837\u672c\u96c6 __getitem__ : \u7528\u4e8e\u9010\u4e2a\u8bfb\u53d6\u6837\u672c\u96c6\u5408\u4e2d\u7684\u5143\u7d20\uff0c\u53ef\u4ee5\u8fdb\u884c\u4e00\u5b9a\u7684\u53d8\u6362\uff0c\u5e76\u5c06\u8fd4\u56de\u8bad\u7ec3/\u9a8c\u8bc1\u6240\u9700\u7684\u6570\u636e __len__ : \u7528\u4e8e\u8fd4\u56de\u6570\u636e\u96c6\u7684\u6837\u672c\u6570 batch_size\uff1a\u6837\u672c\u662f\u6309\u201c\u6279\u201d\u8bfb\u5165\u7684\uff0cbatch_size\u5c31\u662f\u6bcf\u6b21\u8bfb\u5165\u7684\u6837\u672c\u6570 num_workers\uff1a\u6709\u591a\u5c11\u4e2a\u8fdb\u7a0b\u7528\u4e8e\u8bfb\u53d6\u6570\u636e shuffle\uff1a\u662f\u5426\u5c06\u8bfb\u5165\u7684\u6570\u636e\u6253\u4e71 drop_last\uff1a\u5bf9\u4e8e\u6837\u672c\u6700\u540e\u4e00\u90e8\u5206\u6ca1\u6709\u8fbe\u5230\u6279\u6b21\u6570\u7684\u6837\u672c\uff0c\u4e0d\u518d\u53c2\u4e0e\u8bad\u7ec3 \u4e0b\u9762\u662f\u672c\u90e8\u5206\u4ee3\u7801\u5728notebook\u4e2d\u7684\u8fd0\u884c\u60c5\u51b5\u3002\u4e3b\u8981\u53c2\u8003 PyTorch\u5b98\u65b9\u6559\u7a0b\u4e2d\u6587\u7248 https://pytorch123.com/SecondSection/training_a_classifier/ \u6a21\u578b\u6784\u5efa \u00b6 \u795e\u7ecf\u7f51\u7edc\u7684\u6784\u9020 \u00b6 PyTorch\u4e2d\u795e\u7ecf\u7f51\u7edc\u6784\u9020\u4e00\u822c\u662f\u57fa\u4e8e Module \u7c7b\u7684\u6a21\u578b\u6765\u5b8c\u6210\u7684\u3002Module \u7c7b\u662f nn \u6a21\u5757\uf9e9\u63d0\u4f9b\u7684\u4e00\u4e2a\u6a21\u578b\u6784\u9020\u7c7b\uff0c\u662f\u6240\u6709\u795e\u7ecf\u2f79\u7f51\u7edc\u6a21\u5757\u7684\u57fa\u7c7b\uff0c\u6211\u4eec\u53ef\u4ee5\u7ee7\u627f\u5b83\u6765\u5b9a\u4e49\u6211\u4eec\u60f3\u8981\u7684\u6a21\u578b\u3002\u4e0b\u9762\u7ee7\u627f Module \u7c7b\u6784\u9020\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u3002 import torch from torch import nn class MLP ( nn . Module ): # \u58f0\u660e\u5e26\u6709\u6a21\u578b\u53c2\u6570\u7684\u5c42\uff0c\u8fd9\u91cc\u58f0\u660e\u4e86\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42 def __init__ ( self , ** kwargs ): # \u8c03\u7528MLP\u7236\u7c7bBlock\u7684\u6784\u9020\u51fd\u6570\u6765\u8fdb\u884c\u5fc5\u8981\u7684\u521d\u59cb\u5316\u3002\u8fd9\u6837\u5728\u6784\u9020\u5b9e\u4f8b\u4f8b\u65f6\u8fd8\u53ef\u4ee5\u6307\u5b9a\u5176\u4ed6\u51fd\u6570 super ( MLP , self ) . __init__ ( ** kwargs ) self . hidden = nn . Linear ( 784 , 256 ) self . act = nn . ReLU () self . output = nn . Linear ( 256 , 10 ) # \u5b9a\u4e49\u6a21\u578b\u7684\u524d\u5411\u8ba1\u7b97\uff0c\u5373\u5982\u4f55\u6839\u636e\u8f93\u5165x\u8ba1\u7b97\u8fd4\u56de\u6240\u9700\u8981\u7684\u6a21\u578b\u8f93\u51fa def forward ( self , x ): o = self . act ( self . hidden ( x )) return self . output ( o ) \u6211\u4eec\u53ef\u4ee5\u5b9e\u4f8b\u5316 MLP \u7c7b\u5f97\u5230\u6a21\u578b\u53d8\uf97e net \u3002\u4e0b\u2faf\u7684\u4ee3\u7801\u521d\u59cb\u5316 net \u5e76\u4f20\u5165\u8f93\u2f0a\u6570\u636e X \u505a\u4e00\u6b21\u524d\u5411\u8ba1\u7b97\u3002\u5176\u4e2d\uff0c net(X) \u4f1a\u8c03\u7528 MLP \u7ee7\u627f\u2f83\u81ea Module \u7c7b\u7684 call \u51fd\u6570\uff0c\u8fd9\u4e2a\u51fd\u6570\u5c06\u8c03\u2f64\u7528 MLP \u7c7b\u5b9a\u4e49\u7684forward \u51fd\u6570\u6765\u5b8c\u6210\u524d\u5411\u8ba1\u7b97\u3002 >>> import torch >>> X = torch . rand ( 2 , 784 ) >>> X tensor ([[ 0.3277 , 0.2204 , 0.5239 , ... , 0.4333 , 0.1906 , 0.1318 ], [ 0.9850 , 0.2121 , 0.8405 , ... , 0.3796 , 0.2717 , 0.5553 ]]) >>> from torch import nn >>> >>> class MLP ( nn . Module ): ... # \u58f0\u660e\u5e26\u6709\u6a21\u578b\u53c2\u6570\u7684\u5c42\uff0c\u8fd9\u91cc\u58f0\u660e\u4e86\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42 ... def __init__ ( self , ** kwargs ): ... # \u8c03\u7528MLP\u7236\u7c7bBlock\u7684\u6784\u9020\u51fd\u6570\u6765\u8fdb\u884c\u5fc5\u8981\u7684\u521d\u59cb\u5316\u3002\u8fd9\u6837\u5728\u6784\u9020\u5b9e\u4f8b\u4f8b\u65f6\u8fd8\u53ef\u4ee5\u6307\u5b9a\u5176\u4ed6\u51fd\u6570 ... super ( MLP , self ) . __init__ ( ** kwargs ) ... self . hidden = nn . Linear ( 784 , 256 ) ... self . act = nn . ReLU () ... self . output = nn . Linear ( 256 , 10 ) ... ... # \u5b9a\u4e49\u6a21\u578b\u7684\u524d\u5411\u8ba1\u7b97\uff0c\u5373\u5982\u4f55\u6839\u636e\u8f93\u5165x\u8ba1\u7b97\u8fd4\u56de\u6240\u9700\u8981\u7684\u6a21\u578b\u8f93\u51fa ... def forward ( self , x ): ... o = self . act ( self . hidden ( x )) ... return self . output ( o ) ... >>> net = MLP () >>> net MLP ( ( hidden ): Linear ( in_features = 784 , out_features = 256 , bias = True ) ( act ): ReLU () ( output ): Linear ( in_features = 256 , out_features = 10 , bias = True ) ) >>> net ( X ) tensor ([[ 0.1317 , 0.0702 , 0.1707 , - 0.0081 , - 0.2730 , 0.2837 , 0.0700 , 0.1718 , 0.0299 , 0.2082 ], [ 0.1094 , 0.0936 , 0.2474 , - 0.0139 , - 0.1861 , 0.1846 , 0.1658 , 0.2051 , 0.2609 , 0.2227 ]], grad_fn =< AddmmBackward > ) >>> \u795e\u7ecf\u7f51\u7edc\u4e2d\u5e38\u89c1\u7684\u5c42 \u00b6 \u4e0d\u542b\u6a21\u578b\u53c2\u6570\u7684\u5c42 \u00b6 \u4e0b\u2faf\u6784\u9020\u7684 MyLayer \u7c7b\u901a\u8fc7\u7ee7\u627f Module \u7c7b\u81ea\u5b9a\u4e49\uf9ba\u4e00\u4e2a**\u5c06\u8f93\u5165\u51cf\u6389\u5747\u503c\u540e\u8f93\u51fa**\u7684\u5c42\u3002\u8fd9\u4e2a\u5c42\uf9e9\uf967\u542b\u6a21\u578b\u53c2\u6570\u3002 >>> import torch >>> from torch import nn >>> >>> class MyLayer ( nn . Module ): ... def __init__ ( self , ** kwargs ): ... super ( MyLayer , self ) . __init__ ( ** kwargs ) ... def forward ( self , x ): ... return x - x . mean () ... >>> layer = MyLayer () # \u5b9e\u4f8b\u5316\u8be5\u5c42 >>> layer MyLayer () >>> layer ( torch . tensor ([ 1 , 2 , 3 , 4 , 5 ], dtype = torch . float )) tensor ([ - 2. , - 1. , 0. , 1. , 2. ]) \u542b\u6a21\u578b\u53c2\u6570\u7684\u5c42 \u00b6 \u6211\u4eec\u8fd8\u53ef\u4ee5\u81ea\u5b9a\u4e49\u542b\u6a21\u578b\u53c2\u6570\u7684\u81ea\u5b9a\u4e49\u5c42\u3002\u5176\u4e2d\u7684\u6a21\u578b\u53c2\u6570\u53ef\u4ee5\u901a\u8fc7\u8bad\u7ec3\u5b66\u51fa\u3002 Parameter \u7c7b\u5176\u5b9e\u662f Tensor \u7684\u5b50\u7c7b\uff0c\u5982\u679c\u4e00 \u4e2a Tensor \u662f Parameter \uff0c\u90a3\u4e48\u5b83\u4f1a\u2f83\u52a8\u88ab\u6dfb\u52a0\u5230\u6a21\u578b\u7684\u53c2\u6570\uf99c\u8868\uf9e9\u3002\u6240\u4ee5\u5728\u2f83\u5b9a\u4e49\u542b\u6a21\u578b\u53c2\u6570\u7684\u5c42\u65f6\uff0c\u6211\u4eec\u5e94\u8be5\u5c06\u53c2\u6570\u5b9a\u4e49\u6210 Parameter \uff0c\u9664\u4e86\u76f4\u63a5\u5b9a\u4e49\u6210 Parameter \u7c7b\u5916\uff0c\u8fd8\u53ef\u4ee5\u4f7f\u2f64 ParameterList \u548c ParameterDict \u5206\u522b\u5b9a\u4e49\u53c2\u6570\u7684\uf99c\u8868\u548c\u5b57\u5178\u3002 class MyListDense ( nn . Module ): def __init__ ( self ): super ( MyListDense , self ) . __init__ () self . params = nn . ParameterList ([ nn . Parameter ( torch . randn ( 4 , 4 )) for i in range ( 3 )]) self . params . append ( nn . Parameter ( torch . randn ( 4 , 1 ))) def forward ( self , x ): for i in range ( len ( self . params )): x = torch . mm ( x , self . params [ i ]) return x >>> net = MyListDense () >>> print ( net ) MyListDense ( ( params ): ParameterList ( ( 0 ): Parameter containing : [ torch . FloatTensor of size 4 x4 ] ( 1 ): Parameter containing : [ torch . FloatTensor of size 4 x4 ] ( 2 ): Parameter containing : [ torch . FloatTensor of size 4 x4 ] ( 3 ): Parameter containing : [ torch . FloatTensor of size 4 x1 ] ) ) class MyDictDense ( nn . Module ): def __init__ ( self ): super ( MyDictDense , self ) . __init__ () self . params = nn . ParameterDict ({ 'linear1' : nn . Parameter ( torch . randn ( 4 , 4 )), 'linear2' : nn . Parameter ( torch . randn ( 4 , 1 )) }) self . params . update ({ 'linear3' : nn . Parameter ( torch . randn ( 4 , 2 ))}) # \u65b0\u589e def forward ( self , x , choice = 'linear1' ): return torch . mm ( x , self . params [ choice ]) >>> net = MyDictDense () >>> print ( net ) MyDictDense ( ( params ): ParameterDict ( ( linear1 ): Parameter containing : [ torch . FloatTensor of size 4 x4 ] ( linear2 ): Parameter containing : [ torch . FloatTensor of size 4 x1 ] ( linear3 ): Parameter containing : [ torch . FloatTensor of size 4 x2 ] ) ) \u4e0b\u9762\u7ed9\u51fa\u5e38\u89c1\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u4e00\u4e9b\u5c42\uff0c\u6bd4\u5982\u5377\u79ef\u5c42\u3001\u6c60\u5316\u5c42\uff0c\u4ee5\u53ca\u8f83\u4e3a\u57fa\u7840\u7684AlexNet\uff0cLeNet\u7b49\u3002 \u4e8c\u7ef4\u5377\u79ef\u5c42 \u00b6 \u4e8c\u7ef4\u5377\u79ef\u5c42\u5c06\u8f93\u5165\u548c\u5377\u79ef\u6838\u505a\u4e92\u76f8\u5173\u8fd0\u7b97\uff0c\u5e76\u52a0\u4e0a\u4e00\u4e2a\u6807\uf97e\u504f\u5dee\u6765\u5f97\u5230\u8f93\u51fa\u3002 import torch from torch import nn # \u5377\u79ef\u8fd0\u7b97\uff08\u4e8c\u7ef4\u4e92\u76f8\u5173\uff09 def corr2d ( X , K ): h , w = K . shape X , K = X . float (), K . float () Y = torch . zeros (( X . shape [ 0 ] - h + 1 , X . shape [ 1 ] - w + 1 )) for i in range ( Y . shape [ 0 ]): for j in range ( Y . shape [ 1 ]): Y [ i , j ] = ( X [ i : i + h , j : j + w ] * K ) . sum () return Y # \u4e8c\u7ef4\u5377\u79ef\u5c42 class Conv2D ( nn . Module ): def __init__ ( self , kernel_size ): super ( Conv2D , self ) . __init__ () self . weight = nn . Parameter ( torch . randn ( kernel_size )) self . bias = nn . Parameter ( torch . randn ( 1 )) def forward ( self , x ): return corr2d ( x , self . weight ) + self . bias \u586b\u5145(padding)\u662f\u6307\u5728\u8f93\u2f0a\u5165\u2fbc\u9ad8\u548c\u5bbd\u7684\u4e24\u4fa7\u586b\u5145\u5143\u7d20(\u901a\u5e38\u662f0\u5143\u7d20)\u3002 \u5728\u4e8c\u7ef4\u4e92\u76f8\u5173\u8fd0\u7b97\u4e2d\uff0c\u5377\u79ef\u7a97\u53e3\u4ece\u8f93\u5165\u6570\u7ec4\u7684\u6700\u5de6\u4e0a\u65b9\u5f00\u59cb\uff0c\u6309\u4ece\u5de6\u5f80\u53f3\u3001\u4ece\u4e0a\u5f80\u4e0b \u7684\u987a\u5e8f\uff0c\u4f9d\u6b21\u5728\u8f93\u2f0a\u6570\u7ec4\u4e0a\u6ed1\u52a8\u3002\u6211\u4eec\u5c06\u6bcf\u6b21\u6ed1\u52a8\u7684\ufa08\u6570\u548c\uf99c\u6570\u79f0\u4e3a\u6b65\u5e45(stride)\u3002 \uff08skip\uff09 \u6c60\u5316\u5c42 \u00b6 \u6c60\u5316\u5c42\u6bcf\u6b21\u5bf9\u8f93\u5165\u6570\u636e\u7684\u4e00\u4e2a\u56fa\u5b9a\u5f62\u72b6\u7a97\u53e3(\u2f1c\u79f0\u6c60\u5316\u7a97\u53e3)\u4e2d\u7684\u5143\u7d20\u8ba1\u7b97\u8f93\u51fa\u3002\uf967\u540c\u4e8e\u5377\u79ef\u5c42\uf9e9\u8ba1\u7b97\u8f93\u2f0a\u548c\u6838\u7684\u4e92\u76f8\u5173\u6027\uff0c\u6c60\u5316\u5c42\u76f4\u63a5\u8ba1\u7b97\u6c60\u5316\u7a97\u53e3\u5185\u5143\u7d20\u7684\u6700\u5927\u503c\u6216\u8005\u5e73\u5747\u503c\u3002\u8be5\u8fd0\u7b97\u4e5f \u5206\u522b\u53eb\u505a\u6700\u5927\u6c60\u5316\u6216\u5e73\u5747\u6c60\u5316\u3002 >>> import numpy as np >>> import torch >>> from torch import nn >>> >>> def pool2d ( X , pool_size , mode = 'max' ): ... p_h , p_w = pool_size ... Y = np . zeros (( X . shape [ 0 ] - p_h + 1 , X . shape [ 1 ] - p_w + 1 )) ... for i in range ( Y . shape [ 0 ]): ... for j in range ( Y . shape [ 1 ]): ... if mode == 'max' : ... Y [ i , j ] = X [ i : i + p_h , j : j + p_w ] . max () ... elif mode == 'avg' : ... Y [ i , j ] = X [ i : i + p_h , j : j + p_w ] . mean () ... return Y ... >>> X = torch . tensor ([[ 0 , 1 , 2 ], [ 3 , 4 , 5 ], [ 6 , 7 , 8 ]]) >>> pool2d ( X , ( 2 , 2 )) array ([[ 4. , 5. ], [ 7. , 8. ]]) \u6a21\u578b\u793a\u4f8b\uff1aLeNet \u00b6 \uff08\u5f85\u8865\u5145\uff09 \u6a21\u578b\u793a\u4f8b\uff1aAlexNet \u00b6 \uff08\u5f85\u8865\u5145\uff09 \u635f\u5931\u51fd\u6570 \u00b6 \u4e00\u4e2a\u597d\u7684\u8bad\u7ec3\u79bb\u4e0d\u5f00\u4f18\u8d28\u7684\u8d1f\u53cd\u9988\uff0c\u8fd9\u91cc\u7684\u635f\u5931\u51fd\u6570\u5c31\u662f\u6a21\u578b\u7684\u8d1f\u53cd\u9988\u3002 \u8fd9\u91cc\u5c06\u5217\u51faPyTorch\u4e2d\u5e38\u7528\u7684\u635f\u5931\u51fd\u6570\uff08\u4e00\u822c\u901a\u8fc7torch.nn\u8c03\u7528\uff09\uff0c\u5e76\u8be6\u7ec6\u4ecb\u7ecd\u6bcf\u4e2a\u635f\u5931\u51fd\u6570\u7684\u529f\u80fd\u4ecb\u7ecd\u3001\u6570\u5b66\u516c\u5f0f\u548c\u8c03\u7528\u4ee3\u7801\u3002 \u4e8c\u5206\u7c7b\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570 \u00b6 torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean') \u529f\u80fd \uff1a\u8ba1\u7b97\u4e8c\u5206\u7c7b\u4efb\u52a1\u65f6\u7684\u4ea4\u53c9\u71b5\uff08Cross Entropy\uff09\u51fd\u6570\u3002\u5728\u4e8c\u5206\u7c7b\u4e2d\uff0clabel\u662f{0,1}\u3002\u5bf9\u4e8e\u8fdb\u5165\u4ea4\u53c9\u71b5\u51fd\u6570\u7684input\u4e3a\u6982\u7387\u5206\u5e03\u7684\u5f62\u5f0f\u3002\u4e00\u822c\u6765\u8bf4\uff0cinput\u4e3asigmoid\u6fc0\u6d3b\u5c42\u7684\u8f93\u51fa\uff0c\u6216\u8005softmax\u7684\u8f93\u51fa\u3002 \u4e3b\u8981\u53c2\u6570 \uff1a weight :\u6bcf\u4e2a\u7c7b\u522b\u7684loss\u8bbe\u7f6e\u6743\u503c size_average :\u6570\u636e\u4e3abool\uff0c\u4e3aTrue\u65f6\uff0c\u8fd4\u56de\u7684loss\u4e3a\u5e73\u5747\u503c\uff1b\u4e3aFalse\u65f6\uff0c\u8fd4\u56de\u7684\u5404\u6837\u672c\u7684loss\u4e4b\u548c. reduce :\u6570\u636e\u7c7b\u578b\u4e3abool\uff0c\u4e3aTrue\u65f6\uff0closs\u7684\u8fd4\u56de\u662f\u6807\u91cf\u3002 \u5176\u4ed6\u635f\u5931\u51fd\u6570 \u00b6 \u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570 L1\u635f\u5931\u51fd\u6570 MSE\u635f\u5931\u51fd\u6570 \u5e73\u6ed1L1 (Smooth L1)\u635f\u5931\u51fd\u6570 \u76ee\u6807\u6cca\u677e\u5206\u5e03\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931 KL\u6563\u5ea6 \u4f18\u5316\u5668 \u00b6 \u4ec0\u4e48\u662f\u4f18\u5316\u5668 \u00b6 \u6df1\u5ea6\u5b66\u4e60\u7684\u76ee\u6807\u662f\u901a\u8fc7\u4e0d\u65ad\u6539\u53d8\u7f51\u7edc\u53c2\u6570\uff0c\u4f7f\u5f97\u53c2\u6570\u80fd\u591f\u5bf9\u8f93\u5165\u505a\u5404\u79cd\u975e\u7ebf\u6027\u53d8\u6362\u62df\u5408\u8f93\u51fa\uff0c\u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u4e2a\u51fd\u6570\u53bb\u5bfb\u627e\u6700\u4f18\u89e3\uff0c\u53ea\u4e0d\u8fc7\u8fd9\u4e2a\u6700\u4f18\u89e3\u4f7f\u4e00\u4e2a\u77e9\u9635\u3002\u90a3\u4e48\u6211\u4eec\u5982\u4f55\u8ba1\u7b97\u51fa\u6765\u8fd9\u4e48\u591a\u7684\u7cfb\u6570\uff0c\u6709\u4ee5\u4e0b\u4e24\u79cd\u65b9\u6cd5\uff1a \u7b2c\u4e00\u79cd\u662f\u6700\u76f4\u63a5\u7684\u66b4\u529b\u7a77\u4e3e\u4e00\u904d\u53c2\u6570\uff0c\u8fd9\u79cd\u65b9\u6cd5\u7684\u5b9e\u65bd\u53ef\u80fd\u6027\u57fa\u672c\u4e3a0\uff0c\u582a\u6bd4\u611a\u516c\u79fb\u5c71plus\u7684\u96be\u5ea6\u3002 \u4e3a\u4e86\u4f7f\u6c42\u89e3\u53c2\u6570\u8fc7\u7a0b\u66f4\u52a0\u5feb\uff0c\u4eba\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e8c\u79cd\u529e\u6cd5\uff0c\u5373\u5c31\u662f\u662fBP+\u4f18\u5316\u5668\u903c\u8fd1\u6c42\u89e3\u3002 \u56e0\u6b64\uff0c\u4f18\u5316\u5668\u5c31\u662f\u6839\u636e\u7f51\u7edc\u53cd\u5411\u4f20\u64ad\u7684\u68af\u5ea6\u4fe1\u606f\u6765\u66f4\u65b0\u7f51\u7edc\u7684\u53c2\u6570\uff0c\u4ee5\u8d77\u5230\u964d\u4f4eloss\u51fd\u6570\u8ba1\u7b97\u503c\uff0c\u4f7f\u5f97\u6a21\u578b\u8f93\u51fa\u66f4\u52a0\u63a5\u8fd1\u771f\u5b9e\u6807\u7b7e\u3002 PyTorch\u63d0\u4f9b\u7684\u4f18\u5316\u5668 \u00b6 Pytorch\u5f88\u4eba\u6027\u5316\u7684\u7ed9\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f18\u5316\u5668\u7684\u5e93torch.optim\uff0c\u5728\u8fd9\u91cc\u9762\u7ed9\u6211\u4eec\u63d0\u4f9b\u4e86\u5341\u79cd\u4f18\u5316\u5668\u3002 torch.optim.ASGD torch.optim.Adadelta torch.optim.Adagrad torch.optim.Adam torch.optim.AdamW torch.optim.Adamax torch.optim.LBFGS torch.optim.RMSprop torch.optim.Rprop torch.optim.SGD torch.optim.SparseAdam \u8bad\u7ec3\u4e0e\u8bc4\u4f30 \u00b6 \u5b8c\u6210\u4e86\u4e0a\u8ff0\u8bbe\u5b9a\u540e\u5c31\u53ef\u4ee5\u52a0\u8f7d\u6570\u636e\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b\u4e86\u3002\u9996\u5148\u5e94\u8be5\u8bbe\u7f6e\u6a21\u578b\u7684\u72b6\u6001\uff1a\u5982\u679c\u662f\u8bad\u7ec3\u72b6\u6001\uff0c\u90a3\u4e48\u6a21\u578b\u7684\u53c2\u6570\u5e94\u8be5\u652f\u6301\u53cd\u5411\u4f20\u64ad\u7684\u4fee\u6539\uff1b\u5982\u679c\u662f\u9a8c\u8bc1/\u6d4b\u8bd5\u72b6\u6001\uff0c\u5219\u4e0d\u5e94\u8be5\u4fee\u6539\u6a21\u578b\u53c2\u6570\u3002 model . train () # \u8bad\u7ec3\u72b6\u6001 model . eval () # \u9a8c\u8bc1/\u6d4b\u8bd5\u72b6\u6001 \u8bad\u7ec3\u8fc7\u7a0b\uff1a def train ( epoch ): model . train () train_loss = 0 for data , label in train_loader : # \u6b64\u65f6\u8981\u7528for\u5faa\u73af\u8bfb\u53d6DataLoader\u4e2d\u7684\u5168\u90e8\u6570\u636e\u3002 data , label = data . cuda (), label . cuda () # \u4e4b\u540e\u5c06\u6570\u636e\u653e\u5230GPU\u4e0a\u7528\u4e8e\u540e\u7eed\u8ba1\u7b97\uff0c\u6b64\u5904\u4ee5.cuda()\u4e3a\u4f8b optimizer . zero_grad () # \u5f00\u59cb\u7528\u5f53\u524d\u6279\u6b21\u6570\u636e\u505a\u8bad\u7ec3\u65f6\uff0c\u5e94\u5f53\u5148\u5c06\u4f18\u5316\u5668\u7684\u68af\u5ea6\u7f6e\u96f6 output = model ( data ) # \u4e4b\u540e\u5c06data\u9001\u5165\u6a21\u578b\u4e2d\u8bad\u7ec3 loss = criterion ( label , output ) # \u6839\u636e\u9884\u5148\u5b9a\u4e49\u7684criterion\u8ba1\u7b97\u635f\u5931\u51fd\u6570 loss . backward () # \u5c06loss\u53cd\u5411\u4f20\u64ad\u56de\u7f51\u7edc optimizer . step () # \u4f7f\u7528\u4f18\u5316\u5668\u66f4\u65b0\u6a21\u578b\u53c2\u6570 train_loss += loss . item () * data . size ( 0 ) train_loss = train_loss / len ( train_loader . dataset ) print ( 'Epoch: {} \\t Training Loss: {:.6f} ' . format ( epoch , train_loss )) \u9a8c\u8bc1/\u6d4b\u8bd5\u7684\u6d41\u7a0b\u57fa\u672c\u4e0e\u8bad\u7ec3\u8fc7\u7a0b\u4e00\u81f4\uff0c\u4e0d\u540c\u70b9\u5728\u4e8e\uff1a \u9700\u8981\u9884\u5148\u8bbe\u7f6etorch.no_grad\uff0c\u4ee5\u53ca\u5c06model\u8c03\u81f3eval\u6a21\u5f0f \u4e0d\u9700\u8981\u5c06\u4f18\u5316\u5668\u7684\u68af\u5ea6\u7f6e\u96f6 \u4e0d\u9700\u8981\u5c06loss\u53cd\u5411\u56de\u4f20\u5230\u7f51\u7edc \u4e0d\u9700\u8981\u66f4\u65b0optimizer \u9a8c\u8bc1/\u6d4b\u8bd5\u8fc7\u7a0b\uff1a def val ( epoch ): model . eval () val_loss = 0 with torch . no_grad (): for data , label in val_loader : data , label = data . cuda (), label . cuda () output = model ( data ) preds = torch . argmax ( output , 1 ) loss = criterion ( output , label ) val_loss += loss . item () * data . size ( 0 ) running_accu += torch . sum ( preds == label . data ) val_loss = val_loss / len ( val_loader . dataset ) print ( 'Epoch: {} \\t Training Loss: {:.6f} ' . format ( epoch , val_loss )) \u53ef\u89c6\u5316 \u00b6 \u5728PyTorch\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c\u53ef\u89c6\u5316\u662f\u4e00\u4e2a\u53ef\u9009\u9879\uff0c\u6307\u7684\u662f\u67d0\u4e9b\u4efb\u52a1\u5728\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u9700\u8981\u5bf9\u4e00\u4e9b\u5fc5\u8981\u7684\u5185\u5bb9\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u6bd4\u5982\u5206\u7c7b\u7684ROC\u66f2\u7ebf\uff0c\u5377\u79ef\u7f51\u7edc\u4e2d\u7684\u5377\u79ef\u6838\uff0c\u4ee5\u53ca\u8bad\u7ec3/\u9a8c\u8bc1\u8fc7\u7a0b\u7684\u635f\u5931\u51fd\u6570\u66f2\u7ebf\u7b49\u7b49\u3002 \u53c2\u8003\u8d44\u6599 \u00b6 Datawhale\u5f00\u6e90\u9879\u76ee\uff1a\u6df1\u5165\u6d45\u51faPyTorch https://github.com/datawhalechina/thorough-pytorch/ \u674e\u5b8f\u6bc5\u673a\u5668\u5b66\u4e602021\u6625-PyTorch Tutorial https://www.bilibili.com/video/BV1Wv411h7kN?p=5 \u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60pytorch\u7248 https://zh-v2.d2l.ai/chapter_preface/index.html PyTorch\u5b98\u65b9\u6559\u7a0b\u4e2d\u6587\u7248 https://pytorch123.com/SecondSection/training_a_classifier/","title":"Chapter03 PyTorch\u7684\u4e3b\u8981\u7ec4\u6210\u6a21\u5757"},{"location":"pytorch-chap03/#pytorch","text":"","title":"\u7b2c\u4e09\u7ae0 PyTorch\u7684\u4e3b\u8981\u7ec4\u6210\u6a21\u5757"},{"location":"pytorch-chap03/#_1","text":"\u673a\u5668\u5b66\u4e60\uff1a \u6570\u636e\u9884\u5904\u7406\uff08\u6570\u636e\u683c\u5f0f\u3001\u6570\u636e\u8f6c\u6362\u3001\u5212\u5206\u6570\u636e\u96c6\uff09 \u9009\u62e9\u6a21\u578b\uff0c\u8bbe\u5b9a\u635f\u5931\u548c\u4f18\u5316\u51fd\u6570\uff0c\u8bbe\u7f6e\u8d85\u53c2\u6570 \u8bad\u7ec3\u6a21\u578b\uff0c\u62df\u5408\u8bad\u7ec3\u96c6 \u8bc4\u4f30\u6a21\u578b\uff0c\u5728\u5e76\u5728\u9a8c\u8bc1\u96c6/\u6d4b\u8bd5\u96c6\u4e0a\u8ba1\u7b97\u6a21\u578b\u8868\u73b0 \u6df1\u5ea6\u5b66\u4e60\u7684\u6ce8\u610f\u4e8b\u9879\uff1a \u6570\u636e\u9884\u5904\u7406\uff08\u6570\u636e\u52a0\u8f7d\u3001\u6279\u5904\u7406\uff09 \u9010\u5c42\u642d\u5efa\u6a21\u578b\uff0c\u7ec4\u88c5\u4e0d\u540c\u6a21\u5757 GPU\u7684\u914d\u7f6e\u548c\u64cd\u4f5c","title":"\u5b8c\u6210\u6df1\u5ea6\u5b66\u4e60\u7684\u5fc5\u8981\u90e8\u5206"},{"location":"pytorch-chap03/#_2","text":"\u5bfc\u5165\u5fc5\u987b\u7684\u5305\uff1a import os import numpy as np import torch import torch.nn as nn from torch.utils.data import Dataset , DataLoader import torch.optim as optimizer \u8d85\u53c2\u6570\u8bbe\u7f6e\uff1a batch_size = 16 # batch size lr = 1e-4 # \u521d\u59cb\u5b66\u4e60\u7387 max_epochs = 100 # \u8bad\u7ec3\u6b21\u6570 GPU\u7684\u8bbe\u7f6e\uff1a # \u65b9\u6848\u4e00\uff1a\u4f7f\u7528os.environ\uff0c\u8fd9\u79cd\u60c5\u51b5\u5982\u679c\u4f7f\u7528GPU\u4e0d\u9700\u8981\u8bbe\u7f6e os . environ [ 'CUDA_VISIBLE_DEVICES' ] = '0,1' # \u65b9\u6848\u4e8c\uff1a\u4f7f\u7528\u201cdevice\u201d\uff0c\u540e\u7eed\u5bf9\u8981\u4f7f\u7528GPU\u7684\u53d8\u91cf\u7528.to(device)\u5373\u53ef device = torch . device ( \"cuda:1\" if torch . cuda . is_available () else \"cpu\" )","title":"\u57fa\u672c\u914d\u7f6e"},{"location":"pytorch-chap03/#_3","text":"PyTorch\u6570\u636e\u8bfb\u5165\u662f\u901a\u8fc7Dataset+Dataloader\u7684\u65b9\u5f0f\u5b8c\u6210\u7684\uff0cDataset\u5b9a\u4e49\u597d\u6570\u636e\u7684\u683c\u5f0f\u548c\u6570\u636e\u53d8\u6362\u5f62\u5f0f\uff0cDataloader\u7528iterative\u7684\u65b9\u5f0f\u4e0d\u65ad\u8bfb\u5165\u6279\u6b21\u6570\u636e\u3002 \u6211\u4eec\u53ef\u4ee5\u5b9a\u4e49\u81ea\u5df1\u7684Dataset\u7c7b\u6765\u5b9e\u73b0\u7075\u6d3b\u7684\u6570\u636e\u8bfb\u53d6\uff0c\u5b9a\u4e49\u7684\u7c7b\u9700\u8981\u7ee7\u627fPyTorch\u81ea\u8eab\u7684Dataset\u7c7b\u3002\u4e3b\u8981\u5305\u542b\u4e09\u4e2a\u51fd\u6570\uff1a __init__ : \u7528\u4e8e\u5411\u7c7b\u4e2d\u4f20\u5165\u5916\u90e8\u53c2\u6570\uff0c\u540c\u65f6\u5b9a\u4e49\u6837\u672c\u96c6 __getitem__ : \u7528\u4e8e\u9010\u4e2a\u8bfb\u53d6\u6837\u672c\u96c6\u5408\u4e2d\u7684\u5143\u7d20\uff0c\u53ef\u4ee5\u8fdb\u884c\u4e00\u5b9a\u7684\u53d8\u6362\uff0c\u5e76\u5c06\u8fd4\u56de\u8bad\u7ec3/\u9a8c\u8bc1\u6240\u9700\u7684\u6570\u636e __len__ : \u7528\u4e8e\u8fd4\u56de\u6570\u636e\u96c6\u7684\u6837\u672c\u6570 batch_size\uff1a\u6837\u672c\u662f\u6309\u201c\u6279\u201d\u8bfb\u5165\u7684\uff0cbatch_size\u5c31\u662f\u6bcf\u6b21\u8bfb\u5165\u7684\u6837\u672c\u6570 num_workers\uff1a\u6709\u591a\u5c11\u4e2a\u8fdb\u7a0b\u7528\u4e8e\u8bfb\u53d6\u6570\u636e shuffle\uff1a\u662f\u5426\u5c06\u8bfb\u5165\u7684\u6570\u636e\u6253\u4e71 drop_last\uff1a\u5bf9\u4e8e\u6837\u672c\u6700\u540e\u4e00\u90e8\u5206\u6ca1\u6709\u8fbe\u5230\u6279\u6b21\u6570\u7684\u6837\u672c\uff0c\u4e0d\u518d\u53c2\u4e0e\u8bad\u7ec3 \u4e0b\u9762\u662f\u672c\u90e8\u5206\u4ee3\u7801\u5728notebook\u4e2d\u7684\u8fd0\u884c\u60c5\u51b5\u3002\u4e3b\u8981\u53c2\u8003 PyTorch\u5b98\u65b9\u6559\u7a0b\u4e2d\u6587\u7248 https://pytorch123.com/SecondSection/training_a_classifier/","title":"\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406"},{"location":"pytorch-chap03/#_4","text":"","title":"\u6a21\u578b\u6784\u5efa"},{"location":"pytorch-chap03/#_5","text":"PyTorch\u4e2d\u795e\u7ecf\u7f51\u7edc\u6784\u9020\u4e00\u822c\u662f\u57fa\u4e8e Module \u7c7b\u7684\u6a21\u578b\u6765\u5b8c\u6210\u7684\u3002Module \u7c7b\u662f nn \u6a21\u5757\uf9e9\u63d0\u4f9b\u7684\u4e00\u4e2a\u6a21\u578b\u6784\u9020\u7c7b\uff0c\u662f\u6240\u6709\u795e\u7ecf\u2f79\u7f51\u7edc\u6a21\u5757\u7684\u57fa\u7c7b\uff0c\u6211\u4eec\u53ef\u4ee5\u7ee7\u627f\u5b83\u6765\u5b9a\u4e49\u6211\u4eec\u60f3\u8981\u7684\u6a21\u578b\u3002\u4e0b\u9762\u7ee7\u627f Module \u7c7b\u6784\u9020\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u3002 import torch from torch import nn class MLP ( nn . Module ): # \u58f0\u660e\u5e26\u6709\u6a21\u578b\u53c2\u6570\u7684\u5c42\uff0c\u8fd9\u91cc\u58f0\u660e\u4e86\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42 def __init__ ( self , ** kwargs ): # \u8c03\u7528MLP\u7236\u7c7bBlock\u7684\u6784\u9020\u51fd\u6570\u6765\u8fdb\u884c\u5fc5\u8981\u7684\u521d\u59cb\u5316\u3002\u8fd9\u6837\u5728\u6784\u9020\u5b9e\u4f8b\u4f8b\u65f6\u8fd8\u53ef\u4ee5\u6307\u5b9a\u5176\u4ed6\u51fd\u6570 super ( MLP , self ) . __init__ ( ** kwargs ) self . hidden = nn . Linear ( 784 , 256 ) self . act = nn . ReLU () self . output = nn . Linear ( 256 , 10 ) # \u5b9a\u4e49\u6a21\u578b\u7684\u524d\u5411\u8ba1\u7b97\uff0c\u5373\u5982\u4f55\u6839\u636e\u8f93\u5165x\u8ba1\u7b97\u8fd4\u56de\u6240\u9700\u8981\u7684\u6a21\u578b\u8f93\u51fa def forward ( self , x ): o = self . act ( self . hidden ( x )) return self . output ( o ) \u6211\u4eec\u53ef\u4ee5\u5b9e\u4f8b\u5316 MLP \u7c7b\u5f97\u5230\u6a21\u578b\u53d8\uf97e net \u3002\u4e0b\u2faf\u7684\u4ee3\u7801\u521d\u59cb\u5316 net \u5e76\u4f20\u5165\u8f93\u2f0a\u6570\u636e X \u505a\u4e00\u6b21\u524d\u5411\u8ba1\u7b97\u3002\u5176\u4e2d\uff0c net(X) \u4f1a\u8c03\u7528 MLP \u7ee7\u627f\u2f83\u81ea Module \u7c7b\u7684 call \u51fd\u6570\uff0c\u8fd9\u4e2a\u51fd\u6570\u5c06\u8c03\u2f64\u7528 MLP \u7c7b\u5b9a\u4e49\u7684forward \u51fd\u6570\u6765\u5b8c\u6210\u524d\u5411\u8ba1\u7b97\u3002 >>> import torch >>> X = torch . rand ( 2 , 784 ) >>> X tensor ([[ 0.3277 , 0.2204 , 0.5239 , ... , 0.4333 , 0.1906 , 0.1318 ], [ 0.9850 , 0.2121 , 0.8405 , ... , 0.3796 , 0.2717 , 0.5553 ]]) >>> from torch import nn >>> >>> class MLP ( nn . Module ): ... # \u58f0\u660e\u5e26\u6709\u6a21\u578b\u53c2\u6570\u7684\u5c42\uff0c\u8fd9\u91cc\u58f0\u660e\u4e86\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42 ... def __init__ ( self , ** kwargs ): ... # \u8c03\u7528MLP\u7236\u7c7bBlock\u7684\u6784\u9020\u51fd\u6570\u6765\u8fdb\u884c\u5fc5\u8981\u7684\u521d\u59cb\u5316\u3002\u8fd9\u6837\u5728\u6784\u9020\u5b9e\u4f8b\u4f8b\u65f6\u8fd8\u53ef\u4ee5\u6307\u5b9a\u5176\u4ed6\u51fd\u6570 ... super ( MLP , self ) . __init__ ( ** kwargs ) ... self . hidden = nn . Linear ( 784 , 256 ) ... self . act = nn . ReLU () ... self . output = nn . Linear ( 256 , 10 ) ... ... # \u5b9a\u4e49\u6a21\u578b\u7684\u524d\u5411\u8ba1\u7b97\uff0c\u5373\u5982\u4f55\u6839\u636e\u8f93\u5165x\u8ba1\u7b97\u8fd4\u56de\u6240\u9700\u8981\u7684\u6a21\u578b\u8f93\u51fa ... def forward ( self , x ): ... o = self . act ( self . hidden ( x )) ... return self . output ( o ) ... >>> net = MLP () >>> net MLP ( ( hidden ): Linear ( in_features = 784 , out_features = 256 , bias = True ) ( act ): ReLU () ( output ): Linear ( in_features = 256 , out_features = 10 , bias = True ) ) >>> net ( X ) tensor ([[ 0.1317 , 0.0702 , 0.1707 , - 0.0081 , - 0.2730 , 0.2837 , 0.0700 , 0.1718 , 0.0299 , 0.2082 ], [ 0.1094 , 0.0936 , 0.2474 , - 0.0139 , - 0.1861 , 0.1846 , 0.1658 , 0.2051 , 0.2609 , 0.2227 ]], grad_fn =< AddmmBackward > ) >>>","title":"\u795e\u7ecf\u7f51\u7edc\u7684\u6784\u9020"},{"location":"pytorch-chap03/#_6","text":"","title":"\u795e\u7ecf\u7f51\u7edc\u4e2d\u5e38\u89c1\u7684\u5c42"},{"location":"pytorch-chap03/#_7","text":"\u4e0b\u2faf\u6784\u9020\u7684 MyLayer \u7c7b\u901a\u8fc7\u7ee7\u627f Module \u7c7b\u81ea\u5b9a\u4e49\uf9ba\u4e00\u4e2a**\u5c06\u8f93\u5165\u51cf\u6389\u5747\u503c\u540e\u8f93\u51fa**\u7684\u5c42\u3002\u8fd9\u4e2a\u5c42\uf9e9\uf967\u542b\u6a21\u578b\u53c2\u6570\u3002 >>> import torch >>> from torch import nn >>> >>> class MyLayer ( nn . Module ): ... def __init__ ( self , ** kwargs ): ... super ( MyLayer , self ) . __init__ ( ** kwargs ) ... def forward ( self , x ): ... return x - x . mean () ... >>> layer = MyLayer () # \u5b9e\u4f8b\u5316\u8be5\u5c42 >>> layer MyLayer () >>> layer ( torch . tensor ([ 1 , 2 , 3 , 4 , 5 ], dtype = torch . float )) tensor ([ - 2. , - 1. , 0. , 1. , 2. ])","title":"\u4e0d\u542b\u6a21\u578b\u53c2\u6570\u7684\u5c42"},{"location":"pytorch-chap03/#_8","text":"\u6211\u4eec\u8fd8\u53ef\u4ee5\u81ea\u5b9a\u4e49\u542b\u6a21\u578b\u53c2\u6570\u7684\u81ea\u5b9a\u4e49\u5c42\u3002\u5176\u4e2d\u7684\u6a21\u578b\u53c2\u6570\u53ef\u4ee5\u901a\u8fc7\u8bad\u7ec3\u5b66\u51fa\u3002 Parameter \u7c7b\u5176\u5b9e\u662f Tensor \u7684\u5b50\u7c7b\uff0c\u5982\u679c\u4e00 \u4e2a Tensor \u662f Parameter \uff0c\u90a3\u4e48\u5b83\u4f1a\u2f83\u52a8\u88ab\u6dfb\u52a0\u5230\u6a21\u578b\u7684\u53c2\u6570\uf99c\u8868\uf9e9\u3002\u6240\u4ee5\u5728\u2f83\u5b9a\u4e49\u542b\u6a21\u578b\u53c2\u6570\u7684\u5c42\u65f6\uff0c\u6211\u4eec\u5e94\u8be5\u5c06\u53c2\u6570\u5b9a\u4e49\u6210 Parameter \uff0c\u9664\u4e86\u76f4\u63a5\u5b9a\u4e49\u6210 Parameter \u7c7b\u5916\uff0c\u8fd8\u53ef\u4ee5\u4f7f\u2f64 ParameterList \u548c ParameterDict \u5206\u522b\u5b9a\u4e49\u53c2\u6570\u7684\uf99c\u8868\u548c\u5b57\u5178\u3002 class MyListDense ( nn . Module ): def __init__ ( self ): super ( MyListDense , self ) . __init__ () self . params = nn . ParameterList ([ nn . Parameter ( torch . randn ( 4 , 4 )) for i in range ( 3 )]) self . params . append ( nn . Parameter ( torch . randn ( 4 , 1 ))) def forward ( self , x ): for i in range ( len ( self . params )): x = torch . mm ( x , self . params [ i ]) return x >>> net = MyListDense () >>> print ( net ) MyListDense ( ( params ): ParameterList ( ( 0 ): Parameter containing : [ torch . FloatTensor of size 4 x4 ] ( 1 ): Parameter containing : [ torch . FloatTensor of size 4 x4 ] ( 2 ): Parameter containing : [ torch . FloatTensor of size 4 x4 ] ( 3 ): Parameter containing : [ torch . FloatTensor of size 4 x1 ] ) ) class MyDictDense ( nn . Module ): def __init__ ( self ): super ( MyDictDense , self ) . __init__ () self . params = nn . ParameterDict ({ 'linear1' : nn . Parameter ( torch . randn ( 4 , 4 )), 'linear2' : nn . Parameter ( torch . randn ( 4 , 1 )) }) self . params . update ({ 'linear3' : nn . Parameter ( torch . randn ( 4 , 2 ))}) # \u65b0\u589e def forward ( self , x , choice = 'linear1' ): return torch . mm ( x , self . params [ choice ]) >>> net = MyDictDense () >>> print ( net ) MyDictDense ( ( params ): ParameterDict ( ( linear1 ): Parameter containing : [ torch . FloatTensor of size 4 x4 ] ( linear2 ): Parameter containing : [ torch . FloatTensor of size 4 x1 ] ( linear3 ): Parameter containing : [ torch . FloatTensor of size 4 x2 ] ) ) \u4e0b\u9762\u7ed9\u51fa\u5e38\u89c1\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u4e00\u4e9b\u5c42\uff0c\u6bd4\u5982\u5377\u79ef\u5c42\u3001\u6c60\u5316\u5c42\uff0c\u4ee5\u53ca\u8f83\u4e3a\u57fa\u7840\u7684AlexNet\uff0cLeNet\u7b49\u3002","title":"\u542b\u6a21\u578b\u53c2\u6570\u7684\u5c42"},{"location":"pytorch-chap03/#_9","text":"\u4e8c\u7ef4\u5377\u79ef\u5c42\u5c06\u8f93\u5165\u548c\u5377\u79ef\u6838\u505a\u4e92\u76f8\u5173\u8fd0\u7b97\uff0c\u5e76\u52a0\u4e0a\u4e00\u4e2a\u6807\uf97e\u504f\u5dee\u6765\u5f97\u5230\u8f93\u51fa\u3002 import torch from torch import nn # \u5377\u79ef\u8fd0\u7b97\uff08\u4e8c\u7ef4\u4e92\u76f8\u5173\uff09 def corr2d ( X , K ): h , w = K . shape X , K = X . float (), K . float () Y = torch . zeros (( X . shape [ 0 ] - h + 1 , X . shape [ 1 ] - w + 1 )) for i in range ( Y . shape [ 0 ]): for j in range ( Y . shape [ 1 ]): Y [ i , j ] = ( X [ i : i + h , j : j + w ] * K ) . sum () return Y # \u4e8c\u7ef4\u5377\u79ef\u5c42 class Conv2D ( nn . Module ): def __init__ ( self , kernel_size ): super ( Conv2D , self ) . __init__ () self . weight = nn . Parameter ( torch . randn ( kernel_size )) self . bias = nn . Parameter ( torch . randn ( 1 )) def forward ( self , x ): return corr2d ( x , self . weight ) + self . bias \u586b\u5145(padding)\u662f\u6307\u5728\u8f93\u2f0a\u5165\u2fbc\u9ad8\u548c\u5bbd\u7684\u4e24\u4fa7\u586b\u5145\u5143\u7d20(\u901a\u5e38\u662f0\u5143\u7d20)\u3002 \u5728\u4e8c\u7ef4\u4e92\u76f8\u5173\u8fd0\u7b97\u4e2d\uff0c\u5377\u79ef\u7a97\u53e3\u4ece\u8f93\u5165\u6570\u7ec4\u7684\u6700\u5de6\u4e0a\u65b9\u5f00\u59cb\uff0c\u6309\u4ece\u5de6\u5f80\u53f3\u3001\u4ece\u4e0a\u5f80\u4e0b \u7684\u987a\u5e8f\uff0c\u4f9d\u6b21\u5728\u8f93\u2f0a\u6570\u7ec4\u4e0a\u6ed1\u52a8\u3002\u6211\u4eec\u5c06\u6bcf\u6b21\u6ed1\u52a8\u7684\ufa08\u6570\u548c\uf99c\u6570\u79f0\u4e3a\u6b65\u5e45(stride)\u3002 \uff08skip\uff09","title":"\u4e8c\u7ef4\u5377\u79ef\u5c42"},{"location":"pytorch-chap03/#_10","text":"\u6c60\u5316\u5c42\u6bcf\u6b21\u5bf9\u8f93\u5165\u6570\u636e\u7684\u4e00\u4e2a\u56fa\u5b9a\u5f62\u72b6\u7a97\u53e3(\u2f1c\u79f0\u6c60\u5316\u7a97\u53e3)\u4e2d\u7684\u5143\u7d20\u8ba1\u7b97\u8f93\u51fa\u3002\uf967\u540c\u4e8e\u5377\u79ef\u5c42\uf9e9\u8ba1\u7b97\u8f93\u2f0a\u548c\u6838\u7684\u4e92\u76f8\u5173\u6027\uff0c\u6c60\u5316\u5c42\u76f4\u63a5\u8ba1\u7b97\u6c60\u5316\u7a97\u53e3\u5185\u5143\u7d20\u7684\u6700\u5927\u503c\u6216\u8005\u5e73\u5747\u503c\u3002\u8be5\u8fd0\u7b97\u4e5f \u5206\u522b\u53eb\u505a\u6700\u5927\u6c60\u5316\u6216\u5e73\u5747\u6c60\u5316\u3002 >>> import numpy as np >>> import torch >>> from torch import nn >>> >>> def pool2d ( X , pool_size , mode = 'max' ): ... p_h , p_w = pool_size ... Y = np . zeros (( X . shape [ 0 ] - p_h + 1 , X . shape [ 1 ] - p_w + 1 )) ... for i in range ( Y . shape [ 0 ]): ... for j in range ( Y . shape [ 1 ]): ... if mode == 'max' : ... Y [ i , j ] = X [ i : i + p_h , j : j + p_w ] . max () ... elif mode == 'avg' : ... Y [ i , j ] = X [ i : i + p_h , j : j + p_w ] . mean () ... return Y ... >>> X = torch . tensor ([[ 0 , 1 , 2 ], [ 3 , 4 , 5 ], [ 6 , 7 , 8 ]]) >>> pool2d ( X , ( 2 , 2 )) array ([[ 4. , 5. ], [ 7. , 8. ]])","title":"\u6c60\u5316\u5c42"},{"location":"pytorch-chap03/#lenet","text":"\uff08\u5f85\u8865\u5145\uff09","title":"\u6a21\u578b\u793a\u4f8b\uff1aLeNet"},{"location":"pytorch-chap03/#alexnet","text":"\uff08\u5f85\u8865\u5145\uff09","title":"\u6a21\u578b\u793a\u4f8b\uff1aAlexNet"},{"location":"pytorch-chap03/#_11","text":"\u4e00\u4e2a\u597d\u7684\u8bad\u7ec3\u79bb\u4e0d\u5f00\u4f18\u8d28\u7684\u8d1f\u53cd\u9988\uff0c\u8fd9\u91cc\u7684\u635f\u5931\u51fd\u6570\u5c31\u662f\u6a21\u578b\u7684\u8d1f\u53cd\u9988\u3002 \u8fd9\u91cc\u5c06\u5217\u51faPyTorch\u4e2d\u5e38\u7528\u7684\u635f\u5931\u51fd\u6570\uff08\u4e00\u822c\u901a\u8fc7torch.nn\u8c03\u7528\uff09\uff0c\u5e76\u8be6\u7ec6\u4ecb\u7ecd\u6bcf\u4e2a\u635f\u5931\u51fd\u6570\u7684\u529f\u80fd\u4ecb\u7ecd\u3001\u6570\u5b66\u516c\u5f0f\u548c\u8c03\u7528\u4ee3\u7801\u3002","title":"\u635f\u5931\u51fd\u6570"},{"location":"pytorch-chap03/#_12","text":"torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean') \u529f\u80fd \uff1a\u8ba1\u7b97\u4e8c\u5206\u7c7b\u4efb\u52a1\u65f6\u7684\u4ea4\u53c9\u71b5\uff08Cross Entropy\uff09\u51fd\u6570\u3002\u5728\u4e8c\u5206\u7c7b\u4e2d\uff0clabel\u662f{0,1}\u3002\u5bf9\u4e8e\u8fdb\u5165\u4ea4\u53c9\u71b5\u51fd\u6570\u7684input\u4e3a\u6982\u7387\u5206\u5e03\u7684\u5f62\u5f0f\u3002\u4e00\u822c\u6765\u8bf4\uff0cinput\u4e3asigmoid\u6fc0\u6d3b\u5c42\u7684\u8f93\u51fa\uff0c\u6216\u8005softmax\u7684\u8f93\u51fa\u3002 \u4e3b\u8981\u53c2\u6570 \uff1a weight :\u6bcf\u4e2a\u7c7b\u522b\u7684loss\u8bbe\u7f6e\u6743\u503c size_average :\u6570\u636e\u4e3abool\uff0c\u4e3aTrue\u65f6\uff0c\u8fd4\u56de\u7684loss\u4e3a\u5e73\u5747\u503c\uff1b\u4e3aFalse\u65f6\uff0c\u8fd4\u56de\u7684\u5404\u6837\u672c\u7684loss\u4e4b\u548c. reduce :\u6570\u636e\u7c7b\u578b\u4e3abool\uff0c\u4e3aTrue\u65f6\uff0closs\u7684\u8fd4\u56de\u662f\u6807\u91cf\u3002","title":"\u4e8c\u5206\u7c7b\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570"},{"location":"pytorch-chap03/#_13","text":"\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570 L1\u635f\u5931\u51fd\u6570 MSE\u635f\u5931\u51fd\u6570 \u5e73\u6ed1L1 (Smooth L1)\u635f\u5931\u51fd\u6570 \u76ee\u6807\u6cca\u677e\u5206\u5e03\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931 KL\u6563\u5ea6","title":"\u5176\u4ed6\u635f\u5931\u51fd\u6570"},{"location":"pytorch-chap03/#_14","text":"","title":"\u4f18\u5316\u5668"},{"location":"pytorch-chap03/#_15","text":"\u6df1\u5ea6\u5b66\u4e60\u7684\u76ee\u6807\u662f\u901a\u8fc7\u4e0d\u65ad\u6539\u53d8\u7f51\u7edc\u53c2\u6570\uff0c\u4f7f\u5f97\u53c2\u6570\u80fd\u591f\u5bf9\u8f93\u5165\u505a\u5404\u79cd\u975e\u7ebf\u6027\u53d8\u6362\u62df\u5408\u8f93\u51fa\uff0c\u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u4e2a\u51fd\u6570\u53bb\u5bfb\u627e\u6700\u4f18\u89e3\uff0c\u53ea\u4e0d\u8fc7\u8fd9\u4e2a\u6700\u4f18\u89e3\u4f7f\u4e00\u4e2a\u77e9\u9635\u3002\u90a3\u4e48\u6211\u4eec\u5982\u4f55\u8ba1\u7b97\u51fa\u6765\u8fd9\u4e48\u591a\u7684\u7cfb\u6570\uff0c\u6709\u4ee5\u4e0b\u4e24\u79cd\u65b9\u6cd5\uff1a \u7b2c\u4e00\u79cd\u662f\u6700\u76f4\u63a5\u7684\u66b4\u529b\u7a77\u4e3e\u4e00\u904d\u53c2\u6570\uff0c\u8fd9\u79cd\u65b9\u6cd5\u7684\u5b9e\u65bd\u53ef\u80fd\u6027\u57fa\u672c\u4e3a0\uff0c\u582a\u6bd4\u611a\u516c\u79fb\u5c71plus\u7684\u96be\u5ea6\u3002 \u4e3a\u4e86\u4f7f\u6c42\u89e3\u53c2\u6570\u8fc7\u7a0b\u66f4\u52a0\u5feb\uff0c\u4eba\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e8c\u79cd\u529e\u6cd5\uff0c\u5373\u5c31\u662f\u662fBP+\u4f18\u5316\u5668\u903c\u8fd1\u6c42\u89e3\u3002 \u56e0\u6b64\uff0c\u4f18\u5316\u5668\u5c31\u662f\u6839\u636e\u7f51\u7edc\u53cd\u5411\u4f20\u64ad\u7684\u68af\u5ea6\u4fe1\u606f\u6765\u66f4\u65b0\u7f51\u7edc\u7684\u53c2\u6570\uff0c\u4ee5\u8d77\u5230\u964d\u4f4eloss\u51fd\u6570\u8ba1\u7b97\u503c\uff0c\u4f7f\u5f97\u6a21\u578b\u8f93\u51fa\u66f4\u52a0\u63a5\u8fd1\u771f\u5b9e\u6807\u7b7e\u3002","title":"\u4ec0\u4e48\u662f\u4f18\u5316\u5668"},{"location":"pytorch-chap03/#pytorch_1","text":"Pytorch\u5f88\u4eba\u6027\u5316\u7684\u7ed9\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f18\u5316\u5668\u7684\u5e93torch.optim\uff0c\u5728\u8fd9\u91cc\u9762\u7ed9\u6211\u4eec\u63d0\u4f9b\u4e86\u5341\u79cd\u4f18\u5316\u5668\u3002 torch.optim.ASGD torch.optim.Adadelta torch.optim.Adagrad torch.optim.Adam torch.optim.AdamW torch.optim.Adamax torch.optim.LBFGS torch.optim.RMSprop torch.optim.Rprop torch.optim.SGD torch.optim.SparseAdam","title":"PyTorch\u63d0\u4f9b\u7684\u4f18\u5316\u5668"},{"location":"pytorch-chap03/#_16","text":"\u5b8c\u6210\u4e86\u4e0a\u8ff0\u8bbe\u5b9a\u540e\u5c31\u53ef\u4ee5\u52a0\u8f7d\u6570\u636e\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b\u4e86\u3002\u9996\u5148\u5e94\u8be5\u8bbe\u7f6e\u6a21\u578b\u7684\u72b6\u6001\uff1a\u5982\u679c\u662f\u8bad\u7ec3\u72b6\u6001\uff0c\u90a3\u4e48\u6a21\u578b\u7684\u53c2\u6570\u5e94\u8be5\u652f\u6301\u53cd\u5411\u4f20\u64ad\u7684\u4fee\u6539\uff1b\u5982\u679c\u662f\u9a8c\u8bc1/\u6d4b\u8bd5\u72b6\u6001\uff0c\u5219\u4e0d\u5e94\u8be5\u4fee\u6539\u6a21\u578b\u53c2\u6570\u3002 model . train () # \u8bad\u7ec3\u72b6\u6001 model . eval () # \u9a8c\u8bc1/\u6d4b\u8bd5\u72b6\u6001 \u8bad\u7ec3\u8fc7\u7a0b\uff1a def train ( epoch ): model . train () train_loss = 0 for data , label in train_loader : # \u6b64\u65f6\u8981\u7528for\u5faa\u73af\u8bfb\u53d6DataLoader\u4e2d\u7684\u5168\u90e8\u6570\u636e\u3002 data , label = data . cuda (), label . cuda () # \u4e4b\u540e\u5c06\u6570\u636e\u653e\u5230GPU\u4e0a\u7528\u4e8e\u540e\u7eed\u8ba1\u7b97\uff0c\u6b64\u5904\u4ee5.cuda()\u4e3a\u4f8b optimizer . zero_grad () # \u5f00\u59cb\u7528\u5f53\u524d\u6279\u6b21\u6570\u636e\u505a\u8bad\u7ec3\u65f6\uff0c\u5e94\u5f53\u5148\u5c06\u4f18\u5316\u5668\u7684\u68af\u5ea6\u7f6e\u96f6 output = model ( data ) # \u4e4b\u540e\u5c06data\u9001\u5165\u6a21\u578b\u4e2d\u8bad\u7ec3 loss = criterion ( label , output ) # \u6839\u636e\u9884\u5148\u5b9a\u4e49\u7684criterion\u8ba1\u7b97\u635f\u5931\u51fd\u6570 loss . backward () # \u5c06loss\u53cd\u5411\u4f20\u64ad\u56de\u7f51\u7edc optimizer . step () # \u4f7f\u7528\u4f18\u5316\u5668\u66f4\u65b0\u6a21\u578b\u53c2\u6570 train_loss += loss . item () * data . size ( 0 ) train_loss = train_loss / len ( train_loader . dataset ) print ( 'Epoch: {} \\t Training Loss: {:.6f} ' . format ( epoch , train_loss )) \u9a8c\u8bc1/\u6d4b\u8bd5\u7684\u6d41\u7a0b\u57fa\u672c\u4e0e\u8bad\u7ec3\u8fc7\u7a0b\u4e00\u81f4\uff0c\u4e0d\u540c\u70b9\u5728\u4e8e\uff1a \u9700\u8981\u9884\u5148\u8bbe\u7f6etorch.no_grad\uff0c\u4ee5\u53ca\u5c06model\u8c03\u81f3eval\u6a21\u5f0f \u4e0d\u9700\u8981\u5c06\u4f18\u5316\u5668\u7684\u68af\u5ea6\u7f6e\u96f6 \u4e0d\u9700\u8981\u5c06loss\u53cd\u5411\u56de\u4f20\u5230\u7f51\u7edc \u4e0d\u9700\u8981\u66f4\u65b0optimizer \u9a8c\u8bc1/\u6d4b\u8bd5\u8fc7\u7a0b\uff1a def val ( epoch ): model . eval () val_loss = 0 with torch . no_grad (): for data , label in val_loader : data , label = data . cuda (), label . cuda () output = model ( data ) preds = torch . argmax ( output , 1 ) loss = criterion ( output , label ) val_loss += loss . item () * data . size ( 0 ) running_accu += torch . sum ( preds == label . data ) val_loss = val_loss / len ( val_loader . dataset ) print ( 'Epoch: {} \\t Training Loss: {:.6f} ' . format ( epoch , val_loss ))","title":"\u8bad\u7ec3\u4e0e\u8bc4\u4f30"},{"location":"pytorch-chap03/#_17","text":"\u5728PyTorch\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c\u53ef\u89c6\u5316\u662f\u4e00\u4e2a\u53ef\u9009\u9879\uff0c\u6307\u7684\u662f\u67d0\u4e9b\u4efb\u52a1\u5728\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u9700\u8981\u5bf9\u4e00\u4e9b\u5fc5\u8981\u7684\u5185\u5bb9\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u6bd4\u5982\u5206\u7c7b\u7684ROC\u66f2\u7ebf\uff0c\u5377\u79ef\u7f51\u7edc\u4e2d\u7684\u5377\u79ef\u6838\uff0c\u4ee5\u53ca\u8bad\u7ec3/\u9a8c\u8bc1\u8fc7\u7a0b\u7684\u635f\u5931\u51fd\u6570\u66f2\u7ebf\u7b49\u7b49\u3002","title":"\u53ef\u89c6\u5316"},{"location":"pytorch-chap03/#_18","text":"Datawhale\u5f00\u6e90\u9879\u76ee\uff1a\u6df1\u5165\u6d45\u51faPyTorch https://github.com/datawhalechina/thorough-pytorch/ \u674e\u5b8f\u6bc5\u673a\u5668\u5b66\u4e602021\u6625-PyTorch Tutorial https://www.bilibili.com/video/BV1Wv411h7kN?p=5 \u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60pytorch\u7248 https://zh-v2.d2l.ai/chapter_preface/index.html PyTorch\u5b98\u65b9\u6559\u7a0b\u4e2d\u6587\u7248 https://pytorch123.com/SecondSection/training_a_classifier/","title":"\u53c2\u8003\u8d44\u6599"},{"location":"pytorch-chap04/","text":"\u7b2c\u56db\u7ae0 PyTorch\u57fa\u7840\u5b9e\u6218\u2014\u2014FashionMNIST\u56fe\u50cf\u5206\u7c7b \u00b6 \u6570\u636e\u96c6\u548c\u4efb\u52a1\u4ecb\u7ecd \u00b6 \u6211\u4eec\u8fd9\u91cc\u7684\u4efb\u52a1\u662f\u5bf910\u4e2a\u7c7b\u522b\u7684\u201c\u65f6\u88c5\u201d\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\uff0c\u4f7f\u7528FashionMNIST\u6570\u636e\u96c6\u3002 FashionMNIST\u6570\u636e\u96c6\u4e2d\u5305\u542b\u5df2\u7ecf\u9884\u5148\u5212\u5206\u597d\u7684\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u5176\u4e2d\u8bad\u7ec3\u96c6\u517160,000\u5f20\u56fe\u50cf\uff0c\u6d4b\u8bd5\u96c6\u517110,000\u5f20\u56fe\u50cf\u3002\u6bcf\u5f20\u56fe\u50cf\u5747\u4e3a\u5355\u901a\u9053\u9ed1\u767d\u56fe\u50cf\uff0c\u5927\u5c0f\u4e3a32*32pixel\uff0c\u5206\u5c5e10\u4e2a\u7c7b\u522b\u3002 \u5bfc\u5165\u5fc5\u8981\u7684\u5305 \u00b6 import os import numpy as np import pandas as pd import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset , DataLoader \u914d\u7f6e\u8bad\u7ec3\u73af\u5883\u548c\u8d85\u53c2\u6570 \u00b6 # \u914d\u7f6eGPU\uff0c\u8fd9\u91cc\u6709\u4e24\u79cd\u65b9\u5f0f ## \u65b9\u6848\u4e00\uff1a\u4f7f\u7528os.environ # os.environ['CUDA_VISIBLE_DEVICES'] = '0' # \u65b9\u6848\u4e8c\uff1a\u4f7f\u7528\u201cdevice\u201d\uff0c\u540e\u7eed\u5bf9\u8981\u4f7f\u7528GPU\u7684\u53d8\u91cf\u7528.to(device)\u5373\u53ef # device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\") ## \u914d\u7f6e\u5176\u4ed6\u8d85\u53c2\u6570\uff0c\u5982batch_size, num_workers, learning rate, \u4ee5\u53ca\u603b\u7684epochs batch_size = 256 num_workers = 4 lr = 1e-4 epochs = 20 \u6570\u636e\u8bfb\u5165\u548c\u52a0\u8f7d \u00b6 \u6570\u636e\u8bfb\u5165\u6709\u4e24\u79cd\u65b9\u5f0f: \u4e0b\u8f7d\u5e76\u4f7f\u7528PyTorch\u63d0\u4f9b\u7684\u5185\u7f6e\u6570\u636e\u96c6\u3002\u8fd9\u79cd\u65b9\u5f0f\u53ea\u9002\u7528\u4e8e\u5e38\u89c1\u7684\u6570\u636e\u96c6\uff0c\u5982MNIST\uff0cCIFAR10\u7b49\uff0cPyTorch\u5b98\u65b9\u63d0\u4f9b\u4e86\u6570\u636e\u4e0b\u8f7d\u3002\u8fd9\u79cd\u65b9\u5f0f\u5f80\u5f80\u9002\u7528\u4e8e\u5feb\u901f\u6d4b\u8bd5\u65b9\u6cd5\uff08\u6bd4\u5982\u6d4b\u8bd5\u4e0b\u67d0\u4e2aidea\u5728MNIST\u6570\u636e\u96c6\u4e0a\u662f\u5426\u6709\u6548\uff09 \u4ece\u7f51\u7ad9\u4e0b\u8f7d\u4ee5csv\u683c\u5f0f\u5b58\u50a8\u7684\u6570\u636e\uff0c\u8bfb\u5165\u5e76\u8f6c\u6210\u9884\u671f\u7684\u683c\u5f0f\u3002\u8fd9\u79cd\u6570\u636e\u8bfb\u5165\u65b9\u5f0f\u9700\u8981\u81ea\u5df1\u6784\u5efaDataset\uff0c\u8fd9\u5bf9\u4e8ePyTorch\u5e94\u7528\u4e8e\u81ea\u5df1\u7684\u5de5\u4f5c\u4e2d\u5341\u5206\u91cd\u8981 \u540c\u65f6\uff0c\u8fd8\u9700\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u5fc5\u8981\u7684\u53d8\u6362\uff0c\u6bd4\u5982\u8bf4\u9700\u8981\u5c06\u56fe\u7247\u7edf\u4e00\u4e3a\u4e00\u81f4\u7684\u5927\u5c0f\uff0c\u4ee5\u4fbf\u540e\u7eed\u80fd\u591f\u8f93\u5165\u7f51\u7edc\u8bad\u7ec3\uff1b\u9700\u8981\u5c06\u6570\u636e\u683c\u5f0f\u8f6c\u4e3aTensor\u7c7b\uff0c\u7b49\u7b49\u3002\u8fd9\u4e9b\u53d8\u6362\u53ef\u4ee5\u5f88\u65b9\u4fbf\u5730\u501f\u52a9torchvision\u5305\u6765\u5b8c\u6210\uff0ctorchvision\u8fd9\u662fPyTorch\u5b98\u65b9\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u7684\u5de5\u5177\u5e93\u3002 # \u9996\u5148\u8bbe\u7f6e\u6570\u636e\u53d8\u6362 from torchvision import transforms image_size = 28 data_transform = transforms . Compose ([ transforms . ToPILImage (), # \u8fd9\u4e00\u6b65\u53d6\u51b3\u4e8e\u540e\u7eed\u7684\u6570\u636e\u8bfb\u53d6\u65b9\u5f0f\uff0c\u5982\u679c\u4f7f\u7528\u5185\u7f6e\u6570\u636e\u96c6\u5219\u4e0d\u9700\u8981 transforms . Resize ( image_size ), transforms . ToTensor () ]) \u8bfb\u53d6\u65b9\u5f0f\u4e00\uff1a ## \u8bfb\u53d6\u65b9\u5f0f\u4e00\uff1a\u4f7f\u7528torchvision\u81ea\u5e26\u6570\u636e\u96c6\uff0c\u4e0b\u8f7d\u53ef\u80fd\u9700\u8981\u4e00\u6bb5\u65f6\u95f4 from torchvision import datasets train_data = datasets . FashionMNIST ( root = './' , train = True , download = True , transform = data_transform ) test_data = datasets . FashionMNIST ( root = './' , train = False , download = True , transform = data_transform ) \u8bfb\u53d6\u65b9\u5f0f\u4e8c\uff1a ## \u8bfb\u53d6\u65b9\u5f0f\u4e8c\uff1a\u8bfb\u5165csv\u683c\u5f0f\u7684\u6570\u636e\uff0c\u81ea\u884c\u6784\u5efaDataset\u7c7b class FMDataset ( Dataset ): def __init__ ( self , df , transform = None ): self . df = df self . transform = transform self . images = df . iloc [:, 1 :] . values . astype ( np . uint8 ) self . labels = df . iloc [:, 0 ] . values def __len__ ( self ): return len ( self . images ) def __getitem__ ( self , idx ): image = self . images [ idx ] . reshape ( 28 , 28 , 1 ) label = int ( self . labels [ idx ]) if self . transform is not None : image = self . transform ( image ) else : image = torch . tensor ( image / 255. , dtype = torch . float ) label = torch . tensor ( label , dtype = torch . long ) return image , label train_df = pd . read_csv ( \"./FashionMNIST/fashion-mnist_train.csv\" ) test_df = pd . read_csv ( \"./FashionMNIST/fashion-mnist_test.csv\" ) train_data = FMDataset ( train_df , data_transform ) test_data = FMDataset ( test_df , data_transform ) \u6ce8\u610f\uff1a\u8fd9\u91cc\u9700\u8981\u81ea\u5df1\u4e0b\u8f7d\u6570\u636e\u3002\u53ef\u4ee5\u4ecekaggle\u4e0a\u4e0b\u8f7d\uff08\u9700\u79d1\u5b66\u4e0a\u7f51\uff09\uff08\u4f46\u8c8c\u4f3c\u4e5f\u4e0d\u662f\u6559\u7a0b\u7528\u7684\u7248\u672c\uff09\uff1a https://www.kaggle.com/zalando-research/fashionmnist/ # \u5b9a\u4e49DataLoader\u7c7b\uff0c\u4ee5\u4fbf\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u52a0\u8f7d\u6570\u636e train_loader = DataLoader ( train_data , batch_size = batch_size , shuffle = True , num_workers = num_workers , drop_last = True ) test_loader = DataLoader ( test_data , batch_size = batch_size , shuffle = False , num_workers = num_workers ) # \u6570\u636e\u53ef\u89c6\u5316 import matplotlib.pyplot as plt image , label = next ( iter ( test_loader )) print ( image . shape , label . shape ) plt . imshow ( image [ 0 ][ 0 ], cmap = \"gray\" ) \u8fd9\u91cc\u7a0b\u5e8f\u8fd0\u884c\u4e86\u5f88\u4e45\uff0c\u4e00\u76f4\u8dd1\u4e0d\u51fa\u7ed3\u679c\uff0c\u6539\u7528\u4e86colab \u6a21\u578b\u8bbe\u8ba1 \u00b6 \u624b\u642d\u4e00\u4e2aCNN class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv = nn . Sequential ( nn . Conv2d ( 1 , 32 , 5 ), nn . ReLU (), nn . MaxPool2d ( 2 , stride = 2 ), nn . Dropout ( 0.3 ), nn . Conv2d ( 32 , 64 , 5 ), nn . ReLU (), nn . MaxPool2d ( 2 , stride = 2 ), nn . Dropout ( 0.3 ) ) self . fc = nn . Sequential ( nn . Linear ( 64 * 4 * 4 , 512 ), nn . ReLU (), nn . Linear ( 512 , 10 ) ) def forward ( self , x ): x = self . conv ( x ) x = x . view ( - 1 , 64 * 4 * 4 ) x = self . fc ( x ) # x = nn.functional.normalize(x) return x model = Net () # model = model.cuda() # \u5c06\u6a21\u578b\u653e\u5230GPU\u4e0a\u7528\u4e8e\u8bad\u7ec3 # model = nn.DataParallel(model).cuda() # \u591a\u5361\u8bad\u7ec3\u65f6\u7684\u5199\u6cd5\uff0c\u4e4b\u540e\u7684\u8bfe\u7a0b\u4e2d\u4f1a\u8fdb\u4e00\u6b65\u8bb2\u89e3 \u8bbe\u5b9a\u635f\u5931\u51fd\u6570 \u00b6 \u4f7f\u7528torch.nn\u6a21\u5757\u81ea\u5e26\u7684CrossEntropy\u635f\u5931\u3002 PyTorch\u4f1a\u81ea\u52a8\u628a\u6574\u6570\u578b\u7684label\u8f6c\u4e3aone-hot\u578b\uff0c\u7528\u4e8e\u8ba1\u7b97CE loss\u3002 \u8fd9\u91cc\u9700\u8981\u786e\u4fddlabel\u662f\u4ece0\u5f00\u59cb\u7684\uff0c\u540c\u65f6\u6a21\u578b\u4e0d\u52a0softmax\u5c42\uff08\u4f7f\u7528logits\u8ba1\u7b97\uff09,\u8fd9\u4e5f\u8bf4\u660e\u4e86PyTorch\u8bad\u7ec3\u4e2d\u5404\u4e2a\u90e8\u5206\u4e0d\u662f\u72ec\u7acb\u7684\uff0c\u9700\u8981\u901a\u76d8\u8003\u8651\u3002 criterion = nn . CrossEntropyLoss () # criterion = nn.CrossEntropyLoss(weight=[1,1,1,1,3,1,1,1,1,1]) \u8bbe\u5b9a\u4f18\u5316\u5668 \u00b6 optimizer = optim . Adam ( model . parameters (), lr = 0.001 ) \u8bad\u7ec3\u548c\u6d4b\u8bd5 \u00b6 \u8bad\u7ec3\u548c\u6d4b\u8bd5\uff08\u9a8c\u8bc1\uff09 \u5404\u81ea\u5c01\u88c5\u6210\u51fd\u6570\uff0c\u65b9\u4fbf\u540e\u7eed\u8c03\u7528 \u5173\u6ce8\u4e24\u8005\u7684\u4e3b\u8981\u533a\u522b\uff1a \u6a21\u578b\u72b6\u6001\u8bbe\u7f6e \u662f\u5426\u9700\u8981\u521d\u59cb\u5316\u4f18\u5316\u5668 \u662f\u5426\u9700\u8981\u5c06loss\u4f20\u56de\u5230\u7f51\u7edc \u662f\u5426\u9700\u8981\u6bcf\u6b65\u66f4\u65b0optimizer \u6b64\u5916\uff0c\u5bf9\u4e8e\u6d4b\u8bd5\u6216\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u8ba1\u7b97\u5206\u7c7b\u51c6\u786e\u7387 def train ( epoch ): model . train () train_loss = 0 for data , label in train_loader : # data, label = data.cuda(), label.cuda() # \u4e0d\u7528cuda\u5148 optimizer . zero_grad () output = model ( data ) loss = criterion ( output , label ) loss . backward () optimizer . step () train_loss += loss . item () * data . size ( 0 ) train_loss = train_loss / len ( train_loader . dataset ) print ( 'Epoch: {} \\t Training Loss: {:.6f} ' . format ( epoch , train_loss )) def val ( epoch ): model . eval () val_loss = 0 gt_labels = [] pred_labels = [] with torch . no_grad (): for data , label in test_loader : # data, label = data.cuda(), label.cuda() # \u4e0d\u7528cuda\u5148 output = model ( data ) preds = torch . argmax ( output , 1 ) gt_labels . append ( label . cpu () . data . numpy ()) pred_labels . append ( preds . cpu () . data . numpy ()) loss = criterion ( output , label ) val_loss += loss . item () * data . size ( 0 ) val_loss = val_loss / len ( test_loader . dataset ) gt_labels , pred_labels = np . concatenate ( gt_labels ), np . concatenate ( pred_labels ) acc = np . sum ( gt_labels == pred_labels ) / len ( pred_labels ) print ( 'Epoch: {} \\t Validation Loss: {:.6f} , Accuracy: {:6f} ' . format ( epoch , val_loss , acc )) for epoch in range ( 1 , epochs + 1 ): train ( epoch ) val ( epoch ) \u7ed3\u679c\uff08\u4e0d\u662f\u5f88\u597d\uff09\uff1a / usr / local / lib / python3 .7 / dist - packages / torch / utils / data / dataloader . py : 481 : UserWarning : This DataLoader will create 4 worker processes in total . Our suggested max number of worker in current system is 2 , which is smaller than what this DataLoader is going to create . Please be aware that excessive worker creation might get DataLoader running slow or even freeze , lower the worker number to avoid potential slowness / freeze if necessary . cpuset_checked )) / usr / local / lib / python3 .7 / dist - packages / torch / nn / functional . py : 718 : UserWarning : Named tensors and all their associated APIs are an experimental feature and subject to change . Please do not use them for anything important until they are released as stable . ( Triggered internally at / pytorch / c10 / core / TensorImpl . h : 1156. ) return torch . max_pool2d ( input , kernel_size , stride , padding , dilation , ceil_mode ) Epoch : 1 Training Loss : 1.859782 Epoch : 1 Validation Loss : 1.252422 , Accuracy : 0.504242 Epoch : 2 Training Loss : 1.073511 Epoch : 2 Validation Loss : 0.958262 , Accuracy : 0.620891 Epoch : 3 Training Loss : 0.912065 Epoch : 3 Validation Loss : 0.859967 , Accuracy : 0.682927 Epoch : 4 Training Loss : 0.803673 Epoch : 4 Validation Loss : 0.725328 , Accuracy : 0.743902 Epoch : 5 Training Loss : 0.723244 Epoch : 5 Validation Loss : 0.699738 , Accuracy : 0.725345 Epoch : 6 Training Loss : 0.676728 Epoch : 6 Validation Loss : 0.688325 , Accuracy : 0.742312 Epoch : 7 Training Loss : 0.624213 Epoch : 7 Validation Loss : 0.633743 , Accuracy : 0.744963 Epoch : 8 Training Loss : 0.595873 Epoch : 8 Validation Loss : 0.588029 , Accuracy : 0.770414 Epoch : 9 Training Loss : 0.561574 Epoch : 9 Validation Loss : 0.578903 , Accuracy : 0.765642 Epoch : 10 Training Loss : 0.544152 Epoch : 10 Validation Loss : 0.563249 , Accuracy : 0.791622 Epoch : 11 Training Loss : 0.532662 Epoch : 11 Validation Loss : 0.561163 , Accuracy : 0.790032 Epoch : 12 Training Loss : 0.520769 Epoch : 12 Validation Loss : 0.560051 , Accuracy : 0.783139 Epoch : 13 Training Loss : 0.495388 Epoch : 13 Validation Loss : 0.537520 , Accuracy : 0.794804 Epoch : 14 Training Loss : 0.461928 Epoch : 14 Validation Loss : 0.533855 , Accuracy : 0.799046 Epoch : 15 Training Loss : 0.453786 Epoch : 15 Validation Loss : 0.534338 , Accuracy : 0.805408 Epoch : 16 Training Loss : 0.457692 Epoch : 16 Validation Loss : 0.515626 , Accuracy : 0.812831 Epoch : 17 Training Loss : 0.449596 Epoch : 17 Validation Loss : 0.504590 , Accuracy : 0.816013 Epoch : 18 Training Loss : 0.443980 Epoch : 18 Validation Loss : 0.503526 , Accuracy : 0.818134 Epoch : 19 Training Loss : 0.420621 Epoch : 19 Validation Loss : 0.488520 , Accuracy : 0.826087 Epoch : 20 Training Loss : 0.418917 Epoch : 20 Validation Loss : 0.524965 , Accuracy : 0.797985 \u53c2\u8003\u8d44\u6599 \u00b6 \u7a0b\u5e8f\u7684colab\u94fe\u63a5\uff1a https://colab.research.google.com/drive/1kvaBEEgQ_a5G5xOHUe5ih4Qq8L5C9zYY?usp=sharing Datawhale\u5f00\u6e90\u9879\u76ee\uff1a\u6df1\u5165\u6d45\u51faPyTorch https://github.com/datawhalechina/thorough-pytorch/ \u674e\u5b8f\u6bc5\u673a\u5668\u5b66\u4e602021\u6625-PyTorch Tutorial https://www.bilibili.com/video/BV1Wv411h7kN?p=5 \u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60pytorch\u7248 https://zh-v2.d2l.ai/chapter_preface/index.html PyTorch\u5b98\u65b9\u6559\u7a0b\u4e2d\u6587\u7248 https://pytorch123.com/SecondSection/training_a_classifier/","title":"Chapter04 PyTorch\u57fa\u7840\u5b9e\u6218\u2014\u2014FashionMNIST\u56fe\u50cf\u5206\u7c7b"},{"location":"pytorch-chap04/#pytorchfashionmnist","text":"","title":"\u7b2c\u56db\u7ae0 PyTorch\u57fa\u7840\u5b9e\u6218\u2014\u2014FashionMNIST\u56fe\u50cf\u5206\u7c7b"},{"location":"pytorch-chap04/#_1","text":"\u6211\u4eec\u8fd9\u91cc\u7684\u4efb\u52a1\u662f\u5bf910\u4e2a\u7c7b\u522b\u7684\u201c\u65f6\u88c5\u201d\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\uff0c\u4f7f\u7528FashionMNIST\u6570\u636e\u96c6\u3002 FashionMNIST\u6570\u636e\u96c6\u4e2d\u5305\u542b\u5df2\u7ecf\u9884\u5148\u5212\u5206\u597d\u7684\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u5176\u4e2d\u8bad\u7ec3\u96c6\u517160,000\u5f20\u56fe\u50cf\uff0c\u6d4b\u8bd5\u96c6\u517110,000\u5f20\u56fe\u50cf\u3002\u6bcf\u5f20\u56fe\u50cf\u5747\u4e3a\u5355\u901a\u9053\u9ed1\u767d\u56fe\u50cf\uff0c\u5927\u5c0f\u4e3a32*32pixel\uff0c\u5206\u5c5e10\u4e2a\u7c7b\u522b\u3002","title":"\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4ecb\u7ecd"},{"location":"pytorch-chap04/#_2","text":"import os import numpy as np import pandas as pd import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset , DataLoader","title":"\u5bfc\u5165\u5fc5\u8981\u7684\u5305"},{"location":"pytorch-chap04/#_3","text":"# \u914d\u7f6eGPU\uff0c\u8fd9\u91cc\u6709\u4e24\u79cd\u65b9\u5f0f ## \u65b9\u6848\u4e00\uff1a\u4f7f\u7528os.environ # os.environ['CUDA_VISIBLE_DEVICES'] = '0' # \u65b9\u6848\u4e8c\uff1a\u4f7f\u7528\u201cdevice\u201d\uff0c\u540e\u7eed\u5bf9\u8981\u4f7f\u7528GPU\u7684\u53d8\u91cf\u7528.to(device)\u5373\u53ef # device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\") ## \u914d\u7f6e\u5176\u4ed6\u8d85\u53c2\u6570\uff0c\u5982batch_size, num_workers, learning rate, \u4ee5\u53ca\u603b\u7684epochs batch_size = 256 num_workers = 4 lr = 1e-4 epochs = 20","title":"\u914d\u7f6e\u8bad\u7ec3\u73af\u5883\u548c\u8d85\u53c2\u6570"},{"location":"pytorch-chap04/#_4","text":"\u6570\u636e\u8bfb\u5165\u6709\u4e24\u79cd\u65b9\u5f0f: \u4e0b\u8f7d\u5e76\u4f7f\u7528PyTorch\u63d0\u4f9b\u7684\u5185\u7f6e\u6570\u636e\u96c6\u3002\u8fd9\u79cd\u65b9\u5f0f\u53ea\u9002\u7528\u4e8e\u5e38\u89c1\u7684\u6570\u636e\u96c6\uff0c\u5982MNIST\uff0cCIFAR10\u7b49\uff0cPyTorch\u5b98\u65b9\u63d0\u4f9b\u4e86\u6570\u636e\u4e0b\u8f7d\u3002\u8fd9\u79cd\u65b9\u5f0f\u5f80\u5f80\u9002\u7528\u4e8e\u5feb\u901f\u6d4b\u8bd5\u65b9\u6cd5\uff08\u6bd4\u5982\u6d4b\u8bd5\u4e0b\u67d0\u4e2aidea\u5728MNIST\u6570\u636e\u96c6\u4e0a\u662f\u5426\u6709\u6548\uff09 \u4ece\u7f51\u7ad9\u4e0b\u8f7d\u4ee5csv\u683c\u5f0f\u5b58\u50a8\u7684\u6570\u636e\uff0c\u8bfb\u5165\u5e76\u8f6c\u6210\u9884\u671f\u7684\u683c\u5f0f\u3002\u8fd9\u79cd\u6570\u636e\u8bfb\u5165\u65b9\u5f0f\u9700\u8981\u81ea\u5df1\u6784\u5efaDataset\uff0c\u8fd9\u5bf9\u4e8ePyTorch\u5e94\u7528\u4e8e\u81ea\u5df1\u7684\u5de5\u4f5c\u4e2d\u5341\u5206\u91cd\u8981 \u540c\u65f6\uff0c\u8fd8\u9700\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u5fc5\u8981\u7684\u53d8\u6362\uff0c\u6bd4\u5982\u8bf4\u9700\u8981\u5c06\u56fe\u7247\u7edf\u4e00\u4e3a\u4e00\u81f4\u7684\u5927\u5c0f\uff0c\u4ee5\u4fbf\u540e\u7eed\u80fd\u591f\u8f93\u5165\u7f51\u7edc\u8bad\u7ec3\uff1b\u9700\u8981\u5c06\u6570\u636e\u683c\u5f0f\u8f6c\u4e3aTensor\u7c7b\uff0c\u7b49\u7b49\u3002\u8fd9\u4e9b\u53d8\u6362\u53ef\u4ee5\u5f88\u65b9\u4fbf\u5730\u501f\u52a9torchvision\u5305\u6765\u5b8c\u6210\uff0ctorchvision\u8fd9\u662fPyTorch\u5b98\u65b9\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u7684\u5de5\u5177\u5e93\u3002 # \u9996\u5148\u8bbe\u7f6e\u6570\u636e\u53d8\u6362 from torchvision import transforms image_size = 28 data_transform = transforms . Compose ([ transforms . ToPILImage (), # \u8fd9\u4e00\u6b65\u53d6\u51b3\u4e8e\u540e\u7eed\u7684\u6570\u636e\u8bfb\u53d6\u65b9\u5f0f\uff0c\u5982\u679c\u4f7f\u7528\u5185\u7f6e\u6570\u636e\u96c6\u5219\u4e0d\u9700\u8981 transforms . Resize ( image_size ), transforms . ToTensor () ]) \u8bfb\u53d6\u65b9\u5f0f\u4e00\uff1a ## \u8bfb\u53d6\u65b9\u5f0f\u4e00\uff1a\u4f7f\u7528torchvision\u81ea\u5e26\u6570\u636e\u96c6\uff0c\u4e0b\u8f7d\u53ef\u80fd\u9700\u8981\u4e00\u6bb5\u65f6\u95f4 from torchvision import datasets train_data = datasets . FashionMNIST ( root = './' , train = True , download = True , transform = data_transform ) test_data = datasets . FashionMNIST ( root = './' , train = False , download = True , transform = data_transform ) \u8bfb\u53d6\u65b9\u5f0f\u4e8c\uff1a ## \u8bfb\u53d6\u65b9\u5f0f\u4e8c\uff1a\u8bfb\u5165csv\u683c\u5f0f\u7684\u6570\u636e\uff0c\u81ea\u884c\u6784\u5efaDataset\u7c7b class FMDataset ( Dataset ): def __init__ ( self , df , transform = None ): self . df = df self . transform = transform self . images = df . iloc [:, 1 :] . values . astype ( np . uint8 ) self . labels = df . iloc [:, 0 ] . values def __len__ ( self ): return len ( self . images ) def __getitem__ ( self , idx ): image = self . images [ idx ] . reshape ( 28 , 28 , 1 ) label = int ( self . labels [ idx ]) if self . transform is not None : image = self . transform ( image ) else : image = torch . tensor ( image / 255. , dtype = torch . float ) label = torch . tensor ( label , dtype = torch . long ) return image , label train_df = pd . read_csv ( \"./FashionMNIST/fashion-mnist_train.csv\" ) test_df = pd . read_csv ( \"./FashionMNIST/fashion-mnist_test.csv\" ) train_data = FMDataset ( train_df , data_transform ) test_data = FMDataset ( test_df , data_transform ) \u6ce8\u610f\uff1a\u8fd9\u91cc\u9700\u8981\u81ea\u5df1\u4e0b\u8f7d\u6570\u636e\u3002\u53ef\u4ee5\u4ecekaggle\u4e0a\u4e0b\u8f7d\uff08\u9700\u79d1\u5b66\u4e0a\u7f51\uff09\uff08\u4f46\u8c8c\u4f3c\u4e5f\u4e0d\u662f\u6559\u7a0b\u7528\u7684\u7248\u672c\uff09\uff1a https://www.kaggle.com/zalando-research/fashionmnist/ # \u5b9a\u4e49DataLoader\u7c7b\uff0c\u4ee5\u4fbf\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u52a0\u8f7d\u6570\u636e train_loader = DataLoader ( train_data , batch_size = batch_size , shuffle = True , num_workers = num_workers , drop_last = True ) test_loader = DataLoader ( test_data , batch_size = batch_size , shuffle = False , num_workers = num_workers ) # \u6570\u636e\u53ef\u89c6\u5316 import matplotlib.pyplot as plt image , label = next ( iter ( test_loader )) print ( image . shape , label . shape ) plt . imshow ( image [ 0 ][ 0 ], cmap = \"gray\" ) \u8fd9\u91cc\u7a0b\u5e8f\u8fd0\u884c\u4e86\u5f88\u4e45\uff0c\u4e00\u76f4\u8dd1\u4e0d\u51fa\u7ed3\u679c\uff0c\u6539\u7528\u4e86colab","title":"\u6570\u636e\u8bfb\u5165\u548c\u52a0\u8f7d"},{"location":"pytorch-chap04/#_5","text":"\u624b\u642d\u4e00\u4e2aCNN class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv = nn . Sequential ( nn . Conv2d ( 1 , 32 , 5 ), nn . ReLU (), nn . MaxPool2d ( 2 , stride = 2 ), nn . Dropout ( 0.3 ), nn . Conv2d ( 32 , 64 , 5 ), nn . ReLU (), nn . MaxPool2d ( 2 , stride = 2 ), nn . Dropout ( 0.3 ) ) self . fc = nn . Sequential ( nn . Linear ( 64 * 4 * 4 , 512 ), nn . ReLU (), nn . Linear ( 512 , 10 ) ) def forward ( self , x ): x = self . conv ( x ) x = x . view ( - 1 , 64 * 4 * 4 ) x = self . fc ( x ) # x = nn.functional.normalize(x) return x model = Net () # model = model.cuda() # \u5c06\u6a21\u578b\u653e\u5230GPU\u4e0a\u7528\u4e8e\u8bad\u7ec3 # model = nn.DataParallel(model).cuda() # \u591a\u5361\u8bad\u7ec3\u65f6\u7684\u5199\u6cd5\uff0c\u4e4b\u540e\u7684\u8bfe\u7a0b\u4e2d\u4f1a\u8fdb\u4e00\u6b65\u8bb2\u89e3","title":"\u6a21\u578b\u8bbe\u8ba1"},{"location":"pytorch-chap04/#_6","text":"\u4f7f\u7528torch.nn\u6a21\u5757\u81ea\u5e26\u7684CrossEntropy\u635f\u5931\u3002 PyTorch\u4f1a\u81ea\u52a8\u628a\u6574\u6570\u578b\u7684label\u8f6c\u4e3aone-hot\u578b\uff0c\u7528\u4e8e\u8ba1\u7b97CE loss\u3002 \u8fd9\u91cc\u9700\u8981\u786e\u4fddlabel\u662f\u4ece0\u5f00\u59cb\u7684\uff0c\u540c\u65f6\u6a21\u578b\u4e0d\u52a0softmax\u5c42\uff08\u4f7f\u7528logits\u8ba1\u7b97\uff09,\u8fd9\u4e5f\u8bf4\u660e\u4e86PyTorch\u8bad\u7ec3\u4e2d\u5404\u4e2a\u90e8\u5206\u4e0d\u662f\u72ec\u7acb\u7684\uff0c\u9700\u8981\u901a\u76d8\u8003\u8651\u3002 criterion = nn . CrossEntropyLoss () # criterion = nn.CrossEntropyLoss(weight=[1,1,1,1,3,1,1,1,1,1])","title":"\u8bbe\u5b9a\u635f\u5931\u51fd\u6570"},{"location":"pytorch-chap04/#_7","text":"optimizer = optim . Adam ( model . parameters (), lr = 0.001 )","title":"\u8bbe\u5b9a\u4f18\u5316\u5668"},{"location":"pytorch-chap04/#_8","text":"\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff08\u9a8c\u8bc1\uff09 \u5404\u81ea\u5c01\u88c5\u6210\u51fd\u6570\uff0c\u65b9\u4fbf\u540e\u7eed\u8c03\u7528 \u5173\u6ce8\u4e24\u8005\u7684\u4e3b\u8981\u533a\u522b\uff1a \u6a21\u578b\u72b6\u6001\u8bbe\u7f6e \u662f\u5426\u9700\u8981\u521d\u59cb\u5316\u4f18\u5316\u5668 \u662f\u5426\u9700\u8981\u5c06loss\u4f20\u56de\u5230\u7f51\u7edc \u662f\u5426\u9700\u8981\u6bcf\u6b65\u66f4\u65b0optimizer \u6b64\u5916\uff0c\u5bf9\u4e8e\u6d4b\u8bd5\u6216\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u8ba1\u7b97\u5206\u7c7b\u51c6\u786e\u7387 def train ( epoch ): model . train () train_loss = 0 for data , label in train_loader : # data, label = data.cuda(), label.cuda() # \u4e0d\u7528cuda\u5148 optimizer . zero_grad () output = model ( data ) loss = criterion ( output , label ) loss . backward () optimizer . step () train_loss += loss . item () * data . size ( 0 ) train_loss = train_loss / len ( train_loader . dataset ) print ( 'Epoch: {} \\t Training Loss: {:.6f} ' . format ( epoch , train_loss )) def val ( epoch ): model . eval () val_loss = 0 gt_labels = [] pred_labels = [] with torch . no_grad (): for data , label in test_loader : # data, label = data.cuda(), label.cuda() # \u4e0d\u7528cuda\u5148 output = model ( data ) preds = torch . argmax ( output , 1 ) gt_labels . append ( label . cpu () . data . numpy ()) pred_labels . append ( preds . cpu () . data . numpy ()) loss = criterion ( output , label ) val_loss += loss . item () * data . size ( 0 ) val_loss = val_loss / len ( test_loader . dataset ) gt_labels , pred_labels = np . concatenate ( gt_labels ), np . concatenate ( pred_labels ) acc = np . sum ( gt_labels == pred_labels ) / len ( pred_labels ) print ( 'Epoch: {} \\t Validation Loss: {:.6f} , Accuracy: {:6f} ' . format ( epoch , val_loss , acc )) for epoch in range ( 1 , epochs + 1 ): train ( epoch ) val ( epoch ) \u7ed3\u679c\uff08\u4e0d\u662f\u5f88\u597d\uff09\uff1a / usr / local / lib / python3 .7 / dist - packages / torch / utils / data / dataloader . py : 481 : UserWarning : This DataLoader will create 4 worker processes in total . Our suggested max number of worker in current system is 2 , which is smaller than what this DataLoader is going to create . Please be aware that excessive worker creation might get DataLoader running slow or even freeze , lower the worker number to avoid potential slowness / freeze if necessary . cpuset_checked )) / usr / local / lib / python3 .7 / dist - packages / torch / nn / functional . py : 718 : UserWarning : Named tensors and all their associated APIs are an experimental feature and subject to change . Please do not use them for anything important until they are released as stable . ( Triggered internally at / pytorch / c10 / core / TensorImpl . h : 1156. ) return torch . max_pool2d ( input , kernel_size , stride , padding , dilation , ceil_mode ) Epoch : 1 Training Loss : 1.859782 Epoch : 1 Validation Loss : 1.252422 , Accuracy : 0.504242 Epoch : 2 Training Loss : 1.073511 Epoch : 2 Validation Loss : 0.958262 , Accuracy : 0.620891 Epoch : 3 Training Loss : 0.912065 Epoch : 3 Validation Loss : 0.859967 , Accuracy : 0.682927 Epoch : 4 Training Loss : 0.803673 Epoch : 4 Validation Loss : 0.725328 , Accuracy : 0.743902 Epoch : 5 Training Loss : 0.723244 Epoch : 5 Validation Loss : 0.699738 , Accuracy : 0.725345 Epoch : 6 Training Loss : 0.676728 Epoch : 6 Validation Loss : 0.688325 , Accuracy : 0.742312 Epoch : 7 Training Loss : 0.624213 Epoch : 7 Validation Loss : 0.633743 , Accuracy : 0.744963 Epoch : 8 Training Loss : 0.595873 Epoch : 8 Validation Loss : 0.588029 , Accuracy : 0.770414 Epoch : 9 Training Loss : 0.561574 Epoch : 9 Validation Loss : 0.578903 , Accuracy : 0.765642 Epoch : 10 Training Loss : 0.544152 Epoch : 10 Validation Loss : 0.563249 , Accuracy : 0.791622 Epoch : 11 Training Loss : 0.532662 Epoch : 11 Validation Loss : 0.561163 , Accuracy : 0.790032 Epoch : 12 Training Loss : 0.520769 Epoch : 12 Validation Loss : 0.560051 , Accuracy : 0.783139 Epoch : 13 Training Loss : 0.495388 Epoch : 13 Validation Loss : 0.537520 , Accuracy : 0.794804 Epoch : 14 Training Loss : 0.461928 Epoch : 14 Validation Loss : 0.533855 , Accuracy : 0.799046 Epoch : 15 Training Loss : 0.453786 Epoch : 15 Validation Loss : 0.534338 , Accuracy : 0.805408 Epoch : 16 Training Loss : 0.457692 Epoch : 16 Validation Loss : 0.515626 , Accuracy : 0.812831 Epoch : 17 Training Loss : 0.449596 Epoch : 17 Validation Loss : 0.504590 , Accuracy : 0.816013 Epoch : 18 Training Loss : 0.443980 Epoch : 18 Validation Loss : 0.503526 , Accuracy : 0.818134 Epoch : 19 Training Loss : 0.420621 Epoch : 19 Validation Loss : 0.488520 , Accuracy : 0.826087 Epoch : 20 Training Loss : 0.418917 Epoch : 20 Validation Loss : 0.524965 , Accuracy : 0.797985","title":"\u8bad\u7ec3\u548c\u6d4b\u8bd5"},{"location":"pytorch-chap04/#_9","text":"\u7a0b\u5e8f\u7684colab\u94fe\u63a5\uff1a https://colab.research.google.com/drive/1kvaBEEgQ_a5G5xOHUe5ih4Qq8L5C9zYY?usp=sharing Datawhale\u5f00\u6e90\u9879\u76ee\uff1a\u6df1\u5165\u6d45\u51faPyTorch https://github.com/datawhalechina/thorough-pytorch/ \u674e\u5b8f\u6bc5\u673a\u5668\u5b66\u4e602021\u6625-PyTorch Tutorial https://www.bilibili.com/video/BV1Wv411h7kN?p=5 \u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60pytorch\u7248 https://zh-v2.d2l.ai/chapter_preface/index.html PyTorch\u5b98\u65b9\u6559\u7a0b\u4e2d\u6587\u7248 https://pytorch123.com/SecondSection/training_a_classifier/","title":"\u53c2\u8003\u8d44\u6599"},{"location":"sys-index/","text":"Computer Systems \u00b6 Nand2Tetris \u8ba1\u7b97\u673a\u7cfb\u7edf\u8981\u7d20 ML Compilation \u673a\u5668\u5b66\u4e60\u7f16\u8bd1","title":"\u76ee\u5f55"},{"location":"sys-index/#computer-systems","text":"Nand2Tetris \u8ba1\u7b97\u673a\u7cfb\u7edf\u8981\u7d20 ML Compilation \u673a\u5668\u5b66\u4e60\u7f16\u8bd1","title":"Computer Systems"},{"location":"vu-hpc-2022/","text":"VU HPC Course 2022 \u00b6 Links: [1] VU HPC Course 2022 https://hpc.labs.vu.nl/schedule-2022/ [2] SURF User Knowledge Base https://servicedesk.surf.nl/wiki/display/WIKI/SURF+User+Knowledge+Base SURFLisa \u00b6 Apply for account \u00b6 For a regular cpu Lisa account a mail to helpdesk@surfsara.nl is sufficient (see https://bit.ly/SURF-Lisa ) To apply for the Lisa GPU island use the VU/UvA form at https://servicedesk.surf.nl/ In Service Desk, we can apply for different resources of SURF, such as LISA, Snellius, Research Cloud etc. Login \u00b6 cpu node: username@lisa.surfsara.nl gpu node: username@login-gpu.lisa.surfsara.nl Transfer data \u00b6 # copy from local PC to HPC system scp sourcefile <username>@lisa.surfsara.nl:destinationdir scp -r sourcedir <username>@lisa.surfsara.nl:destinationdir # copy from HPC system to local PC scp -r <username>@lisa.surfsara.nl:sourcefile ~ Writing a job \u00b6 SURF Cloud \u00b6","title":"VU HPC Course 2022"},{"location":"vu-hpc-2022/#vu-hpc-course-2022","text":"Links: [1] VU HPC Course 2022 https://hpc.labs.vu.nl/schedule-2022/ [2] SURF User Knowledge Base https://servicedesk.surf.nl/wiki/display/WIKI/SURF+User+Knowledge+Base","title":"VU HPC Course 2022"},{"location":"vu-hpc-2022/#surflisa","text":"","title":"SURFLisa"},{"location":"vu-hpc-2022/#apply-for-account","text":"For a regular cpu Lisa account a mail to helpdesk@surfsara.nl is sufficient (see https://bit.ly/SURF-Lisa ) To apply for the Lisa GPU island use the VU/UvA form at https://servicedesk.surf.nl/ In Service Desk, we can apply for different resources of SURF, such as LISA, Snellius, Research Cloud etc.","title":"Apply for account"},{"location":"vu-hpc-2022/#login","text":"cpu node: username@lisa.surfsara.nl gpu node: username@login-gpu.lisa.surfsara.nl","title":"Login"},{"location":"vu-hpc-2022/#transfer-data","text":"# copy from local PC to HPC system scp sourcefile <username>@lisa.surfsara.nl:destinationdir scp -r sourcedir <username>@lisa.surfsara.nl:destinationdir # copy from HPC system to local PC scp -r <username>@lisa.surfsara.nl:sourcefile ~","title":"Transfer data"},{"location":"vu-hpc-2022/#writing-a-job","text":"","title":"Writing a job"},{"location":"vu-hpc-2022/#surf-cloud","text":"","title":"SURF Cloud"}]}